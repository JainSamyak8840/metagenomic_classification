{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Metagenomics_Bacteria_Phylum.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1pJgDl0qRIdJSmA-2foG91U4jRU9NV3cg","authorship_tag":"ABX9TyMGK3zmuDOrGes3xSTf/qWX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"TgOJN0RBiekF","colab_type":"text"},"source":["## Microbes(Bacteria and Archaea) Phylum Taxanomy Classification by taking read as input"]},{"cell_type":"code","metadata":{"id":"qrsrVZTx8VOb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1596094552382,"user_tz":-330,"elapsed":4474,"user":{"displayName":"Samyak Jain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gioocp3b6d0F-BZZ3D9c4FJz-F7ie6BPVSG0xZr=s64","userId":"08656034860933804166"}},"outputId":"b8e8a63f-a39c-4fce-e204-304d721b62d1"},"source":["import torch, gc\n","import math\n","import numpy as np\n","import pandas as pd\n","import pickle as pkl\n","\n","gc.collect()\n","torch.cuda.empty_cache()\n","\n","use_cuda = torch.cuda.is_available()\n","\n","if not use_cuda:\n","    print('CUDA is not available.  Training on CPU ...')\n","else:\n","    print('CUDA is available!  Training on GPU ...')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["CUDA is available!  Training on GPU ...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"25PiqrZy3U8m","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1596094561901,"user_tz":-330,"elapsed":4376,"user":{"displayName":"Samyak Jain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gioocp3b6d0F-BZZ3D9c4FJz-F7ie6BPVSG0xZr=s64","userId":"08656034860933804166"}},"outputId":"742056e7-5383-4a5e-97d0-a09eefc50366"},"source":["from torchvision import datasets, transforms\n","\n","path='/content/drive/My Drive/Metagenomics/Bacteria/'\n","data=pd.read_csv(path+\"data/Phylum/\"+\"data_phylum.csv\")\n","target_phylum=data['Phylum']\n","feature_read=data['read']\n","print(len(feature_read))"],"execution_count":2,"outputs":[{"output_type":"stream","text":["280\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nLOz-uvaqEJB","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596094568109,"user_tz":-330,"elapsed":4003,"user":{"displayName":"Samyak Jain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gioocp3b6d0F-BZZ3D9c4FJz-F7ie6BPVSG0xZr=s64","userId":"08656034860933804166"}}},"source":["path='/content/drive/My Drive/Metagenomics/Bacteria/Phylum/'\n","\n","train_data=pd.read_csv(path+\"train.csv\")\n","valid_data=pd.read_csv(path+\"valid.csv\")\n","test_data=pd.read_csv(path+\"test.csv\")"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bcpt-5CQulH0","colab_type":"code","colab":{}},"source":["train_target=train_data['Phylum']\n","train_feature=train_data['read']\n","valid_target=valid_data['Phylum']\n","valid_feature=valid_data['read']\n","test_target=test_data['Phylum']\n","test_feature=test_data['read']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3YLZpnn15G_M","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":170},"executionInfo":{"status":"ok","timestamp":1596094574419,"user_tz":-330,"elapsed":1530,"user":{"displayName":"Samyak Jain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gioocp3b6d0F-BZZ3D9c4FJz-F7ie6BPVSG0xZr=s64","userId":"08656034860933804166"}},"outputId":"352f64cd-6802-48f1-cded-aac40544718c"},"source":["print(\"Total labels: \",len(target_phylum))\n","classes=sorted(set(target_phylum))\n","class_dict={y:x for x,y in enumerate(classes)}\n","class_dict"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Total labels:  280\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["{'Acidobacteria': 0,\n"," 'Actinobacteria': 1,\n"," 'Bacteroidetes': 2,\n"," 'Firmicutes': 3,\n"," 'Gemmatimonadetes': 4,\n"," 'Parcubacteria': 5,\n"," 'Proteobacteria': 6,\n"," 'Saccharibacteria': 7}"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"kmV02k2o8D1N","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":306},"executionInfo":{"status":"ok","timestamp":1596094578706,"user_tz":-330,"elapsed":2076,"user":{"displayName":"Samyak Jain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gioocp3b6d0F-BZZ3D9c4FJz-F7ie6BPVSG0xZr=s64","userId":"08656034860933804166"}},"outputId":"763f6949-b154-4cf8-cc32-925671020b09"},"source":["sets= {'A','C','G','T','U','W','S','M','K','R','Y','B','D','H','V','N','Z'}\n","char=sorted(sets)\n","char_encode={y:x+1 for x,y in enumerate(char)}\n","char_encode"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'A': 1,\n"," 'B': 2,\n"," 'C': 3,\n"," 'D': 4,\n"," 'G': 5,\n"," 'H': 6,\n"," 'K': 7,\n"," 'M': 8,\n"," 'N': 9,\n"," 'R': 10,\n"," 'S': 11,\n"," 'T': 12,\n"," 'U': 13,\n"," 'V': 14,\n"," 'W': 15,\n"," 'Y': 16,\n"," 'Z': 17}"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"4Kg22WR4Ll3h","colab_type":"code","colab":{}},"source":["path='/content/drive/My Drive/Metagenomics/Bacteria/Phylum/'\n","train_path1 = path + 'train_input.pkl'\n","train_path2 = path + 'train_target.pkl'\n","valid_path1 = path + 'valid_input.pkl'\n","valid_path2 = path + 'valid_target.pkl'\n","test_path1 = path + 'test_input.pkl'\n","test_path2 = path + 'test_target.pkl'\n","train_file1 = open(train_path1, 'wb')\n","train_file2 = open(train_path2, 'wb')\n","valid_file1 = open(valid_path1, 'wb')\n","valid_file2 = open(valid_path2, 'wb')\n","test_file1 = open(test_path1, 'wb')\n","test_file2 = open(test_path2, 'wb')\n","def f(feature,target,file1,file2):\n","  lst1=[]\n","  lst2=[]\n","  save=True\n","  if save:\n","    for i,j in enumerate(feature):\n","      read=np.zeros(1650)\n","      for x,y in enumerate(j):\n","            read[x]=char_encode[y]\n","      read=read.reshape(33,50)\n","      lst1.append(read/20)\n","      lst2.append(np.array([class_dict[target[i]]]))\n","  pkl.dump(lst1,file1)\n","  pkl.dump(lst2,file2)\n","  file1.close()\n","  file2.close()\n","\n","f(train_feature,train_target,train_file1,train_file2)\n","f(valid_feature,valid_target,valid_file1,valid_file2)\n","f(test_feature,test_target,test_file1,test_file2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"27Q78aEMgSN1","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596094593442,"user_tz":-330,"elapsed":6194,"user":{"displayName":"Samyak Jain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gioocp3b6d0F-BZZ3D9c4FJz-F7ie6BPVSG0xZr=s64","userId":"08656034860933804166"}}},"source":["path='/content/drive/My Drive/Metagenomics/Bacteria/Phylum/'\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","num_workers=0\n","batch_size = 20\n","\n","train_path1 = path + 'train_input.pkl'\n","train_path2 = path + 'train_target.pkl'\n","\n","valid_path1 = path + 'valid_input.pkl'\n","valid_path2 = path + 'valid_target.pkl'\n","\n","test_path1 = path + 'test_input.pkl'\n","test_path2 = path + 'test_target.pkl'\n","\n","train_file1 = open(train_path1, 'rb')\n","train_file2 = open(train_path2, 'rb')\n","\n","valid_file1 = open(valid_path1, 'rb')\n","valid_file2 = open(valid_path2, 'rb')\n","\n","test_file1 = open(test_path1, 'rb')\n","test_file2 = open(test_path2, 'rb')\n","\n","\n","train_input = pkl.load(train_file1)\n","train_target = pkl.load(train_file2)\n","\n","valid_input = pkl.load(valid_file1)\n","valid_target = pkl.load(valid_file2)\n","\n","test_input = pkl.load(test_file1)\n","test_target = pkl.load(test_file2)\n","\n","train_x = torch.Tensor(train_input)\n","train_y = torch.Tensor(train_target)\n","\n","valid_x = torch.Tensor(valid_input)\n","valid_y = torch.Tensor(valid_target)\n","\n","test_x = torch.Tensor(test_input)\n","test_y = torch.Tensor(test_target)\n","\n","train_file1.close()\n","train_file2.close()\n","\n","valid_file1.close()\n","valid_file2.close()\n","\n","test_file1.close()\n","test_file2.close()\n","\n","train_dataset = TensorDataset(train_x,train_y.long())\n","valid_dataset = TensorDataset(valid_x,valid_y.long())\n","test_dataset = TensorDataset(test_x,test_y.long())\n","\n","train_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle= True)\n","valid_loader = DataLoader(valid_dataset,batch_size=batch_size,shuffle= True)\n","test_loader = DataLoader(test_dataset,batch_size=batch_size,shuffle= True)\n","\n","loaders = {\n","    'train': train_loader,\n","    'valid': valid_loader,\n","    'test': test_loader\n","}"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"89c5Kt4axC6K","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1595207789730,"user_tz":420,"elapsed":1815,"user":{"displayName":"Samyak Jain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gioocp3b6d0F-BZZ3D9c4FJz-F7ie6BPVSG0xZr=s64","userId":"08656034860933804166"}},"outputId":"f694217a-4686-45b4-e33f-2ed93db77c86"},"source":["dataiter = iter(valid_loader)\n","images, labels = dataiter.next()\n","print(\"Input: \",images,\" Label: \",labels)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Input:  tensor([[[0.0500, 0.2500, 0.0500,  ..., 0.0500, 0.6000, 0.2500],\n","         [0.1500, 0.0500, 0.0500,  ..., 0.1500, 0.2500, 0.6000],\n","         [0.2500, 0.2500, 0.2500,  ..., 0.6000, 0.0500, 0.1500],\n","         ...,\n","         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n","\n","        [[0.0500, 0.2500, 0.0500,  ..., 0.0500, 0.6000, 0.2500],\n","         [0.1500, 0.0500, 0.0500,  ..., 0.1500, 0.2500, 0.6000],\n","         [0.2500, 0.2500, 0.2500,  ..., 0.6000, 0.0500, 0.1500],\n","         ...,\n","         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n","\n","        [[0.0500, 0.2500, 0.0500,  ..., 0.0500, 0.6000, 0.2500],\n","         [0.1500, 0.0500, 0.0500,  ..., 0.2500, 0.2500, 0.6000],\n","         [0.2500, 0.0500, 0.2500,  ..., 0.0500, 0.0500, 0.2500],\n","         ...,\n","         [0.1500, 0.2500, 0.6000,  ..., 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n","\n","        ...,\n","\n","        [[0.0500, 0.0500, 0.2500,  ..., 0.1500, 0.0500, 0.6000],\n","         [0.2500, 0.1500, 0.0500,  ..., 0.0500, 0.1500, 0.0500],\n","         [0.1500, 0.2500, 0.6000,  ..., 0.2500, 0.6000, 0.0500],\n","         ...,\n","         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n","\n","        [[0.0500, 0.2500, 0.2500,  ..., 0.2500, 0.0500, 0.0500],\n","         [0.1500, 0.2500, 0.1500,  ..., 0.0500, 0.0500, 0.2500],\n","         [0.1500, 0.6000, 0.6000,  ..., 0.1500, 0.0500, 0.0500],\n","         ...,\n","         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n","\n","        [[0.2500, 0.1500, 0.1500,  ..., 0.0500, 0.2500, 0.2500],\n","         [0.0500, 0.6000, 0.2500,  ..., 0.6000, 0.6000, 0.1500],\n","         [0.1500, 0.0500, 0.2500,  ..., 0.2500, 0.0500, 0.1500],\n","         ...,\n","         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]])  Label:  tensor([[6],\n","        [6],\n","        [0],\n","        [7],\n","        [2],\n","        [2],\n","        [3],\n","        [6],\n","        [6],\n","        [0],\n","        [7],\n","        [0],\n","        [7],\n","        [0],\n","        [0],\n","        [1],\n","        [4],\n","        [4],\n","        [2],\n","        [5]])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9ka2UO8LEXZR","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596094602126,"user_tz":-330,"elapsed":1559,"user":{"displayName":"Samyak Jain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gioocp3b6d0F-BZZ3D9c4FJz-F7ie6BPVSG0xZr=s64","userId":"08656034860933804166"}}},"source":["import torch as T\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# helper conv function\n","def conv(in_channels, out_channels, kernel_size, stride=2, padding=1, batch_norm=False):\n","    \"\"\"Creates a convolutional layer, with optional batch normalization.\n","    \"\"\"\n","    layers = []\n","    conv_layer = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, \n","                           kernel_size=kernel_size, stride=stride, padding=padding, bias=False)\n","    \n","    layers.append(conv_layer)\n","\n","    if batch_norm:\n","        layers.append(nn.BatchNorm2d(out_channels))\n","    return nn.Sequential(*layers)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"FPAmiCCoNhGQ","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596094608048,"user_tz":-330,"elapsed":1536,"user":{"displayName":"Samyak Jain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gioocp3b6d0F-BZZ3D9c4FJz-F7ie6BPVSG0xZr=s64","userId":"08656034860933804166"}}},"source":["# helper avg_pool function\n","def pool(out_channels,kernel_size=2, stride=2, padding=1, batch_norm=True):\n","    \"\"\"Creates a convolutional layer, with optional batch normalization.\n","    \"\"\"\n","    layers = []\n","    pool_layer = nn.AvgPool2d(kernel_size, stride, padding)\n","    layers.append(pool_layer)\n","\n","    if batch_norm:\n","        layers.append(nn.BatchNorm2d(out_channels))\n","    return nn.Sequential(*layers)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"o4HWiWvBFdvI","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596094614242,"user_tz":-330,"elapsed":1618,"user":{"displayName":"Samyak Jain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gioocp3b6d0F-BZZ3D9c4FJz-F7ie6BPVSG0xZr=s64","userId":"08656034860933804166"}}},"source":["# residual block class\n","class ResidualBlock(nn.Module):\n","    \"\"\"Defines a residual block.\n","       This adds an input x to a convolutional layer (applied to x) with the same size input and output.\n","       These blocks allow a model to learn an effective transformation from one domain to another.\n","    \"\"\"\n","    def __init__(self, conv_dim,nf1=1,nf2=1):\n","        super(ResidualBlock, self).__init__()\n","        # conv_dim = number of inputs\n","        \n","        # define two convolutional layers + batch normalization that will act as our residual function, F(x)\n","        # layers should have the same shape input as output; I suggest a kernel_size of 3\n","        \n","        self.conv_dim_rslv_layer = conv(in_channels=conv_dim*nf1, out_channels=conv_dim*nf2, \n","                                kernel_size= 3, stride=1, padding=1)\n","        self.conv_layer1 = conv(in_channels=conv_dim*nf1, out_channels=conv_dim*nf2, \n","                                kernel_size= 3, stride=1, padding=1,batch_norm=True)\n","        \n","        self.conv_layer2 = conv(in_channels=conv_dim*nf2, out_channels=conv_dim*nf2, \n","                               kernel_size= 3, stride=1, padding=1)\n","        \n","    def forward(self, x):\n","        # apply a ReLu activation the outputs of the first layer\n","        # return a summed output, x + resnet_block(x)\n","        out_1 = F.relu(self.conv_layer1(x)) \n","        y= self.conv_layer2(out_1)\n","        if x.size()[1]!=y.size()[1]:\n","           x= self.conv_dim_rslv_layer(x)\n","        out_2 = x + y\n","        return out_2\n","    "],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"XIZZq1vrFvJC","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596094664310,"user_tz":-330,"elapsed":1192,"user":{"displayName":"Samyak Jain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gioocp3b6d0F-BZZ3D9c4FJz-F7ie6BPVSG0xZr=s64","userId":"08656034860933804166"}}},"source":["total_classes= 8\n","\n","class  GeNet(nn.Module):\n","    \n","    def __init__(self, conv_dim=128, n_res_blocks=4):\n","        super(GeNet, self).__init__()\n","\n","        # 1. Define the encoder part of the generator\n","        \n","        # initial convolutional layer given, below\n","        self.conv1 = conv(1, conv_dim, 4)\n","      \n","        self.avg_pool = pool(2*conv_dim,kernel_size=[2,1],stride=[2,1],padding=0)\n","        # 2. Define the resnet part of the generator\n","        # Residual blocks\n","        res_layers = []\n","\n","        res_layers.append(pool(conv_dim))\n","        res_layers.append(ResidualBlock(conv_dim))\n","        \n","        res_layers.append(pool(conv_dim))\n","        res_layers.append(ResidualBlock(conv_dim))\n","        \n","        res_layers.append(pool(conv_dim))\n","        res_layers.append(ResidualBlock(conv_dim,nf2=2))\n","\n","        res_layers.append(pool(2*conv_dim))\n","        res_layers.append(ResidualBlock(conv_dim,nf1=2,nf2=2))\n","\n","        # use sequential to create these layers\n","        self.res_blocks = nn.Sequential(*res_layers)\n","        \n","        fc_layers=[]\n","        # fully-connected\n","        fc_layers.append(nn.Linear(768,384))\n","        fc_layers.append(nn.Dropout2d(p=0.20))\n","        fc_layers.append(nn.ReLU())\n","        fc_layers.append(nn.Linear(384,768))\n","        fc_layers.append(nn.Dropout2d(p=0.20))\n","        fc_layers.append(nn.ReLU())\n","        fc_layers.append(nn.Linear(768,total_classes))\n","\n","        # use sequential to create these layers\n","        self.fc = nn.Sequential(*fc_layers)\n","\n","\n","         # drop-out\n","        self.drop_out= nn.Dropout2d(p=0.20)\n","\n","    def forward(self, x):\n","        \"\"\"Given an image x, returns a transformed image.\"\"\"\n","        # define feedforward behavior, applying activations as necessary\n","\n","        out = F.relu(self.conv1(x))\n","        out = F.relu(self.drop_out(self.res_blocks(out)))\n","        out = self.avg_pool(out)\n","\n","         # Flatten\n","        out = out.view(-1, 768)\n","        \n","        # tanh applied to last layer\n","        out = self.fc(out)\n","        return F.log_softmax(out, dim=1)\n","\n","# instantiate the CNN\n","model = GeNet()\n","\n","# check if CUDA is available\n","use_cuda = torch.cuda.is_available()\n","\n","# move tensors to GPU if CUDA is available\n","if use_cuda:\n","    model.cuda()"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"pVBfBhTQmvNS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1596094635023,"user_tz":-330,"elapsed":1837,"user":{"displayName":"Samyak Jain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gioocp3b6d0F-BZZ3D9c4FJz-F7ie6BPVSG0xZr=s64","userId":"08656034860933804166"}},"outputId":"0cb6e25b-8644-47e2-a999-84353e2058c8"},"source":["model.parameters"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<bound method Module.parameters of GeNet(\n","  (conv1): Sequential(\n","    (0): Conv2d(1, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","  )\n","  (avg_pool): Sequential(\n","    (0): AvgPool2d(kernel_size=[2, 1], stride=[2, 1], padding=0)\n","    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  )\n","  (res_blocks): Sequential(\n","    (0): Sequential(\n","      (0): AvgPool2d(kernel_size=2, stride=2, padding=1)\n","      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (1): ResidualBlock(\n","      (conv_dim_rslv_layer): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","      (conv_layer1): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (conv_layer2): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","    (2): Sequential(\n","      (0): AvgPool2d(kernel_size=2, stride=2, padding=1)\n","      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (3): ResidualBlock(\n","      (conv_dim_rslv_layer): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","      (conv_layer1): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (conv_layer2): Sequential(\n","        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","    (4): Sequential(\n","      (0): AvgPool2d(kernel_size=2, stride=2, padding=1)\n","      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (5): ResidualBlock(\n","      (conv_dim_rslv_layer): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","      (conv_layer1): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (conv_layer2): Sequential(\n","        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","    (6): Sequential(\n","      (0): AvgPool2d(kernel_size=2, stride=2, padding=1)\n","      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (7): ResidualBlock(\n","      (conv_dim_rslv_layer): Sequential(\n","        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","      (conv_layer1): Sequential(\n","        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (conv_layer2): Sequential(\n","        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","  )\n","  (fc): Sequential(\n","    (0): Linear(in_features=768, out_features=384, bias=True)\n","    (1): Dropout2d(p=0.2, inplace=False)\n","    (2): ReLU()\n","    (3): Linear(in_features=384, out_features=768, bias=True)\n","    (4): Dropout2d(p=0.2, inplace=False)\n","    (5): ReLU()\n","    (6): Linear(in_features=768, out_features=8, bias=True)\n","  )\n","  (drop_out): Dropout2d(p=0.2, inplace=False)\n",")>"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"NzrYUlvYhkU9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":272},"executionInfo":{"status":"ok","timestamp":1596096455660,"user_tz":-330,"elapsed":8882,"user":{"displayName":"Samyak Jain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gioocp3b6d0F-BZZ3D9c4FJz-F7ie6BPVSG0xZr=s64","userId":"08656034860933804166"}},"outputId":"76744369-9a8d-40ea-defe-8b7d819ca6d3"},"source":["# Do not uncomment this cell but add this\n","from collections import OrderedDict\n","model_transfer = GeNet() \n","model_transfer.load_state_dict(torch.load(path+'bacteria_phylum_0.pt'))\n","\n","layers= OrderedDict()\n","mod = list(model_transfer.fc.children())\n","mod[-1]=nn.Linear(768,384)\n","mod.append(nn.Dropout2d(p=0.40))\n","mod.append(nn.ReLU())\n","mod.append(nn.Linear(384,192))\n","mod.append(nn.Dropout2d(p=0.40))\n","mod.append(nn.ReLU())\n","mod.append(nn.Linear(192,8))\n","\n","layer=['fc','drop_out','relu']\n","c=0\n","while len(mod)!=1:\n","    for i,j in zip(layer,mod):\n","        layers[i+str(c)]=j\n","    c+=1\n","    mod=mod[3:]\n","else:\n","    layers['fc'+str(c)]=mod[-1]\n","\n","new_fc = nn.Sequential(layers)\n","model_transfer.fc=new_fc\n","model_transfer.fc"],"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Sequential(\n","  (fc0): Linear(in_features=768, out_features=384, bias=True)\n","  (drop_out0): Dropout2d(p=0.2, inplace=False)\n","  (relu0): ReLU()\n","  (fc1): Linear(in_features=384, out_features=768, bias=True)\n","  (drop_out1): Dropout2d(p=0.2, inplace=False)\n","  (relu1): ReLU()\n","  (fc2): Linear(in_features=768, out_features=384, bias=True)\n","  (drop_out2): Dropout2d(p=0.4, inplace=False)\n","  (relu2): ReLU()\n","  (fc3): Linear(in_features=384, out_features=192, bias=True)\n","  (drop_out3): Dropout2d(p=0.4, inplace=False)\n","  (relu3): ReLU()\n","  (fc4): Linear(in_features=192, out_features=8, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"code","metadata":{"id":"90sSuV5vhpIT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":187},"executionInfo":{"status":"ok","timestamp":1596096474013,"user_tz":-330,"elapsed":8985,"user":{"displayName":"Samyak Jain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gioocp3b6d0F-BZZ3D9c4FJz-F7ie6BPVSG0xZr=s64","userId":"08656034860933804166"}},"outputId":"ceec482a-a115-4e67-d867-ccf22a2d7eda"},"source":["for param in model_transfer.parameters():\n","    param.requires_grad = False\n","\n","\n","c=0\n","for param in model_transfer.fc.parameters():\n","    if c>3:\n","       param.requires_grad = True\n","    if c%2==0:\n","       size=\"weight_size\"\n","    else:\n","       size=\"bias_size\"\n","    print(f\"fc{c} : {size}({param.size()}) require_grad= {param.requires_grad}\") \n","    c+=1\n","\n","# check if CUDA is available\n","use_cuda = torch.cuda.is_available()\n","\n","# move tensors to GPU if CUDA is available\n","if use_cuda:\n","    model_transfer.cuda()"],"execution_count":42,"outputs":[{"output_type":"stream","text":["fc0 : weight_size(torch.Size([384, 768])) require_grad= False\n","fc1 : bias_size(torch.Size([384])) require_grad= False\n","fc2 : weight_size(torch.Size([768, 384])) require_grad= False\n","fc3 : bias_size(torch.Size([768])) require_grad= False\n","fc4 : weight_size(torch.Size([384, 768])) require_grad= True\n","fc5 : bias_size(torch.Size([384])) require_grad= True\n","fc6 : weight_size(torch.Size([192, 384])) require_grad= True\n","fc7 : bias_size(torch.Size([192])) require_grad= True\n","fc8 : weight_size(torch.Size([8, 192])) require_grad= True\n","fc9 : bias_size(torch.Size([8])) require_grad= True\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QjODnMJDjbEl","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596096573436,"user_tz":-330,"elapsed":1340,"user":{"displayName":"Samyak Jain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gioocp3b6d0F-BZZ3D9c4FJz-F7ie6BPVSG0xZr=s64","userId":"08656034860933804166"}}},"source":["import torch.optim as optim\n","\n","loaders_transfer = loaders\n","criterion_transfer = nn.CrossEntropyLoss()\n","optimizer_transfer = optim.SGD(model_transfer.parameters(), lr=0.0001)"],"execution_count":45,"outputs":[]},{"cell_type":"code","metadata":{"id":"o2pWyLhejqz_","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596096509177,"user_tz":-330,"elapsed":9013,"user":{"displayName":"Samyak Jain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gioocp3b6d0F-BZZ3D9c4FJz-F7ie6BPVSG0xZr=s64","userId":"08656034860933804166"}}},"source":["train_losses, valid_losses = [], [] \n","\n","def train(n_epochs, loader, model, optimizer, criterion, use_cuda, save_path):\n","    \"\"\"returns trained model\"\"\"\n","    # initialize tracker for minimum validation loss\n","    valid_loss_min = np.Inf \n","    for epoch in range(1, n_epochs+1):\n","        # initialize variables to monitor training and validation loss\n","        train_loss = 0.0\n","        valid_loss = 0.0\n","\n","        ###################\n","        # train the model #\n","        ###################\n","        model.train()\n","        for batch_idx, (data, target) in enumerate(loader['train']):\n","            # move to GPU\n","            if use_cuda:\n","                data, target = data.cuda(), target.cuda()\n","            ## find the loss and update the model parameters accordingly\n","            ## record the average training loss, using something like\n","            ## train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n","            # initialize weights to zero\n","            optimizer.zero_grad()\n","            data = data.unsqueeze(1)\n","            output = model(data)\n","            target = target.squeeze(1)\n","            # calculate the loss\n","            loss = criterion(output,target)\n","\n","            # backward pass: compute gradient of the loss with respect to model parameters\n","            loss.backward()\n","            \n","            # parameter update\n","            optimizer.step()\n","            \n","            # update running training loss\n","            train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n","            \n","            \n","       ######################    \n","        # validate the model #\n","        ######################\n","        model.eval()\n","        for batch_idx, (data, target) in enumerate(loader['valid']):\n","            # move to GPU\n","            if use_cuda:\n","                data, target = data.cuda(), target.cuda()\n","            ## update the average validation loss\n","            data = data.unsqueeze(1)\n","            output = model(data)\n","            target = target.squeeze(1)\n","            loss = criterion(output, target)\n","            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n","\n","        new_data = {'epochs': [epoch],\n","            'trainlosses': [train_loss],\n","            'vallosses': [valid_loss] }\n","       \n","        train_losses.append(train_loss) \n","        valid_losses.append(valid_loss)  \n","  \n","        # print training/validation statistics \n","        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n","            epoch, \n","            train_loss,\n","            valid_loss\n","            ))\n","        \n","        ## TODO: save the model if validation loss has decreased\n","        if valid_loss < valid_loss_min:\n","            torch.save(model.state_dict(), save_path)\n","            print('Validation loss has decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n","            valid_loss_min,\n","            valid_loss))\n","            valid_loss_min = valid_loss\n","\n","    # return trained model\n","    return model"],"execution_count":44,"outputs":[]},{"cell_type":"code","metadata":{"id":"PkCCkdTrjcyS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1596096885719,"user_tz":-330,"elapsed":302921,"user":{"displayName":"Samyak Jain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gioocp3b6d0F-BZZ3D9c4FJz-F7ie6BPVSG0xZr=s64","userId":"08656034860933804166"}},"outputId":"fea36e61-9865-45a4-a170-099c5ab15855"},"source":["# train the model\n","n_epochs= 4000\n","#model_transfer.load_state_dict(torch.load(path+'bacteria_phylum_1.pt'))\n","model_transfer = train(n_epochs, loaders_transfer, model_transfer, optimizer_transfer, criterion_transfer, use_cuda,path+'bacteria_phylum_1.pt')"],"execution_count":46,"outputs":[{"output_type":"stream","text":["Epoch: 1 \tTraining Loss: 2.080221 \tValidation Loss: 2.080151\n","Validation loss has decreased (inf --> 2.080151).  Saving model ...\n","Epoch: 2 \tTraining Loss: 2.088263 \tValidation Loss: 2.078395\n","Validation loss has decreased (2.080151 --> 2.078395).  Saving model ...\n","Epoch: 3 \tTraining Loss: 2.096566 \tValidation Loss: 2.076458\n","Validation loss has decreased (2.078395 --> 2.076458).  Saving model ...\n","Epoch: 4 \tTraining Loss: 2.097621 \tValidation Loss: 2.075479\n","Validation loss has decreased (2.076458 --> 2.075479).  Saving model ...\n","Epoch: 5 \tTraining Loss: 2.103354 \tValidation Loss: 2.074626\n","Validation loss has decreased (2.075479 --> 2.074626).  Saving model ...\n","Epoch: 6 \tTraining Loss: 2.058797 \tValidation Loss: 2.074169\n","Validation loss has decreased (2.074626 --> 2.074169).  Saving model ...\n","Epoch: 7 \tTraining Loss: 2.083983 \tValidation Loss: 2.071946\n","Validation loss has decreased (2.074169 --> 2.071946).  Saving model ...\n","Epoch: 8 \tTraining Loss: 2.063109 \tValidation Loss: 2.071234\n","Validation loss has decreased (2.071946 --> 2.071234).  Saving model ...\n","Epoch: 9 \tTraining Loss: 2.085869 \tValidation Loss: 2.069657\n","Validation loss has decreased (2.071234 --> 2.069657).  Saving model ...\n","Epoch: 10 \tTraining Loss: 2.055332 \tValidation Loss: 2.067415\n","Validation loss has decreased (2.069657 --> 2.067415).  Saving model ...\n","Epoch: 11 \tTraining Loss: 2.055866 \tValidation Loss: 2.066240\n","Validation loss has decreased (2.067415 --> 2.066240).  Saving model ...\n","Epoch: 12 \tTraining Loss: 2.043275 \tValidation Loss: 2.064889\n","Validation loss has decreased (2.066240 --> 2.064889).  Saving model ...\n","Epoch: 13 \tTraining Loss: 2.080211 \tValidation Loss: 2.063570\n","Validation loss has decreased (2.064889 --> 2.063570).  Saving model ...\n","Epoch: 14 \tTraining Loss: 2.050244 \tValidation Loss: 2.062036\n","Validation loss has decreased (2.063570 --> 2.062036).  Saving model ...\n","Epoch: 15 \tTraining Loss: 2.047493 \tValidation Loss: 2.061231\n","Validation loss has decreased (2.062036 --> 2.061231).  Saving model ...\n","Epoch: 16 \tTraining Loss: 2.063290 \tValidation Loss: 2.059966\n","Validation loss has decreased (2.061231 --> 2.059966).  Saving model ...\n","Epoch: 17 \tTraining Loss: 2.028903 \tValidation Loss: 2.058389\n","Validation loss has decreased (2.059966 --> 2.058389).  Saving model ...\n","Epoch: 18 \tTraining Loss: 2.062438 \tValidation Loss: 2.057001\n","Validation loss has decreased (2.058389 --> 2.057001).  Saving model ...\n","Epoch: 19 \tTraining Loss: 2.053290 \tValidation Loss: 2.053801\n","Validation loss has decreased (2.057001 --> 2.053801).  Saving model ...\n","Epoch: 20 \tTraining Loss: 2.037796 \tValidation Loss: 2.053822\n","Epoch: 21 \tTraining Loss: 2.029123 \tValidation Loss: 2.053104\n","Validation loss has decreased (2.053801 --> 2.053104).  Saving model ...\n","Epoch: 22 \tTraining Loss: 2.024736 \tValidation Loss: 2.051566\n","Validation loss has decreased (2.053104 --> 2.051566).  Saving model ...\n","Epoch: 23 \tTraining Loss: 2.035895 \tValidation Loss: 2.051105\n","Validation loss has decreased (2.051566 --> 2.051105).  Saving model ...\n","Epoch: 24 \tTraining Loss: 2.019886 \tValidation Loss: 2.049285\n","Validation loss has decreased (2.051105 --> 2.049285).  Saving model ...\n","Epoch: 25 \tTraining Loss: 2.042186 \tValidation Loss: 2.047848\n","Validation loss has decreased (2.049285 --> 2.047848).  Saving model ...\n","Epoch: 26 \tTraining Loss: 2.015765 \tValidation Loss: 2.048218\n","Epoch: 27 \tTraining Loss: 2.029282 \tValidation Loss: 2.044880\n","Validation loss has decreased (2.047848 --> 2.044880).  Saving model ...\n","Epoch: 28 \tTraining Loss: 2.017503 \tValidation Loss: 2.043749\n","Validation loss has decreased (2.044880 --> 2.043749).  Saving model ...\n","Epoch: 29 \tTraining Loss: 2.015744 \tValidation Loss: 2.043072\n","Validation loss has decreased (2.043749 --> 2.043072).  Saving model ...\n","Epoch: 30 \tTraining Loss: 1.992096 \tValidation Loss: 2.039941\n","Validation loss has decreased (2.043072 --> 2.039941).  Saving model ...\n","Epoch: 31 \tTraining Loss: 2.026542 \tValidation Loss: 2.040094\n","Epoch: 32 \tTraining Loss: 2.001703 \tValidation Loss: 2.039976\n","Epoch: 33 \tTraining Loss: 2.000839 \tValidation Loss: 2.038303\n","Validation loss has decreased (2.039941 --> 2.038303).  Saving model ...\n","Epoch: 34 \tTraining Loss: 1.998794 \tValidation Loss: 2.036781\n","Validation loss has decreased (2.038303 --> 2.036781).  Saving model ...\n","Epoch: 35 \tTraining Loss: 2.016043 \tValidation Loss: 2.035105\n","Validation loss has decreased (2.036781 --> 2.035105).  Saving model ...\n","Epoch: 36 \tTraining Loss: 2.005209 \tValidation Loss: 2.033925\n","Validation loss has decreased (2.035105 --> 2.033925).  Saving model ...\n","Epoch: 37 \tTraining Loss: 2.036352 \tValidation Loss: 2.032757\n","Validation loss has decreased (2.033925 --> 2.032757).  Saving model ...\n","Epoch: 38 \tTraining Loss: 1.994723 \tValidation Loss: 2.032130\n","Validation loss has decreased (2.032757 --> 2.032130).  Saving model ...\n","Epoch: 39 \tTraining Loss: 1.983959 \tValidation Loss: 2.029718\n","Validation loss has decreased (2.032130 --> 2.029718).  Saving model ...\n","Epoch: 40 \tTraining Loss: 2.019701 \tValidation Loss: 2.027713\n","Validation loss has decreased (2.029718 --> 2.027713).  Saving model ...\n","Epoch: 41 \tTraining Loss: 1.987581 \tValidation Loss: 2.027513\n","Validation loss has decreased (2.027713 --> 2.027513).  Saving model ...\n","Epoch: 42 \tTraining Loss: 1.949386 \tValidation Loss: 2.025455\n","Validation loss has decreased (2.027513 --> 2.025455).  Saving model ...\n","Epoch: 43 \tTraining Loss: 1.976587 \tValidation Loss: 2.024629\n","Validation loss has decreased (2.025455 --> 2.024629).  Saving model ...\n","Epoch: 44 \tTraining Loss: 1.974693 \tValidation Loss: 2.021415\n","Validation loss has decreased (2.024629 --> 2.021415).  Saving model ...\n","Epoch: 45 \tTraining Loss: 1.992572 \tValidation Loss: 2.020658\n","Validation loss has decreased (2.021415 --> 2.020658).  Saving model ...\n","Epoch: 46 \tTraining Loss: 1.999300 \tValidation Loss: 2.021087\n","Epoch: 47 \tTraining Loss: 1.974794 \tValidation Loss: 2.020122\n","Validation loss has decreased (2.020658 --> 2.020122).  Saving model ...\n","Epoch: 48 \tTraining Loss: 1.971667 \tValidation Loss: 2.018278\n","Validation loss has decreased (2.020122 --> 2.018278).  Saving model ...\n","Epoch: 49 \tTraining Loss: 1.972488 \tValidation Loss: 2.016636\n","Validation loss has decreased (2.018278 --> 2.016636).  Saving model ...\n","Epoch: 50 \tTraining Loss: 1.953921 \tValidation Loss: 2.015000\n","Validation loss has decreased (2.016636 --> 2.015000).  Saving model ...\n","Epoch: 51 \tTraining Loss: 1.973602 \tValidation Loss: 2.014106\n","Validation loss has decreased (2.015000 --> 2.014106).  Saving model ...\n","Epoch: 52 \tTraining Loss: 1.951563 \tValidation Loss: 2.013022\n","Validation loss has decreased (2.014106 --> 2.013022).  Saving model ...\n","Epoch: 53 \tTraining Loss: 1.975489 \tValidation Loss: 2.010457\n","Validation loss has decreased (2.013022 --> 2.010457).  Saving model ...\n","Epoch: 54 \tTraining Loss: 1.956990 \tValidation Loss: 2.009957\n","Validation loss has decreased (2.010457 --> 2.009957).  Saving model ...\n","Epoch: 55 \tTraining Loss: 1.961659 \tValidation Loss: 2.008874\n","Validation loss has decreased (2.009957 --> 2.008874).  Saving model ...\n","Epoch: 56 \tTraining Loss: 1.954513 \tValidation Loss: 2.007203\n","Validation loss has decreased (2.008874 --> 2.007203).  Saving model ...\n","Epoch: 57 \tTraining Loss: 1.946499 \tValidation Loss: 2.005243\n","Validation loss has decreased (2.007203 --> 2.005243).  Saving model ...\n","Epoch: 58 \tTraining Loss: 1.967455 \tValidation Loss: 2.005543\n","Epoch: 59 \tTraining Loss: 1.947943 \tValidation Loss: 2.002749\n","Validation loss has decreased (2.005243 --> 2.002749).  Saving model ...\n","Epoch: 60 \tTraining Loss: 1.933728 \tValidation Loss: 2.002092\n","Validation loss has decreased (2.002749 --> 2.002092).  Saving model ...\n","Epoch: 61 \tTraining Loss: 1.952899 \tValidation Loss: 2.000443\n","Validation loss has decreased (2.002092 --> 2.000443).  Saving model ...\n","Epoch: 62 \tTraining Loss: 1.918604 \tValidation Loss: 1.999130\n","Validation loss has decreased (2.000443 --> 1.999130).  Saving model ...\n","Epoch: 63 \tTraining Loss: 1.946835 \tValidation Loss: 1.996454\n","Validation loss has decreased (1.999130 --> 1.996454).  Saving model ...\n","Epoch: 64 \tTraining Loss: 1.939169 \tValidation Loss: 1.996162\n","Validation loss has decreased (1.996454 --> 1.996162).  Saving model ...\n","Epoch: 65 \tTraining Loss: 1.901872 \tValidation Loss: 1.995818\n","Validation loss has decreased (1.996162 --> 1.995818).  Saving model ...\n","Epoch: 66 \tTraining Loss: 1.945934 \tValidation Loss: 1.995249\n","Validation loss has decreased (1.995818 --> 1.995249).  Saving model ...\n","Epoch: 67 \tTraining Loss: 1.922034 \tValidation Loss: 1.995601\n","Epoch: 68 \tTraining Loss: 1.911294 \tValidation Loss: 1.991397\n","Validation loss has decreased (1.995249 --> 1.991397).  Saving model ...\n","Epoch: 69 \tTraining Loss: 1.944742 \tValidation Loss: 1.991653\n","Epoch: 70 \tTraining Loss: 1.926474 \tValidation Loss: 1.990993\n","Validation loss has decreased (1.991397 --> 1.990993).  Saving model ...\n","Epoch: 71 \tTraining Loss: 1.938587 \tValidation Loss: 1.989772\n","Validation loss has decreased (1.990993 --> 1.989772).  Saving model ...\n","Epoch: 72 \tTraining Loss: 1.900185 \tValidation Loss: 1.987126\n","Validation loss has decreased (1.989772 --> 1.987126).  Saving model ...\n","Epoch: 73 \tTraining Loss: 1.939158 \tValidation Loss: 1.987108\n","Validation loss has decreased (1.987126 --> 1.987108).  Saving model ...\n","Epoch: 74 \tTraining Loss: 1.919433 \tValidation Loss: 1.985676\n","Validation loss has decreased (1.987108 --> 1.985676).  Saving model ...\n","Epoch: 75 \tTraining Loss: 1.918673 \tValidation Loss: 1.983324\n","Validation loss has decreased (1.985676 --> 1.983324).  Saving model ...\n","Epoch: 76 \tTraining Loss: 1.921847 \tValidation Loss: 1.980844\n","Validation loss has decreased (1.983324 --> 1.980844).  Saving model ...\n","Epoch: 77 \tTraining Loss: 1.885349 \tValidation Loss: 1.981704\n","Epoch: 78 \tTraining Loss: 1.883426 \tValidation Loss: 1.978043\n","Validation loss has decreased (1.980844 --> 1.978043).  Saving model ...\n","Epoch: 79 \tTraining Loss: 1.889608 \tValidation Loss: 1.978116\n","Epoch: 80 \tTraining Loss: 1.898039 \tValidation Loss: 1.974874\n","Validation loss has decreased (1.978043 --> 1.974874).  Saving model ...\n","Epoch: 81 \tTraining Loss: 1.883720 \tValidation Loss: 1.974933\n","Epoch: 82 \tTraining Loss: 1.881636 \tValidation Loss: 1.971769\n","Validation loss has decreased (1.974874 --> 1.971769).  Saving model ...\n","Epoch: 83 \tTraining Loss: 1.886015 \tValidation Loss: 1.973294\n","Epoch: 84 \tTraining Loss: 1.908039 \tValidation Loss: 1.972383\n","Epoch: 85 \tTraining Loss: 1.890585 \tValidation Loss: 1.970959\n","Validation loss has decreased (1.971769 --> 1.970959).  Saving model ...\n","Epoch: 86 \tTraining Loss: 1.903445 \tValidation Loss: 1.967358\n","Validation loss has decreased (1.970959 --> 1.967358).  Saving model ...\n","Epoch: 87 \tTraining Loss: 1.888777 \tValidation Loss: 1.965196\n","Validation loss has decreased (1.967358 --> 1.965196).  Saving model ...\n","Epoch: 88 \tTraining Loss: 1.892138 \tValidation Loss: 1.967400\n","Epoch: 89 \tTraining Loss: 1.877424 \tValidation Loss: 1.962794\n","Validation loss has decreased (1.965196 --> 1.962794).  Saving model ...\n","Epoch: 90 \tTraining Loss: 1.836631 \tValidation Loss: 1.964156\n","Epoch: 91 \tTraining Loss: 1.862010 \tValidation Loss: 1.962791\n","Validation loss has decreased (1.962794 --> 1.962791).  Saving model ...\n","Epoch: 92 \tTraining Loss: 1.875268 \tValidation Loss: 1.959514\n","Validation loss has decreased (1.962791 --> 1.959514).  Saving model ...\n","Epoch: 93 \tTraining Loss: 1.880318 \tValidation Loss: 1.957454\n","Validation loss has decreased (1.959514 --> 1.957454).  Saving model ...\n","Epoch: 94 \tTraining Loss: 1.833063 \tValidation Loss: 1.958968\n","Epoch: 95 \tTraining Loss: 1.851025 \tValidation Loss: 1.956310\n","Validation loss has decreased (1.957454 --> 1.956310).  Saving model ...\n","Epoch: 96 \tTraining Loss: 1.829345 \tValidation Loss: 1.955030\n","Validation loss has decreased (1.956310 --> 1.955030).  Saving model ...\n","Epoch: 97 \tTraining Loss: 1.882979 \tValidation Loss: 1.954229\n","Validation loss has decreased (1.955030 --> 1.954229).  Saving model ...\n","Epoch: 98 \tTraining Loss: 1.865974 \tValidation Loss: 1.955191\n","Epoch: 99 \tTraining Loss: 1.830909 \tValidation Loss: 1.950370\n","Validation loss has decreased (1.954229 --> 1.950370).  Saving model ...\n","Epoch: 100 \tTraining Loss: 1.847548 \tValidation Loss: 1.952049\n","Epoch: 101 \tTraining Loss: 1.853109 \tValidation Loss: 1.950941\n","Epoch: 102 \tTraining Loss: 1.831923 \tValidation Loss: 1.949602\n","Validation loss has decreased (1.950370 --> 1.949602).  Saving model ...\n","Epoch: 103 \tTraining Loss: 1.821297 \tValidation Loss: 1.946909\n","Validation loss has decreased (1.949602 --> 1.946909).  Saving model ...\n","Epoch: 104 \tTraining Loss: 1.838949 \tValidation Loss: 1.945480\n","Validation loss has decreased (1.946909 --> 1.945480).  Saving model ...\n","Epoch: 105 \tTraining Loss: 1.845302 \tValidation Loss: 1.945741\n","Epoch: 106 \tTraining Loss: 1.851083 \tValidation Loss: 1.940936\n","Validation loss has decreased (1.945480 --> 1.940936).  Saving model ...\n","Epoch: 107 \tTraining Loss: 1.797601 \tValidation Loss: 1.941727\n","Epoch: 108 \tTraining Loss: 1.837825 \tValidation Loss: 1.941381\n","Epoch: 109 \tTraining Loss: 1.808516 \tValidation Loss: 1.938288\n","Validation loss has decreased (1.940936 --> 1.938288).  Saving model ...\n","Epoch: 110 \tTraining Loss: 1.818000 \tValidation Loss: 1.937015\n","Validation loss has decreased (1.938288 --> 1.937015).  Saving model ...\n","Epoch: 111 \tTraining Loss: 1.792618 \tValidation Loss: 1.937800\n","Epoch: 112 \tTraining Loss: 1.805125 \tValidation Loss: 1.933367\n","Validation loss has decreased (1.937015 --> 1.933367).  Saving model ...\n","Epoch: 113 \tTraining Loss: 1.812240 \tValidation Loss: 1.933116\n","Validation loss has decreased (1.933367 --> 1.933116).  Saving model ...\n","Epoch: 114 \tTraining Loss: 1.820715 \tValidation Loss: 1.934510\n","Epoch: 115 \tTraining Loss: 1.801676 \tValidation Loss: 1.933983\n","Epoch: 116 \tTraining Loss: 1.777387 \tValidation Loss: 1.929606\n","Validation loss has decreased (1.933116 --> 1.929606).  Saving model ...\n","Epoch: 117 \tTraining Loss: 1.774615 \tValidation Loss: 1.927799\n","Validation loss has decreased (1.929606 --> 1.927799).  Saving model ...\n","Epoch: 118 \tTraining Loss: 1.805954 \tValidation Loss: 1.928455\n","Epoch: 119 \tTraining Loss: 1.781324 \tValidation Loss: 1.928271\n","Epoch: 120 \tTraining Loss: 1.785922 \tValidation Loss: 1.925201\n","Validation loss has decreased (1.927799 --> 1.925201).  Saving model ...\n","Epoch: 121 \tTraining Loss: 1.808848 \tValidation Loss: 1.922622\n","Validation loss has decreased (1.925201 --> 1.922622).  Saving model ...\n","Epoch: 122 \tTraining Loss: 1.790138 \tValidation Loss: 1.926216\n","Epoch: 123 \tTraining Loss: 1.767087 \tValidation Loss: 1.923167\n","Epoch: 124 \tTraining Loss: 1.789508 \tValidation Loss: 1.922096\n","Validation loss has decreased (1.922622 --> 1.922096).  Saving model ...\n","Epoch: 125 \tTraining Loss: 1.795887 \tValidation Loss: 1.921389\n","Validation loss has decreased (1.922096 --> 1.921389).  Saving model ...\n","Epoch: 126 \tTraining Loss: 1.757834 \tValidation Loss: 1.915883\n","Validation loss has decreased (1.921389 --> 1.915883).  Saving model ...\n","Epoch: 127 \tTraining Loss: 1.785512 \tValidation Loss: 1.916729\n","Epoch: 128 \tTraining Loss: 1.773818 \tValidation Loss: 1.915702\n","Validation loss has decreased (1.915883 --> 1.915702).  Saving model ...\n","Epoch: 129 \tTraining Loss: 1.766229 \tValidation Loss: 1.911904\n","Validation loss has decreased (1.915702 --> 1.911904).  Saving model ...\n","Epoch: 130 \tTraining Loss: 1.746259 \tValidation Loss: 1.914197\n","Epoch: 131 \tTraining Loss: 1.766473 \tValidation Loss: 1.912821\n","Epoch: 132 \tTraining Loss: 1.769391 \tValidation Loss: 1.908332\n","Validation loss has decreased (1.911904 --> 1.908332).  Saving model ...\n","Epoch: 133 \tTraining Loss: 1.784236 \tValidation Loss: 1.908564\n","Epoch: 134 \tTraining Loss: 1.740481 \tValidation Loss: 1.906343\n","Validation loss has decreased (1.908332 --> 1.906343).  Saving model ...\n","Epoch: 135 \tTraining Loss: 1.757310 \tValidation Loss: 1.906102\n","Validation loss has decreased (1.906343 --> 1.906102).  Saving model ...\n","Epoch: 136 \tTraining Loss: 1.741797 \tValidation Loss: 1.908281\n","Epoch: 137 \tTraining Loss: 1.754432 \tValidation Loss: 1.902639\n","Validation loss has decreased (1.906102 --> 1.902639).  Saving model ...\n","Epoch: 138 \tTraining Loss: 1.747591 \tValidation Loss: 1.904558\n","Epoch: 139 \tTraining Loss: 1.766064 \tValidation Loss: 1.899308\n","Validation loss has decreased (1.902639 --> 1.899308).  Saving model ...\n","Epoch: 140 \tTraining Loss: 1.729669 \tValidation Loss: 1.900766\n","Epoch: 141 \tTraining Loss: 1.720808 \tValidation Loss: 1.898498\n","Validation loss has decreased (1.899308 --> 1.898498).  Saving model ...\n","Epoch: 142 \tTraining Loss: 1.741825 \tValidation Loss: 1.899360\n","Epoch: 143 \tTraining Loss: 1.736039 \tValidation Loss: 1.897037\n","Validation loss has decreased (1.898498 --> 1.897037).  Saving model ...\n","Epoch: 144 \tTraining Loss: 1.723916 \tValidation Loss: 1.898621\n","Epoch: 145 \tTraining Loss: 1.737117 \tValidation Loss: 1.898296\n","Epoch: 146 \tTraining Loss: 1.740898 \tValidation Loss: 1.889895\n","Validation loss has decreased (1.897037 --> 1.889895).  Saving model ...\n","Epoch: 147 \tTraining Loss: 1.739269 \tValidation Loss: 1.891794\n","Epoch: 148 \tTraining Loss: 1.705845 \tValidation Loss: 1.889041\n","Validation loss has decreased (1.889895 --> 1.889041).  Saving model ...\n","Epoch: 149 \tTraining Loss: 1.697472 \tValidation Loss: 1.887684\n","Validation loss has decreased (1.889041 --> 1.887684).  Saving model ...\n","Epoch: 150 \tTraining Loss: 1.697114 \tValidation Loss: 1.884459\n","Validation loss has decreased (1.887684 --> 1.884459).  Saving model ...\n","Epoch: 151 \tTraining Loss: 1.723299 \tValidation Loss: 1.887466\n","Epoch: 152 \tTraining Loss: 1.731448 \tValidation Loss: 1.889410\n","Epoch: 153 \tTraining Loss: 1.710506 \tValidation Loss: 1.889266\n","Epoch: 154 \tTraining Loss: 1.688049 \tValidation Loss: 1.882462\n","Validation loss has decreased (1.884459 --> 1.882462).  Saving model ...\n","Epoch: 155 \tTraining Loss: 1.733855 \tValidation Loss: 1.878866\n","Validation loss has decreased (1.882462 --> 1.878866).  Saving model ...\n","Epoch: 156 \tTraining Loss: 1.708583 \tValidation Loss: 1.876529\n","Validation loss has decreased (1.878866 --> 1.876529).  Saving model ...\n","Epoch: 157 \tTraining Loss: 1.702156 \tValidation Loss: 1.876719\n","Epoch: 158 \tTraining Loss: 1.688580 \tValidation Loss: 1.874821\n","Validation loss has decreased (1.876529 --> 1.874821).  Saving model ...\n","Epoch: 159 \tTraining Loss: 1.695853 \tValidation Loss: 1.872573\n","Validation loss has decreased (1.874821 --> 1.872573).  Saving model ...\n","Epoch: 160 \tTraining Loss: 1.678007 \tValidation Loss: 1.867900\n","Validation loss has decreased (1.872573 --> 1.867900).  Saving model ...\n","Epoch: 161 \tTraining Loss: 1.678155 \tValidation Loss: 1.874130\n","Epoch: 162 \tTraining Loss: 1.701943 \tValidation Loss: 1.865184\n","Validation loss has decreased (1.867900 --> 1.865184).  Saving model ...\n","Epoch: 163 \tTraining Loss: 1.728725 \tValidation Loss: 1.868105\n","Epoch: 164 \tTraining Loss: 1.699211 \tValidation Loss: 1.871438\n","Epoch: 165 \tTraining Loss: 1.696430 \tValidation Loss: 1.869234\n","Epoch: 166 \tTraining Loss: 1.688137 \tValidation Loss: 1.862605\n","Validation loss has decreased (1.865184 --> 1.862605).  Saving model ...\n","Epoch: 167 \tTraining Loss: 1.680463 \tValidation Loss: 1.862226\n","Validation loss has decreased (1.862605 --> 1.862226).  Saving model ...\n","Epoch: 168 \tTraining Loss: 1.692960 \tValidation Loss: 1.863220\n","Epoch: 169 \tTraining Loss: 1.679886 \tValidation Loss: 1.860549\n","Validation loss has decreased (1.862226 --> 1.860549).  Saving model ...\n","Epoch: 170 \tTraining Loss: 1.678651 \tValidation Loss: 1.855178\n","Validation loss has decreased (1.860549 --> 1.855178).  Saving model ...\n","Epoch: 171 \tTraining Loss: 1.652543 \tValidation Loss: 1.856727\n","Epoch: 172 \tTraining Loss: 1.682317 \tValidation Loss: 1.856294\n","Epoch: 173 \tTraining Loss: 1.664839 \tValidation Loss: 1.857454\n","Epoch: 174 \tTraining Loss: 1.638485 \tValidation Loss: 1.857638\n","Epoch: 175 \tTraining Loss: 1.668622 \tValidation Loss: 1.853028\n","Validation loss has decreased (1.855178 --> 1.853028).  Saving model ...\n","Epoch: 176 \tTraining Loss: 1.647374 \tValidation Loss: 1.855238\n","Epoch: 177 \tTraining Loss: 1.660859 \tValidation Loss: 1.850196\n","Validation loss has decreased (1.853028 --> 1.850196).  Saving model ...\n","Epoch: 178 \tTraining Loss: 1.627407 \tValidation Loss: 1.849136\n","Validation loss has decreased (1.850196 --> 1.849136).  Saving model ...\n","Epoch: 179 \tTraining Loss: 1.691400 \tValidation Loss: 1.852313\n","Epoch: 180 \tTraining Loss: 1.643852 \tValidation Loss: 1.849290\n","Epoch: 181 \tTraining Loss: 1.634008 \tValidation Loss: 1.845680\n","Validation loss has decreased (1.849136 --> 1.845680).  Saving model ...\n","Epoch: 182 \tTraining Loss: 1.645658 \tValidation Loss: 1.844358\n","Validation loss has decreased (1.845680 --> 1.844358).  Saving model ...\n","Epoch: 183 \tTraining Loss: 1.639279 \tValidation Loss: 1.842978\n","Validation loss has decreased (1.844358 --> 1.842978).  Saving model ...\n","Epoch: 184 \tTraining Loss: 1.630126 \tValidation Loss: 1.841289\n","Validation loss has decreased (1.842978 --> 1.841289).  Saving model ...\n","Epoch: 185 \tTraining Loss: 1.643513 \tValidation Loss: 1.841665\n","Epoch: 186 \tTraining Loss: 1.607145 \tValidation Loss: 1.839033\n","Validation loss has decreased (1.841289 --> 1.839033).  Saving model ...\n","Epoch: 187 \tTraining Loss: 1.608938 \tValidation Loss: 1.836886\n","Validation loss has decreased (1.839033 --> 1.836886).  Saving model ...\n","Epoch: 188 \tTraining Loss: 1.614023 \tValidation Loss: 1.838218\n","Epoch: 189 \tTraining Loss: 1.577134 \tValidation Loss: 1.840443\n","Epoch: 190 \tTraining Loss: 1.623591 \tValidation Loss: 1.827828\n","Validation loss has decreased (1.836886 --> 1.827828).  Saving model ...\n","Epoch: 191 \tTraining Loss: 1.600801 \tValidation Loss: 1.833080\n","Epoch: 192 \tTraining Loss: 1.615176 \tValidation Loss: 1.829309\n","Epoch: 193 \tTraining Loss: 1.603693 \tValidation Loss: 1.832393\n","Epoch: 194 \tTraining Loss: 1.576819 \tValidation Loss: 1.829792\n","Epoch: 195 \tTraining Loss: 1.578498 \tValidation Loss: 1.830375\n","Epoch: 196 \tTraining Loss: 1.603580 \tValidation Loss: 1.827255\n","Validation loss has decreased (1.827828 --> 1.827255).  Saving model ...\n","Epoch: 197 \tTraining Loss: 1.651763 \tValidation Loss: 1.820340\n","Validation loss has decreased (1.827255 --> 1.820340).  Saving model ...\n","Epoch: 198 \tTraining Loss: 1.581363 \tValidation Loss: 1.824242\n","Epoch: 199 \tTraining Loss: 1.597498 \tValidation Loss: 1.822511\n","Epoch: 200 \tTraining Loss: 1.590852 \tValidation Loss: 1.816439\n","Validation loss has decreased (1.820340 --> 1.816439).  Saving model ...\n","Epoch: 201 \tTraining Loss: 1.633304 \tValidation Loss: 1.813386\n","Validation loss has decreased (1.816439 --> 1.813386).  Saving model ...\n","Epoch: 202 \tTraining Loss: 1.531387 \tValidation Loss: 1.818863\n","Epoch: 203 \tTraining Loss: 1.561945 \tValidation Loss: 1.820899\n","Epoch: 204 \tTraining Loss: 1.596546 \tValidation Loss: 1.813154\n","Validation loss has decreased (1.813386 --> 1.813154).  Saving model ...\n","Epoch: 205 \tTraining Loss: 1.578093 \tValidation Loss: 1.819937\n","Epoch: 206 \tTraining Loss: 1.572796 \tValidation Loss: 1.814942\n","Epoch: 207 \tTraining Loss: 1.566475 \tValidation Loss: 1.812425\n","Validation loss has decreased (1.813154 --> 1.812425).  Saving model ...\n","Epoch: 208 \tTraining Loss: 1.586510 \tValidation Loss: 1.814493\n","Epoch: 209 \tTraining Loss: 1.561735 \tValidation Loss: 1.811873\n","Validation loss has decreased (1.812425 --> 1.811873).  Saving model ...\n","Epoch: 210 \tTraining Loss: 1.555049 \tValidation Loss: 1.807072\n","Validation loss has decreased (1.811873 --> 1.807072).  Saving model ...\n","Epoch: 211 \tTraining Loss: 1.559110 \tValidation Loss: 1.800741\n","Validation loss has decreased (1.807072 --> 1.800741).  Saving model ...\n","Epoch: 212 \tTraining Loss: 1.565014 \tValidation Loss: 1.803259\n","Epoch: 213 \tTraining Loss: 1.547771 \tValidation Loss: 1.799935\n","Validation loss has decreased (1.800741 --> 1.799935).  Saving model ...\n","Epoch: 214 \tTraining Loss: 1.561276 \tValidation Loss: 1.800117\n","Epoch: 215 \tTraining Loss: 1.577314 \tValidation Loss: 1.798135\n","Validation loss has decreased (1.799935 --> 1.798135).  Saving model ...\n","Epoch: 216 \tTraining Loss: 1.553156 \tValidation Loss: 1.796514\n","Validation loss has decreased (1.798135 --> 1.796514).  Saving model ...\n","Epoch: 217 \tTraining Loss: 1.501364 \tValidation Loss: 1.797803\n","Epoch: 218 \tTraining Loss: 1.533723 \tValidation Loss: 1.797423\n","Epoch: 219 \tTraining Loss: 1.509501 \tValidation Loss: 1.796500\n","Validation loss has decreased (1.796514 --> 1.796500).  Saving model ...\n","Epoch: 220 \tTraining Loss: 1.527376 \tValidation Loss: 1.787069\n","Validation loss has decreased (1.796500 --> 1.787069).  Saving model ...\n","Epoch: 221 \tTraining Loss: 1.530080 \tValidation Loss: 1.791003\n","Epoch: 222 \tTraining Loss: 1.538030 \tValidation Loss: 1.791555\n","Epoch: 223 \tTraining Loss: 1.571857 \tValidation Loss: 1.790765\n","Epoch: 224 \tTraining Loss: 1.526026 \tValidation Loss: 1.780608\n","Validation loss has decreased (1.787069 --> 1.780608).  Saving model ...\n","Epoch: 225 \tTraining Loss: 1.538007 \tValidation Loss: 1.777285\n","Validation loss has decreased (1.780608 --> 1.777285).  Saving model ...\n","Epoch: 226 \tTraining Loss: 1.522169 \tValidation Loss: 1.779882\n","Epoch: 227 \tTraining Loss: 1.522335 \tValidation Loss: 1.781503\n","Epoch: 228 \tTraining Loss: 1.487384 \tValidation Loss: 1.781747\n","Epoch: 229 \tTraining Loss: 1.530012 \tValidation Loss: 1.771904\n","Validation loss has decreased (1.777285 --> 1.771904).  Saving model ...\n","Epoch: 230 \tTraining Loss: 1.505978 \tValidation Loss: 1.780460\n","Epoch: 231 \tTraining Loss: 1.526530 \tValidation Loss: 1.781328\n","Epoch: 232 \tTraining Loss: 1.496291 \tValidation Loss: 1.782805\n","Epoch: 233 \tTraining Loss: 1.490036 \tValidation Loss: 1.771969\n","Epoch: 234 \tTraining Loss: 1.492377 \tValidation Loss: 1.775692\n","Epoch: 235 \tTraining Loss: 1.485091 \tValidation Loss: 1.774630\n","Epoch: 236 \tTraining Loss: 1.509151 \tValidation Loss: 1.771642\n","Validation loss has decreased (1.771904 --> 1.771642).  Saving model ...\n","Epoch: 237 \tTraining Loss: 1.498000 \tValidation Loss: 1.768964\n","Validation loss has decreased (1.771642 --> 1.768964).  Saving model ...\n","Epoch: 238 \tTraining Loss: 1.501348 \tValidation Loss: 1.765947\n","Validation loss has decreased (1.768964 --> 1.765947).  Saving model ...\n","Epoch: 239 \tTraining Loss: 1.479517 \tValidation Loss: 1.764319\n","Validation loss has decreased (1.765947 --> 1.764319).  Saving model ...\n","Epoch: 240 \tTraining Loss: 1.484837 \tValidation Loss: 1.769508\n","Epoch: 241 \tTraining Loss: 1.524802 \tValidation Loss: 1.768107\n","Epoch: 242 \tTraining Loss: 1.511207 \tValidation Loss: 1.765345\n","Epoch: 243 \tTraining Loss: 1.484358 \tValidation Loss: 1.764239\n","Validation loss has decreased (1.764319 --> 1.764239).  Saving model ...\n","Epoch: 244 \tTraining Loss: 1.474617 \tValidation Loss: 1.758583\n","Validation loss has decreased (1.764239 --> 1.758583).  Saving model ...\n","Epoch: 245 \tTraining Loss: 1.468371 \tValidation Loss: 1.755573\n","Validation loss has decreased (1.758583 --> 1.755573).  Saving model ...\n","Epoch: 246 \tTraining Loss: 1.480107 \tValidation Loss: 1.751520\n","Validation loss has decreased (1.755573 --> 1.751520).  Saving model ...\n","Epoch: 247 \tTraining Loss: 1.472954 \tValidation Loss: 1.760948\n","Epoch: 248 \tTraining Loss: 1.485915 \tValidation Loss: 1.752800\n","Epoch: 249 \tTraining Loss: 1.436870 \tValidation Loss: 1.755707\n","Epoch: 250 \tTraining Loss: 1.442447 \tValidation Loss: 1.758017\n","Epoch: 251 \tTraining Loss: 1.436634 \tValidation Loss: 1.754763\n","Epoch: 252 \tTraining Loss: 1.470141 \tValidation Loss: 1.745058\n","Validation loss has decreased (1.751520 --> 1.745058).  Saving model ...\n","Epoch: 253 \tTraining Loss: 1.470687 \tValidation Loss: 1.740319\n","Validation loss has decreased (1.745058 --> 1.740319).  Saving model ...\n","Epoch: 254 \tTraining Loss: 1.440572 \tValidation Loss: 1.744550\n","Epoch: 255 \tTraining Loss: 1.481254 \tValidation Loss: 1.737317\n","Validation loss has decreased (1.740319 --> 1.737317).  Saving model ...\n","Epoch: 256 \tTraining Loss: 1.433810 \tValidation Loss: 1.743248\n","Epoch: 257 \tTraining Loss: 1.448844 \tValidation Loss: 1.742039\n","Epoch: 258 \tTraining Loss: 1.466161 \tValidation Loss: 1.740229\n","Epoch: 259 \tTraining Loss: 1.452818 \tValidation Loss: 1.736419\n","Validation loss has decreased (1.737317 --> 1.736419).  Saving model ...\n","Epoch: 260 \tTraining Loss: 1.429093 \tValidation Loss: 1.738752\n","Epoch: 261 \tTraining Loss: 1.421098 \tValidation Loss: 1.732710\n","Validation loss has decreased (1.736419 --> 1.732710).  Saving model ...\n","Epoch: 262 \tTraining Loss: 1.412655 \tValidation Loss: 1.733559\n","Epoch: 263 \tTraining Loss: 1.426613 \tValidation Loss: 1.734970\n","Epoch: 264 \tTraining Loss: 1.379680 \tValidation Loss: 1.738775\n","Epoch: 265 \tTraining Loss: 1.444934 \tValidation Loss: 1.727489\n","Validation loss has decreased (1.732710 --> 1.727489).  Saving model ...\n","Epoch: 266 \tTraining Loss: 1.417446 \tValidation Loss: 1.727646\n","Epoch: 267 \tTraining Loss: 1.401126 \tValidation Loss: 1.718771\n","Validation loss has decreased (1.727489 --> 1.718771).  Saving model ...\n","Epoch: 268 \tTraining Loss: 1.437882 \tValidation Loss: 1.717894\n","Validation loss has decreased (1.718771 --> 1.717894).  Saving model ...\n","Epoch: 269 \tTraining Loss: 1.414801 \tValidation Loss: 1.722270\n","Epoch: 270 \tTraining Loss: 1.392432 \tValidation Loss: 1.721539\n","Epoch: 271 \tTraining Loss: 1.398738 \tValidation Loss: 1.718551\n","Epoch: 272 \tTraining Loss: 1.398639 \tValidation Loss: 1.725477\n","Epoch: 273 \tTraining Loss: 1.412956 \tValidation Loss: 1.719384\n","Epoch: 274 \tTraining Loss: 1.434194 \tValidation Loss: 1.714026\n","Validation loss has decreased (1.717894 --> 1.714026).  Saving model ...\n","Epoch: 275 \tTraining Loss: 1.422816 \tValidation Loss: 1.719599\n","Epoch: 276 \tTraining Loss: 1.383276 \tValidation Loss: 1.719880\n","Epoch: 277 \tTraining Loss: 1.370218 \tValidation Loss: 1.719876\n","Epoch: 278 \tTraining Loss: 1.370613 \tValidation Loss: 1.711874\n","Validation loss has decreased (1.714026 --> 1.711874).  Saving model ...\n","Epoch: 279 \tTraining Loss: 1.394869 \tValidation Loss: 1.711536\n","Validation loss has decreased (1.711874 --> 1.711536).  Saving model ...\n","Epoch: 280 \tTraining Loss: 1.379460 \tValidation Loss: 1.707796\n","Validation loss has decreased (1.711536 --> 1.707796).  Saving model ...\n","Epoch: 281 \tTraining Loss: 1.399445 \tValidation Loss: 1.709555\n","Epoch: 282 \tTraining Loss: 1.364638 \tValidation Loss: 1.705489\n","Validation loss has decreased (1.707796 --> 1.705489).  Saving model ...\n","Epoch: 283 \tTraining Loss: 1.390676 \tValidation Loss: 1.704461\n","Validation loss has decreased (1.705489 --> 1.704461).  Saving model ...\n","Epoch: 284 \tTraining Loss: 1.364399 \tValidation Loss: 1.699345\n","Validation loss has decreased (1.704461 --> 1.699345).  Saving model ...\n","Epoch: 285 \tTraining Loss: 1.364825 \tValidation Loss: 1.695669\n","Validation loss has decreased (1.699345 --> 1.695669).  Saving model ...\n","Epoch: 286 \tTraining Loss: 1.364583 \tValidation Loss: 1.696989\n","Epoch: 287 \tTraining Loss: 1.375113 \tValidation Loss: 1.699140\n","Epoch: 288 \tTraining Loss: 1.356428 \tValidation Loss: 1.705477\n","Epoch: 289 \tTraining Loss: 1.394227 \tValidation Loss: 1.697989\n","Epoch: 290 \tTraining Loss: 1.331517 \tValidation Loss: 1.697041\n","Epoch: 291 \tTraining Loss: 1.295804 \tValidation Loss: 1.695620\n","Validation loss has decreased (1.695669 --> 1.695620).  Saving model ...\n","Epoch: 292 \tTraining Loss: 1.357352 \tValidation Loss: 1.686394\n","Validation loss has decreased (1.695620 --> 1.686394).  Saving model ...\n","Epoch: 293 \tTraining Loss: 1.353300 \tValidation Loss: 1.695792\n","Epoch: 294 \tTraining Loss: 1.334445 \tValidation Loss: 1.688973\n","Epoch: 295 \tTraining Loss: 1.311899 \tValidation Loss: 1.687669\n","Epoch: 296 \tTraining Loss: 1.334853 \tValidation Loss: 1.692658\n","Epoch: 297 \tTraining Loss: 1.322615 \tValidation Loss: 1.688945\n","Epoch: 298 \tTraining Loss: 1.357764 \tValidation Loss: 1.687720\n","Epoch: 299 \tTraining Loss: 1.323771 \tValidation Loss: 1.687467\n","Epoch: 300 \tTraining Loss: 1.352927 \tValidation Loss: 1.688674\n","Epoch: 301 \tTraining Loss: 1.322592 \tValidation Loss: 1.681751\n","Validation loss has decreased (1.686394 --> 1.681751).  Saving model ...\n","Epoch: 302 \tTraining Loss: 1.331892 \tValidation Loss: 1.674029\n","Validation loss has decreased (1.681751 --> 1.674029).  Saving model ...\n","Epoch: 303 \tTraining Loss: 1.291707 \tValidation Loss: 1.684133\n","Epoch: 304 \tTraining Loss: 1.301927 \tValidation Loss: 1.685866\n","Epoch: 305 \tTraining Loss: 1.293416 \tValidation Loss: 1.676540\n","Epoch: 306 \tTraining Loss: 1.318932 \tValidation Loss: 1.674212\n","Epoch: 307 \tTraining Loss: 1.322491 \tValidation Loss: 1.674587\n","Epoch: 308 \tTraining Loss: 1.307609 \tValidation Loss: 1.680659\n","Epoch: 309 \tTraining Loss: 1.310746 \tValidation Loss: 1.668886\n","Validation loss has decreased (1.674029 --> 1.668886).  Saving model ...\n","Epoch: 310 \tTraining Loss: 1.278624 \tValidation Loss: 1.668319\n","Validation loss has decreased (1.668886 --> 1.668319).  Saving model ...\n","Epoch: 311 \tTraining Loss: 1.283784 \tValidation Loss: 1.669798\n","Epoch: 312 \tTraining Loss: 1.279819 \tValidation Loss: 1.665584\n","Validation loss has decreased (1.668319 --> 1.665584).  Saving model ...\n","Epoch: 313 \tTraining Loss: 1.334032 \tValidation Loss: 1.662624\n","Validation loss has decreased (1.665584 --> 1.662624).  Saving model ...\n","Epoch: 314 \tTraining Loss: 1.331437 \tValidation Loss: 1.662595\n","Validation loss has decreased (1.662624 --> 1.662595).  Saving model ...\n","Epoch: 315 \tTraining Loss: 1.274042 \tValidation Loss: 1.666385\n","Epoch: 316 \tTraining Loss: 1.291815 \tValidation Loss: 1.658564\n","Validation loss has decreased (1.662595 --> 1.658564).  Saving model ...\n","Epoch: 317 \tTraining Loss: 1.284122 \tValidation Loss: 1.658288\n","Validation loss has decreased (1.658564 --> 1.658288).  Saving model ...\n","Epoch: 318 \tTraining Loss: 1.255365 \tValidation Loss: 1.662732\n","Epoch: 319 \tTraining Loss: 1.240381 \tValidation Loss: 1.653039\n","Validation loss has decreased (1.658288 --> 1.653039).  Saving model ...\n","Epoch: 320 \tTraining Loss: 1.234901 \tValidation Loss: 1.652996\n","Validation loss has decreased (1.653039 --> 1.652996).  Saving model ...\n","Epoch: 321 \tTraining Loss: 1.270732 \tValidation Loss: 1.646761\n","Validation loss has decreased (1.652996 --> 1.646761).  Saving model ...\n","Epoch: 322 \tTraining Loss: 1.232516 \tValidation Loss: 1.651526\n","Epoch: 323 \tTraining Loss: 1.215610 \tValidation Loss: 1.651535\n","Epoch: 324 \tTraining Loss: 1.262718 \tValidation Loss: 1.645762\n","Validation loss has decreased (1.646761 --> 1.645762).  Saving model ...\n","Epoch: 325 \tTraining Loss: 1.231427 \tValidation Loss: 1.647304\n","Epoch: 326 \tTraining Loss: 1.229367 \tValidation Loss: 1.647582\n","Epoch: 327 \tTraining Loss: 1.264477 \tValidation Loss: 1.645350\n","Validation loss has decreased (1.645762 --> 1.645350).  Saving model ...\n","Epoch: 328 \tTraining Loss: 1.248998 \tValidation Loss: 1.641583\n","Validation loss has decreased (1.645350 --> 1.641583).  Saving model ...\n","Epoch: 329 \tTraining Loss: 1.240242 \tValidation Loss: 1.639064\n","Validation loss has decreased (1.641583 --> 1.639064).  Saving model ...\n","Epoch: 330 \tTraining Loss: 1.233679 \tValidation Loss: 1.633030\n","Validation loss has decreased (1.639064 --> 1.633030).  Saving model ...\n","Epoch: 331 \tTraining Loss: 1.243347 \tValidation Loss: 1.630588\n","Validation loss has decreased (1.633030 --> 1.630588).  Saving model ...\n","Epoch: 332 \tTraining Loss: 1.234137 \tValidation Loss: 1.633347\n","Epoch: 333 \tTraining Loss: 1.250831 \tValidation Loss: 1.640924\n","Epoch: 334 \tTraining Loss: 1.205106 \tValidation Loss: 1.638559\n","Epoch: 335 \tTraining Loss: 1.228923 \tValidation Loss: 1.629841\n","Validation loss has decreased (1.630588 --> 1.629841).  Saving model ...\n","Epoch: 336 \tTraining Loss: 1.228162 \tValidation Loss: 1.623727\n","Validation loss has decreased (1.629841 --> 1.623727).  Saving model ...\n","Epoch: 337 \tTraining Loss: 1.221497 \tValidation Loss: 1.624913\n","Epoch: 338 \tTraining Loss: 1.214378 \tValidation Loss: 1.629083\n","Epoch: 339 \tTraining Loss: 1.212274 \tValidation Loss: 1.630775\n","Epoch: 340 \tTraining Loss: 1.226127 \tValidation Loss: 1.617607\n","Validation loss has decreased (1.623727 --> 1.617607).  Saving model ...\n","Epoch: 341 \tTraining Loss: 1.205656 \tValidation Loss: 1.621378\n","Epoch: 342 \tTraining Loss: 1.231244 \tValidation Loss: 1.609706\n","Validation loss has decreased (1.617607 --> 1.609706).  Saving model ...\n","Epoch: 343 \tTraining Loss: 1.193795 \tValidation Loss: 1.615481\n","Epoch: 344 \tTraining Loss: 1.190720 \tValidation Loss: 1.618384\n","Epoch: 345 \tTraining Loss: 1.220105 \tValidation Loss: 1.620245\n","Epoch: 346 \tTraining Loss: 1.177399 \tValidation Loss: 1.609503\n","Validation loss has decreased (1.609706 --> 1.609503).  Saving model ...\n","Epoch: 347 \tTraining Loss: 1.194571 \tValidation Loss: 1.613726\n","Epoch: 348 \tTraining Loss: 1.215867 \tValidation Loss: 1.617232\n","Epoch: 349 \tTraining Loss: 1.180921 \tValidation Loss: 1.613904\n","Epoch: 350 \tTraining Loss: 1.207090 \tValidation Loss: 1.594665\n","Validation loss has decreased (1.609503 --> 1.594665).  Saving model ...\n","Epoch: 351 \tTraining Loss: 1.170030 \tValidation Loss: 1.609174\n","Epoch: 352 \tTraining Loss: 1.191065 \tValidation Loss: 1.601018\n","Epoch: 353 \tTraining Loss: 1.168623 \tValidation Loss: 1.597839\n","Epoch: 354 \tTraining Loss: 1.195430 \tValidation Loss: 1.597850\n","Epoch: 355 \tTraining Loss: 1.169688 \tValidation Loss: 1.605373\n","Epoch: 356 \tTraining Loss: 1.169824 \tValidation Loss: 1.602465\n","Epoch: 357 \tTraining Loss: 1.143516 \tValidation Loss: 1.595538\n","Epoch: 358 \tTraining Loss: 1.166844 \tValidation Loss: 1.596828\n","Epoch: 359 \tTraining Loss: 1.129236 \tValidation Loss: 1.593892\n","Validation loss has decreased (1.594665 --> 1.593892).  Saving model ...\n","Epoch: 360 \tTraining Loss: 1.163302 \tValidation Loss: 1.592373\n","Validation loss has decreased (1.593892 --> 1.592373).  Saving model ...\n","Epoch: 361 \tTraining Loss: 1.185949 \tValidation Loss: 1.595884\n","Epoch: 362 \tTraining Loss: 1.145793 \tValidation Loss: 1.597253\n","Epoch: 363 \tTraining Loss: 1.117559 \tValidation Loss: 1.588624\n","Validation loss has decreased (1.592373 --> 1.588624).  Saving model ...\n","Epoch: 364 \tTraining Loss: 1.104129 \tValidation Loss: 1.589119\n","Epoch: 365 \tTraining Loss: 1.131565 \tValidation Loss: 1.592550\n","Epoch: 366 \tTraining Loss: 1.140996 \tValidation Loss: 1.588010\n","Validation loss has decreased (1.588624 --> 1.588010).  Saving model ...\n","Epoch: 367 \tTraining Loss: 1.118800 \tValidation Loss: 1.583966\n","Validation loss has decreased (1.588010 --> 1.583966).  Saving model ...\n","Epoch: 368 \tTraining Loss: 1.146516 \tValidation Loss: 1.582085\n","Validation loss has decreased (1.583966 --> 1.582085).  Saving model ...\n","Epoch: 369 \tTraining Loss: 1.162306 \tValidation Loss: 1.588359\n","Epoch: 370 \tTraining Loss: 1.110791 \tValidation Loss: 1.585692\n","Epoch: 371 \tTraining Loss: 1.131435 \tValidation Loss: 1.577943\n","Validation loss has decreased (1.582085 --> 1.577943).  Saving model ...\n","Epoch: 372 \tTraining Loss: 1.161615 \tValidation Loss: 1.568908\n","Validation loss has decreased (1.577943 --> 1.568908).  Saving model ...\n","Epoch: 373 \tTraining Loss: 1.112023 \tValidation Loss: 1.569611\n","Epoch: 374 \tTraining Loss: 1.122828 \tValidation Loss: 1.563701\n","Validation loss has decreased (1.568908 --> 1.563701).  Saving model ...\n","Epoch: 375 \tTraining Loss: 1.129458 \tValidation Loss: 1.567962\n","Epoch: 376 \tTraining Loss: 1.084405 \tValidation Loss: 1.568052\n","Epoch: 377 \tTraining Loss: 1.118235 \tValidation Loss: 1.571198\n","Epoch: 378 \tTraining Loss: 1.146101 \tValidation Loss: 1.566710\n","Epoch: 379 \tTraining Loss: 1.133397 \tValidation Loss: 1.569916\n","Epoch: 380 \tTraining Loss: 1.112069 \tValidation Loss: 1.566922\n","Epoch: 381 \tTraining Loss: 1.106471 \tValidation Loss: 1.565534\n","Epoch: 382 \tTraining Loss: 1.102639 \tValidation Loss: 1.561054\n","Validation loss has decreased (1.563701 --> 1.561054).  Saving model ...\n","Epoch: 383 \tTraining Loss: 1.111793 \tValidation Loss: 1.565111\n","Epoch: 384 \tTraining Loss: 1.066916 \tValidation Loss: 1.556324\n","Validation loss has decreased (1.561054 --> 1.556324).  Saving model ...\n","Epoch: 385 \tTraining Loss: 1.098920 \tValidation Loss: 1.563368\n","Epoch: 386 \tTraining Loss: 1.127492 \tValidation Loss: 1.552773\n","Validation loss has decreased (1.556324 --> 1.552773).  Saving model ...\n","Epoch: 387 \tTraining Loss: 1.084007 \tValidation Loss: 1.548752\n","Validation loss has decreased (1.552773 --> 1.548752).  Saving model ...\n","Epoch: 388 \tTraining Loss: 1.065969 \tValidation Loss: 1.559255\n","Epoch: 389 \tTraining Loss: 1.084805 \tValidation Loss: 1.554955\n","Epoch: 390 \tTraining Loss: 1.093107 \tValidation Loss: 1.554717\n","Epoch: 391 \tTraining Loss: 1.070051 \tValidation Loss: 1.553700\n","Epoch: 392 \tTraining Loss: 1.038219 \tValidation Loss: 1.554186\n","Epoch: 393 \tTraining Loss: 1.063272 \tValidation Loss: 1.546022\n","Validation loss has decreased (1.548752 --> 1.546022).  Saving model ...\n","Epoch: 394 \tTraining Loss: 1.055059 \tValidation Loss: 1.538166\n","Validation loss has decreased (1.546022 --> 1.538166).  Saving model ...\n","Epoch: 395 \tTraining Loss: 1.042066 \tValidation Loss: 1.549558\n","Epoch: 396 \tTraining Loss: 1.058125 \tValidation Loss: 1.548358\n","Epoch: 397 \tTraining Loss: 1.081367 \tValidation Loss: 1.533962\n","Validation loss has decreased (1.538166 --> 1.533962).  Saving model ...\n","Epoch: 398 \tTraining Loss: 1.025084 \tValidation Loss: 1.546606\n","Epoch: 399 \tTraining Loss: 1.011860 \tValidation Loss: 1.543815\n","Epoch: 400 \tTraining Loss: 1.085346 \tValidation Loss: 1.537348\n","Epoch: 401 \tTraining Loss: 1.049224 \tValidation Loss: 1.533220\n","Validation loss has decreased (1.533962 --> 1.533220).  Saving model ...\n","Epoch: 402 \tTraining Loss: 1.042434 \tValidation Loss: 1.525424\n","Validation loss has decreased (1.533220 --> 1.525424).  Saving model ...\n","Epoch: 403 \tTraining Loss: 1.011415 \tValidation Loss: 1.530859\n","Epoch: 404 \tTraining Loss: 1.020075 \tValidation Loss: 1.541393\n","Epoch: 405 \tTraining Loss: 1.069068 \tValidation Loss: 1.526466\n","Epoch: 406 \tTraining Loss: 1.022490 \tValidation Loss: 1.525938\n","Epoch: 407 \tTraining Loss: 1.020725 \tValidation Loss: 1.533104\n","Epoch: 408 \tTraining Loss: 1.018857 \tValidation Loss: 1.527575\n","Epoch: 409 \tTraining Loss: 1.046530 \tValidation Loss: 1.518797\n","Validation loss has decreased (1.525424 --> 1.518797).  Saving model ...\n","Epoch: 410 \tTraining Loss: 1.040034 \tValidation Loss: 1.513949\n","Validation loss has decreased (1.518797 --> 1.513949).  Saving model ...\n","Epoch: 411 \tTraining Loss: 1.009520 \tValidation Loss: 1.525730\n","Epoch: 412 \tTraining Loss: 1.006674 \tValidation Loss: 1.516904\n","Epoch: 413 \tTraining Loss: 0.973735 \tValidation Loss: 1.520048\n","Epoch: 414 \tTraining Loss: 1.022210 \tValidation Loss: 1.513697\n","Validation loss has decreased (1.513949 --> 1.513697).  Saving model ...\n","Epoch: 415 \tTraining Loss: 1.002144 \tValidation Loss: 1.513314\n","Validation loss has decreased (1.513697 --> 1.513314).  Saving model ...\n","Epoch: 416 \tTraining Loss: 1.011901 \tValidation Loss: 1.504153\n","Validation loss has decreased (1.513314 --> 1.504153).  Saving model ...\n","Epoch: 417 \tTraining Loss: 1.008767 \tValidation Loss: 1.520063\n","Epoch: 418 \tTraining Loss: 0.975755 \tValidation Loss: 1.513085\n","Epoch: 419 \tTraining Loss: 1.015724 \tValidation Loss: 1.516282\n","Epoch: 420 \tTraining Loss: 0.993067 \tValidation Loss: 1.512674\n","Epoch: 421 \tTraining Loss: 0.981180 \tValidation Loss: 1.506456\n","Epoch: 422 \tTraining Loss: 0.966329 \tValidation Loss: 1.507124\n","Epoch: 423 \tTraining Loss: 0.988712 \tValidation Loss: 1.504586\n","Epoch: 424 \tTraining Loss: 0.958688 \tValidation Loss: 1.500306\n","Validation loss has decreased (1.504153 --> 1.500306).  Saving model ...\n","Epoch: 425 \tTraining Loss: 0.976466 \tValidation Loss: 1.501821\n","Epoch: 426 \tTraining Loss: 0.993397 \tValidation Loss: 1.511493\n","Epoch: 427 \tTraining Loss: 0.996989 \tValidation Loss: 1.493920\n","Validation loss has decreased (1.500306 --> 1.493920).  Saving model ...\n","Epoch: 428 \tTraining Loss: 0.983288 \tValidation Loss: 1.508472\n","Epoch: 429 \tTraining Loss: 0.958289 \tValidation Loss: 1.496366\n","Epoch: 430 \tTraining Loss: 0.926601 \tValidation Loss: 1.497308\n","Epoch: 431 \tTraining Loss: 0.959970 \tValidation Loss: 1.501111\n","Epoch: 432 \tTraining Loss: 0.958496 \tValidation Loss: 1.502410\n","Epoch: 433 \tTraining Loss: 0.941849 \tValidation Loss: 1.491278\n","Validation loss has decreased (1.493920 --> 1.491278).  Saving model ...\n","Epoch: 434 \tTraining Loss: 0.946375 \tValidation Loss: 1.491544\n","Epoch: 435 \tTraining Loss: 0.960545 \tValidation Loss: 1.484298\n","Validation loss has decreased (1.491278 --> 1.484298).  Saving model ...\n","Epoch: 436 \tTraining Loss: 0.956067 \tValidation Loss: 1.494651\n","Epoch: 437 \tTraining Loss: 0.953290 \tValidation Loss: 1.486366\n","Epoch: 438 \tTraining Loss: 0.998903 \tValidation Loss: 1.488881\n","Epoch: 439 \tTraining Loss: 0.955271 \tValidation Loss: 1.482347\n","Validation loss has decreased (1.484298 --> 1.482347).  Saving model ...\n","Epoch: 440 \tTraining Loss: 0.956122 \tValidation Loss: 1.481524\n","Validation loss has decreased (1.482347 --> 1.481524).  Saving model ...\n","Epoch: 441 \tTraining Loss: 0.930551 \tValidation Loss: 1.484696\n","Epoch: 442 \tTraining Loss: 0.990671 \tValidation Loss: 1.484100\n","Epoch: 443 \tTraining Loss: 0.944279 \tValidation Loss: 1.478626\n","Validation loss has decreased (1.481524 --> 1.478626).  Saving model ...\n","Epoch: 444 \tTraining Loss: 0.936132 \tValidation Loss: 1.485034\n","Epoch: 445 \tTraining Loss: 0.903051 \tValidation Loss: 1.483294\n","Epoch: 446 \tTraining Loss: 0.919487 \tValidation Loss: 1.473776\n","Validation loss has decreased (1.478626 --> 1.473776).  Saving model ...\n","Epoch: 447 \tTraining Loss: 0.946416 \tValidation Loss: 1.459799\n","Validation loss has decreased (1.473776 --> 1.459799).  Saving model ...\n","Epoch: 448 \tTraining Loss: 0.929871 \tValidation Loss: 1.467607\n","Epoch: 449 \tTraining Loss: 0.958500 \tValidation Loss: 1.463283\n","Epoch: 450 \tTraining Loss: 0.923707 \tValidation Loss: 1.465450\n","Epoch: 451 \tTraining Loss: 0.884887 \tValidation Loss: 1.462745\n","Epoch: 452 \tTraining Loss: 0.905813 \tValidation Loss: 1.463439\n","Epoch: 453 \tTraining Loss: 0.924672 \tValidation Loss: 1.455219\n","Validation loss has decreased (1.459799 --> 1.455219).  Saving model ...\n","Epoch: 454 \tTraining Loss: 0.879591 \tValidation Loss: 1.466937\n","Epoch: 455 \tTraining Loss: 0.922831 \tValidation Loss: 1.465186\n","Epoch: 456 \tTraining Loss: 0.935553 \tValidation Loss: 1.452590\n","Validation loss has decreased (1.455219 --> 1.452590).  Saving model ...\n","Epoch: 457 \tTraining Loss: 0.888236 \tValidation Loss: 1.461338\n","Epoch: 458 \tTraining Loss: 0.898176 \tValidation Loss: 1.463157\n","Epoch: 459 \tTraining Loss: 0.915827 \tValidation Loss: 1.457609\n","Epoch: 460 \tTraining Loss: 0.870514 \tValidation Loss: 1.445566\n","Validation loss has decreased (1.452590 --> 1.445566).  Saving model ...\n","Epoch: 461 \tTraining Loss: 0.923899 \tValidation Loss: 1.462926\n","Epoch: 462 \tTraining Loss: 0.872562 \tValidation Loss: 1.453345\n","Epoch: 463 \tTraining Loss: 0.874057 \tValidation Loss: 1.455354\n","Epoch: 464 \tTraining Loss: 0.882398 \tValidation Loss: 1.443117\n","Validation loss has decreased (1.445566 --> 1.443117).  Saving model ...\n","Epoch: 465 \tTraining Loss: 0.876825 \tValidation Loss: 1.453974\n","Epoch: 466 \tTraining Loss: 0.909934 \tValidation Loss: 1.460692\n","Epoch: 467 \tTraining Loss: 0.864564 \tValidation Loss: 1.446850\n","Epoch: 468 \tTraining Loss: 0.895151 \tValidation Loss: 1.451280\n","Epoch: 469 \tTraining Loss: 0.862436 \tValidation Loss: 1.451260\n","Epoch: 470 \tTraining Loss: 0.894359 \tValidation Loss: 1.441759\n","Validation loss has decreased (1.443117 --> 1.441759).  Saving model ...\n","Epoch: 471 \tTraining Loss: 0.874673 \tValidation Loss: 1.431180\n","Validation loss has decreased (1.441759 --> 1.431180).  Saving model ...\n","Epoch: 472 \tTraining Loss: 0.896808 \tValidation Loss: 1.430287\n","Validation loss has decreased (1.431180 --> 1.430287).  Saving model ...\n","Epoch: 473 \tTraining Loss: 0.879146 \tValidation Loss: 1.439682\n","Epoch: 474 \tTraining Loss: 0.891545 \tValidation Loss: 1.429630\n","Validation loss has decreased (1.430287 --> 1.429630).  Saving model ...\n","Epoch: 475 \tTraining Loss: 0.867995 \tValidation Loss: 1.438245\n","Epoch: 476 \tTraining Loss: 0.906428 \tValidation Loss: 1.425557\n","Validation loss has decreased (1.429630 --> 1.425557).  Saving model ...\n","Epoch: 477 \tTraining Loss: 0.862686 \tValidation Loss: 1.429323\n","Epoch: 478 \tTraining Loss: 0.819500 \tValidation Loss: 1.431921\n","Epoch: 479 \tTraining Loss: 0.876028 \tValidation Loss: 1.432984\n","Epoch: 480 \tTraining Loss: 0.873809 \tValidation Loss: 1.433465\n","Epoch: 481 \tTraining Loss: 0.846542 \tValidation Loss: 1.420460\n","Validation loss has decreased (1.425557 --> 1.420460).  Saving model ...\n","Epoch: 482 \tTraining Loss: 0.854261 \tValidation Loss: 1.437782\n","Epoch: 483 \tTraining Loss: 0.880451 \tValidation Loss: 1.423158\n","Epoch: 484 \tTraining Loss: 0.853806 \tValidation Loss: 1.426993\n","Epoch: 485 \tTraining Loss: 0.831211 \tValidation Loss: 1.422987\n","Epoch: 486 \tTraining Loss: 0.820233 \tValidation Loss: 1.417981\n","Validation loss has decreased (1.420460 --> 1.417981).  Saving model ...\n","Epoch: 487 \tTraining Loss: 0.846813 \tValidation Loss: 1.412382\n","Validation loss has decreased (1.417981 --> 1.412382).  Saving model ...\n","Epoch: 488 \tTraining Loss: 0.857062 \tValidation Loss: 1.431587\n","Epoch: 489 \tTraining Loss: 0.786336 \tValidation Loss: 1.428855\n","Epoch: 490 \tTraining Loss: 0.786229 \tValidation Loss: 1.425967\n","Epoch: 491 \tTraining Loss: 0.806565 \tValidation Loss: 1.410477\n","Validation loss has decreased (1.412382 --> 1.410477).  Saving model ...\n","Epoch: 492 \tTraining Loss: 0.830907 \tValidation Loss: 1.407383\n","Validation loss has decreased (1.410477 --> 1.407383).  Saving model ...\n","Epoch: 493 \tTraining Loss: 0.853571 \tValidation Loss: 1.411370\n","Epoch: 494 \tTraining Loss: 0.839855 \tValidation Loss: 1.410353\n","Epoch: 495 \tTraining Loss: 0.855797 \tValidation Loss: 1.405334\n","Validation loss has decreased (1.407383 --> 1.405334).  Saving model ...\n","Epoch: 496 \tTraining Loss: 0.763482 \tValidation Loss: 1.401803\n","Validation loss has decreased (1.405334 --> 1.401803).  Saving model ...\n","Epoch: 497 \tTraining Loss: 0.802627 \tValidation Loss: 1.412282\n","Epoch: 498 \tTraining Loss: 0.830401 \tValidation Loss: 1.400222\n","Validation loss has decreased (1.401803 --> 1.400222).  Saving model ...\n","Epoch: 499 \tTraining Loss: 0.791276 \tValidation Loss: 1.411123\n","Epoch: 500 \tTraining Loss: 0.814346 \tValidation Loss: 1.392704\n","Validation loss has decreased (1.400222 --> 1.392704).  Saving model ...\n","Epoch: 501 \tTraining Loss: 0.801446 \tValidation Loss: 1.393555\n","Epoch: 502 \tTraining Loss: 0.806924 \tValidation Loss: 1.412781\n","Epoch: 503 \tTraining Loss: 0.822356 \tValidation Loss: 1.391853\n","Validation loss has decreased (1.392704 --> 1.391853).  Saving model ...\n","Epoch: 504 \tTraining Loss: 0.809238 \tValidation Loss: 1.387286\n","Validation loss has decreased (1.391853 --> 1.387286).  Saving model ...\n","Epoch: 505 \tTraining Loss: 0.785166 \tValidation Loss: 1.392834\n","Epoch: 506 \tTraining Loss: 0.751110 \tValidation Loss: 1.381245\n","Validation loss has decreased (1.387286 --> 1.381245).  Saving model ...\n","Epoch: 507 \tTraining Loss: 0.851954 \tValidation Loss: 1.394783\n","Epoch: 508 \tTraining Loss: 0.811067 \tValidation Loss: 1.389147\n","Epoch: 509 \tTraining Loss: 0.759976 \tValidation Loss: 1.385099\n","Epoch: 510 \tTraining Loss: 0.759060 \tValidation Loss: 1.392018\n","Epoch: 511 \tTraining Loss: 0.779271 \tValidation Loss: 1.394381\n","Epoch: 512 \tTraining Loss: 0.783000 \tValidation Loss: 1.393358\n","Epoch: 513 \tTraining Loss: 0.761842 \tValidation Loss: 1.375132\n","Validation loss has decreased (1.381245 --> 1.375132).  Saving model ...\n","Epoch: 514 \tTraining Loss: 0.841470 \tValidation Loss: 1.377169\n","Epoch: 515 \tTraining Loss: 0.761244 \tValidation Loss: 1.382033\n","Epoch: 516 \tTraining Loss: 0.789751 \tValidation Loss: 1.377943\n","Epoch: 517 \tTraining Loss: 0.806043 \tValidation Loss: 1.374477\n","Validation loss has decreased (1.375132 --> 1.374477).  Saving model ...\n","Epoch: 518 \tTraining Loss: 0.731402 \tValidation Loss: 1.384020\n","Epoch: 519 \tTraining Loss: 0.771957 \tValidation Loss: 1.377169\n","Epoch: 520 \tTraining Loss: 0.791603 \tValidation Loss: 1.385909\n","Epoch: 521 \tTraining Loss: 0.737665 \tValidation Loss: 1.372381\n","Validation loss has decreased (1.374477 --> 1.372381).  Saving model ...\n","Epoch: 522 \tTraining Loss: 0.746634 \tValidation Loss: 1.376800\n","Epoch: 523 \tTraining Loss: 0.751966 \tValidation Loss: 1.376853\n","Epoch: 524 \tTraining Loss: 0.782421 \tValidation Loss: 1.377376\n","Epoch: 525 \tTraining Loss: 0.773789 \tValidation Loss: 1.366174\n","Validation loss has decreased (1.372381 --> 1.366174).  Saving model ...\n","Epoch: 526 \tTraining Loss: 0.769809 \tValidation Loss: 1.363622\n","Validation loss has decreased (1.366174 --> 1.363622).  Saving model ...\n","Epoch: 527 \tTraining Loss: 0.768931 \tValidation Loss: 1.375116\n","Epoch: 528 \tTraining Loss: 0.735576 \tValidation Loss: 1.368884\n","Epoch: 529 \tTraining Loss: 0.748435 \tValidation Loss: 1.363392\n","Validation loss has decreased (1.363622 --> 1.363392).  Saving model ...\n","Epoch: 530 \tTraining Loss: 0.740637 \tValidation Loss: 1.362292\n","Validation loss has decreased (1.363392 --> 1.362292).  Saving model ...\n","Epoch: 531 \tTraining Loss: 0.732935 \tValidation Loss: 1.362341\n","Epoch: 532 \tTraining Loss: 0.708957 \tValidation Loss: 1.361566\n","Validation loss has decreased (1.362292 --> 1.361566).  Saving model ...\n","Epoch: 533 \tTraining Loss: 0.753055 \tValidation Loss: 1.357066\n","Validation loss has decreased (1.361566 --> 1.357066).  Saving model ...\n","Epoch: 534 \tTraining Loss: 0.726400 \tValidation Loss: 1.345826\n","Validation loss has decreased (1.357066 --> 1.345826).  Saving model ...\n","Epoch: 535 \tTraining Loss: 0.746635 \tValidation Loss: 1.337507\n","Validation loss has decreased (1.345826 --> 1.337507).  Saving model ...\n","Epoch: 536 \tTraining Loss: 0.700531 \tValidation Loss: 1.352858\n","Epoch: 537 \tTraining Loss: 0.706043 \tValidation Loss: 1.361907\n","Epoch: 538 \tTraining Loss: 0.692409 \tValidation Loss: 1.364810\n","Epoch: 539 \tTraining Loss: 0.730740 \tValidation Loss: 1.352913\n","Epoch: 540 \tTraining Loss: 0.736552 \tValidation Loss: 1.354532\n","Epoch: 541 \tTraining Loss: 0.736848 \tValidation Loss: 1.357607\n","Epoch: 542 \tTraining Loss: 0.696549 \tValidation Loss: 1.357245\n","Epoch: 543 \tTraining Loss: 0.657565 \tValidation Loss: 1.352849\n","Epoch: 544 \tTraining Loss: 0.680883 \tValidation Loss: 1.350422\n","Epoch: 545 \tTraining Loss: 0.726695 \tValidation Loss: 1.348727\n","Epoch: 546 \tTraining Loss: 0.679940 \tValidation Loss: 1.352073\n","Epoch: 547 \tTraining Loss: 0.743503 \tValidation Loss: 1.349841\n","Epoch: 548 \tTraining Loss: 0.718354 \tValidation Loss: 1.350524\n","Epoch: 549 \tTraining Loss: 0.680582 \tValidation Loss: 1.361616\n","Epoch: 550 \tTraining Loss: 0.690344 \tValidation Loss: 1.347847\n","Epoch: 551 \tTraining Loss: 0.703695 \tValidation Loss: 1.352990\n","Epoch: 552 \tTraining Loss: 0.674355 \tValidation Loss: 1.351020\n","Epoch: 553 \tTraining Loss: 0.708348 \tValidation Loss: 1.354070\n","Epoch: 554 \tTraining Loss: 0.687461 \tValidation Loss: 1.344998\n","Epoch: 555 \tTraining Loss: 0.666499 \tValidation Loss: 1.340778\n","Epoch: 556 \tTraining Loss: 0.725962 \tValidation Loss: 1.332764\n","Validation loss has decreased (1.337507 --> 1.332764).  Saving model ...\n","Epoch: 557 \tTraining Loss: 0.672931 \tValidation Loss: 1.329863\n","Validation loss has decreased (1.332764 --> 1.329863).  Saving model ...\n","Epoch: 558 \tTraining Loss: 0.662146 \tValidation Loss: 1.337196\n","Epoch: 559 \tTraining Loss: 0.711020 \tValidation Loss: 1.340292\n","Epoch: 560 \tTraining Loss: 0.638357 \tValidation Loss: 1.347911\n","Epoch: 561 \tTraining Loss: 0.689257 \tValidation Loss: 1.349185\n","Epoch: 562 \tTraining Loss: 0.687533 \tValidation Loss: 1.319025\n","Validation loss has decreased (1.329863 --> 1.319025).  Saving model ...\n","Epoch: 563 \tTraining Loss: 0.693385 \tValidation Loss: 1.335472\n","Epoch: 564 \tTraining Loss: 0.670716 \tValidation Loss: 1.343155\n","Epoch: 565 \tTraining Loss: 0.679622 \tValidation Loss: 1.320366\n","Epoch: 566 \tTraining Loss: 0.677120 \tValidation Loss: 1.328364\n","Epoch: 567 \tTraining Loss: 0.674263 \tValidation Loss: 1.322019\n","Epoch: 568 \tTraining Loss: 0.631739 \tValidation Loss: 1.319783\n","Epoch: 569 \tTraining Loss: 0.649119 \tValidation Loss: 1.313380\n","Validation loss has decreased (1.319025 --> 1.313380).  Saving model ...\n","Epoch: 570 \tTraining Loss: 0.680999 \tValidation Loss: 1.321216\n","Epoch: 571 \tTraining Loss: 0.707422 \tValidation Loss: 1.325219\n","Epoch: 572 \tTraining Loss: 0.654204 \tValidation Loss: 1.312818\n","Validation loss has decreased (1.313380 --> 1.312818).  Saving model ...\n","Epoch: 573 \tTraining Loss: 0.626697 \tValidation Loss: 1.313409\n","Epoch: 574 \tTraining Loss: 0.665383 \tValidation Loss: 1.312009\n","Validation loss has decreased (1.312818 --> 1.312009).  Saving model ...\n","Epoch: 575 \tTraining Loss: 0.637389 \tValidation Loss: 1.331430\n","Epoch: 576 \tTraining Loss: 0.637303 \tValidation Loss: 1.327803\n","Epoch: 577 \tTraining Loss: 0.623548 \tValidation Loss: 1.314402\n","Epoch: 578 \tTraining Loss: 0.660717 \tValidation Loss: 1.311044\n","Validation loss has decreased (1.312009 --> 1.311044).  Saving model ...\n","Epoch: 579 \tTraining Loss: 0.624865 \tValidation Loss: 1.310624\n","Validation loss has decreased (1.311044 --> 1.310624).  Saving model ...\n","Epoch: 580 \tTraining Loss: 0.617002 \tValidation Loss: 1.311552\n","Epoch: 581 \tTraining Loss: 0.661773 \tValidation Loss: 1.300263\n","Validation loss has decreased (1.310624 --> 1.300263).  Saving model ...\n","Epoch: 582 \tTraining Loss: 0.641474 \tValidation Loss: 1.309112\n","Epoch: 583 \tTraining Loss: 0.648683 \tValidation Loss: 1.303762\n","Epoch: 584 \tTraining Loss: 0.637016 \tValidation Loss: 1.295903\n","Validation loss has decreased (1.300263 --> 1.295903).  Saving model ...\n","Epoch: 585 \tTraining Loss: 0.638945 \tValidation Loss: 1.303348\n","Epoch: 586 \tTraining Loss: 0.613329 \tValidation Loss: 1.313363\n","Epoch: 587 \tTraining Loss: 0.625206 \tValidation Loss: 1.305230\n","Epoch: 588 \tTraining Loss: 0.626787 \tValidation Loss: 1.304882\n","Epoch: 589 \tTraining Loss: 0.686123 \tValidation Loss: 1.296344\n","Epoch: 590 \tTraining Loss: 0.643246 \tValidation Loss: 1.295580\n","Validation loss has decreased (1.295903 --> 1.295580).  Saving model ...\n","Epoch: 591 \tTraining Loss: 0.585010 \tValidation Loss: 1.295750\n","Epoch: 592 \tTraining Loss: 0.594985 \tValidation Loss: 1.303822\n","Epoch: 593 \tTraining Loss: 0.615826 \tValidation Loss: 1.297257\n","Epoch: 594 \tTraining Loss: 0.623451 \tValidation Loss: 1.297068\n","Epoch: 595 \tTraining Loss: 0.589171 \tValidation Loss: 1.283151\n","Validation loss has decreased (1.295580 --> 1.283151).  Saving model ...\n","Epoch: 596 \tTraining Loss: 0.619475 \tValidation Loss: 1.300968\n","Epoch: 597 \tTraining Loss: 0.613304 \tValidation Loss: 1.288579\n","Epoch: 598 \tTraining Loss: 0.582282 \tValidation Loss: 1.296788\n","Epoch: 599 \tTraining Loss: 0.607656 \tValidation Loss: 1.286816\n","Epoch: 600 \tTraining Loss: 0.631368 \tValidation Loss: 1.288826\n","Epoch: 601 \tTraining Loss: 0.593417 \tValidation Loss: 1.290268\n","Epoch: 602 \tTraining Loss: 0.607571 \tValidation Loss: 1.288219\n","Epoch: 603 \tTraining Loss: 0.579004 \tValidation Loss: 1.300840\n","Epoch: 604 \tTraining Loss: 0.601052 \tValidation Loss: 1.292832\n","Epoch: 605 \tTraining Loss: 0.603820 \tValidation Loss: 1.300674\n","Epoch: 606 \tTraining Loss: 0.643947 \tValidation Loss: 1.285927\n","Epoch: 607 \tTraining Loss: 0.584193 \tValidation Loss: 1.279844\n","Validation loss has decreased (1.283151 --> 1.279844).  Saving model ...\n","Epoch: 608 \tTraining Loss: 0.606387 \tValidation Loss: 1.282849\n","Epoch: 609 \tTraining Loss: 0.547184 \tValidation Loss: 1.288536\n","Epoch: 610 \tTraining Loss: 0.576122 \tValidation Loss: 1.286014\n","Epoch: 611 \tTraining Loss: 0.575986 \tValidation Loss: 1.280753\n","Epoch: 612 \tTraining Loss: 0.589802 \tValidation Loss: 1.272298\n","Validation loss has decreased (1.279844 --> 1.272298).  Saving model ...\n","Epoch: 613 \tTraining Loss: 0.592091 \tValidation Loss: 1.286456\n","Epoch: 614 \tTraining Loss: 0.565236 \tValidation Loss: 1.276260\n","Epoch: 615 \tTraining Loss: 0.526289 \tValidation Loss: 1.284410\n","Epoch: 616 \tTraining Loss: 0.584898 \tValidation Loss: 1.285045\n","Epoch: 617 \tTraining Loss: 0.612899 \tValidation Loss: 1.275635\n","Epoch: 618 \tTraining Loss: 0.585761 \tValidation Loss: 1.279116\n","Epoch: 619 \tTraining Loss: 0.585476 \tValidation Loss: 1.276317\n","Epoch: 620 \tTraining Loss: 0.570976 \tValidation Loss: 1.274688\n","Epoch: 621 \tTraining Loss: 0.653260 \tValidation Loss: 1.272586\n","Epoch: 622 \tTraining Loss: 0.525861 \tValidation Loss: 1.266547\n","Validation loss has decreased (1.272298 --> 1.266547).  Saving model ...\n","Epoch: 623 \tTraining Loss: 0.553248 \tValidation Loss: 1.258961\n","Validation loss has decreased (1.266547 --> 1.258961).  Saving model ...\n","Epoch: 624 \tTraining Loss: 0.536005 \tValidation Loss: 1.273851\n","Epoch: 625 \tTraining Loss: 0.545373 \tValidation Loss: 1.270165\n","Epoch: 626 \tTraining Loss: 0.589220 \tValidation Loss: 1.260793\n","Epoch: 627 \tTraining Loss: 0.555148 \tValidation Loss: 1.256871\n","Validation loss has decreased (1.258961 --> 1.256871).  Saving model ...\n","Epoch: 628 \tTraining Loss: 0.569051 \tValidation Loss: 1.268345\n","Epoch: 629 \tTraining Loss: 0.520852 \tValidation Loss: 1.268172\n","Epoch: 630 \tTraining Loss: 0.575747 \tValidation Loss: 1.263387\n","Epoch: 631 \tTraining Loss: 0.542226 \tValidation Loss: 1.263495\n","Epoch: 632 \tTraining Loss: 0.537124 \tValidation Loss: 1.269382\n","Epoch: 633 \tTraining Loss: 0.520537 \tValidation Loss: 1.267724\n","Epoch: 634 \tTraining Loss: 0.580505 \tValidation Loss: 1.260226\n","Epoch: 635 \tTraining Loss: 0.594100 \tValidation Loss: 1.260971\n","Epoch: 636 \tTraining Loss: 0.565445 \tValidation Loss: 1.255404\n","Validation loss has decreased (1.256871 --> 1.255404).  Saving model ...\n","Epoch: 637 \tTraining Loss: 0.526990 \tValidation Loss: 1.248794\n","Validation loss has decreased (1.255404 --> 1.248794).  Saving model ...\n","Epoch: 638 \tTraining Loss: 0.548427 \tValidation Loss: 1.259777\n","Epoch: 639 \tTraining Loss: 0.567252 \tValidation Loss: 1.250587\n","Epoch: 640 \tTraining Loss: 0.522739 \tValidation Loss: 1.245158\n","Validation loss has decreased (1.248794 --> 1.245158).  Saving model ...\n","Epoch: 641 \tTraining Loss: 0.562268 \tValidation Loss: 1.254678\n","Epoch: 642 \tTraining Loss: 0.545580 \tValidation Loss: 1.260037\n","Epoch: 643 \tTraining Loss: 0.533305 \tValidation Loss: 1.254388\n","Epoch: 644 \tTraining Loss: 0.530374 \tValidation Loss: 1.251536\n","Epoch: 645 \tTraining Loss: 0.511857 \tValidation Loss: 1.252901\n","Epoch: 646 \tTraining Loss: 0.566535 \tValidation Loss: 1.250537\n","Epoch: 647 \tTraining Loss: 0.572237 \tValidation Loss: 1.256323\n","Epoch: 648 \tTraining Loss: 0.532122 \tValidation Loss: 1.248546\n","Epoch: 649 \tTraining Loss: 0.528756 \tValidation Loss: 1.247283\n","Epoch: 650 \tTraining Loss: 0.515663 \tValidation Loss: 1.241229\n","Validation loss has decreased (1.245158 --> 1.241229).  Saving model ...\n","Epoch: 651 \tTraining Loss: 0.527145 \tValidation Loss: 1.245532\n","Epoch: 652 \tTraining Loss: 0.465536 \tValidation Loss: 1.251208\n","Epoch: 653 \tTraining Loss: 0.516725 \tValidation Loss: 1.227678\n","Validation loss has decreased (1.241229 --> 1.227678).  Saving model ...\n","Epoch: 654 \tTraining Loss: 0.535977 \tValidation Loss: 1.237833\n","Epoch: 655 \tTraining Loss: 0.537704 \tValidation Loss: 1.243192\n","Epoch: 656 \tTraining Loss: 0.524208 \tValidation Loss: 1.246239\n","Epoch: 657 \tTraining Loss: 0.562317 \tValidation Loss: 1.236972\n","Epoch: 658 \tTraining Loss: 0.484040 \tValidation Loss: 1.234841\n","Epoch: 659 \tTraining Loss: 0.574327 \tValidation Loss: 1.242799\n","Epoch: 660 \tTraining Loss: 0.496095 \tValidation Loss: 1.232857\n","Epoch: 661 \tTraining Loss: 0.532757 \tValidation Loss: 1.227738\n","Epoch: 662 \tTraining Loss: 0.501637 \tValidation Loss: 1.228160\n","Epoch: 663 \tTraining Loss: 0.535627 \tValidation Loss: 1.223670\n","Validation loss has decreased (1.227678 --> 1.223670).  Saving model ...\n","Epoch: 664 \tTraining Loss: 0.534599 \tValidation Loss: 1.234336\n","Epoch: 665 \tTraining Loss: 0.523213 \tValidation Loss: 1.217269\n","Validation loss has decreased (1.223670 --> 1.217269).  Saving model ...\n","Epoch: 666 \tTraining Loss: 0.514814 \tValidation Loss: 1.232127\n","Epoch: 667 \tTraining Loss: 0.503260 \tValidation Loss: 1.226716\n","Epoch: 668 \tTraining Loss: 0.512869 \tValidation Loss: 1.232463\n","Epoch: 669 \tTraining Loss: 0.481998 \tValidation Loss: 1.226738\n","Epoch: 670 \tTraining Loss: 0.493470 \tValidation Loss: 1.223180\n","Epoch: 671 \tTraining Loss: 0.518990 \tValidation Loss: 1.229233\n","Epoch: 672 \tTraining Loss: 0.481590 \tValidation Loss: 1.241528\n","Epoch: 673 \tTraining Loss: 0.493652 \tValidation Loss: 1.222043\n","Epoch: 674 \tTraining Loss: 0.489216 \tValidation Loss: 1.216548\n","Validation loss has decreased (1.217269 --> 1.216548).  Saving model ...\n","Epoch: 675 \tTraining Loss: 0.443397 \tValidation Loss: 1.225141\n","Epoch: 676 \tTraining Loss: 0.515405 \tValidation Loss: 1.220839\n","Epoch: 677 \tTraining Loss: 0.527481 \tValidation Loss: 1.233501\n","Epoch: 678 \tTraining Loss: 0.522649 \tValidation Loss: 1.224390\n","Epoch: 679 \tTraining Loss: 0.475173 \tValidation Loss: 1.228705\n","Epoch: 680 \tTraining Loss: 0.473126 \tValidation Loss: 1.224720\n","Epoch: 681 \tTraining Loss: 0.486183 \tValidation Loss: 1.221479\n","Epoch: 682 \tTraining Loss: 0.440596 \tValidation Loss: 1.222692\n","Epoch: 683 \tTraining Loss: 0.507926 \tValidation Loss: 1.205893\n","Validation loss has decreased (1.216548 --> 1.205893).  Saving model ...\n","Epoch: 684 \tTraining Loss: 0.436989 \tValidation Loss: 1.225037\n","Epoch: 685 \tTraining Loss: 0.481624 \tValidation Loss: 1.207931\n","Epoch: 686 \tTraining Loss: 0.495100 \tValidation Loss: 1.218765\n","Epoch: 687 \tTraining Loss: 0.460774 \tValidation Loss: 1.211165\n","Epoch: 688 \tTraining Loss: 0.493169 \tValidation Loss: 1.206123\n","Epoch: 689 \tTraining Loss: 0.467830 \tValidation Loss: 1.232144\n","Epoch: 690 \tTraining Loss: 0.500311 \tValidation Loss: 1.209480\n","Epoch: 691 \tTraining Loss: 0.486666 \tValidation Loss: 1.226788\n","Epoch: 692 \tTraining Loss: 0.466579 \tValidation Loss: 1.215080\n","Epoch: 693 \tTraining Loss: 0.456761 \tValidation Loss: 1.215310\n","Epoch: 694 \tTraining Loss: 0.457320 \tValidation Loss: 1.215778\n","Epoch: 695 \tTraining Loss: 0.486424 \tValidation Loss: 1.223513\n","Epoch: 696 \tTraining Loss: 0.446432 \tValidation Loss: 1.213056\n","Epoch: 697 \tTraining Loss: 0.477262 \tValidation Loss: 1.206729\n","Epoch: 698 \tTraining Loss: 0.446275 \tValidation Loss: 1.208930\n","Epoch: 699 \tTraining Loss: 0.430846 \tValidation Loss: 1.213774\n","Epoch: 700 \tTraining Loss: 0.482008 \tValidation Loss: 1.213723\n","Epoch: 701 \tTraining Loss: 0.443225 \tValidation Loss: 1.217828\n","Epoch: 702 \tTraining Loss: 0.481104 \tValidation Loss: 1.204770\n","Validation loss has decreased (1.205893 --> 1.204770).  Saving model ...\n","Epoch: 703 \tTraining Loss: 0.447619 \tValidation Loss: 1.212051\n","Epoch: 704 \tTraining Loss: 0.425432 \tValidation Loss: 1.201920\n","Validation loss has decreased (1.204770 --> 1.201920).  Saving model ...\n","Epoch: 705 \tTraining Loss: 0.444944 \tValidation Loss: 1.200788\n","Validation loss has decreased (1.201920 --> 1.200788).  Saving model ...\n","Epoch: 706 \tTraining Loss: 0.446488 \tValidation Loss: 1.196191\n","Validation loss has decreased (1.200788 --> 1.196191).  Saving model ...\n","Epoch: 707 \tTraining Loss: 0.437824 \tValidation Loss: 1.210978\n","Epoch: 708 \tTraining Loss: 0.453439 \tValidation Loss: 1.207509\n","Epoch: 709 \tTraining Loss: 0.445478 \tValidation Loss: 1.195483\n","Validation loss has decreased (1.196191 --> 1.195483).  Saving model ...\n","Epoch: 710 \tTraining Loss: 0.460441 \tValidation Loss: 1.194425\n","Validation loss has decreased (1.195483 --> 1.194425).  Saving model ...\n","Epoch: 711 \tTraining Loss: 0.472018 \tValidation Loss: 1.189651\n","Validation loss has decreased (1.194425 --> 1.189651).  Saving model ...\n","Epoch: 712 \tTraining Loss: 0.438381 \tValidation Loss: 1.200143\n","Epoch: 713 \tTraining Loss: 0.448306 \tValidation Loss: 1.196149\n","Epoch: 714 \tTraining Loss: 0.452608 \tValidation Loss: 1.196369\n","Epoch: 715 \tTraining Loss: 0.472667 \tValidation Loss: 1.196102\n","Epoch: 716 \tTraining Loss: 0.436869 \tValidation Loss: 1.188862\n","Validation loss has decreased (1.189651 --> 1.188862).  Saving model ...\n","Epoch: 717 \tTraining Loss: 0.433152 \tValidation Loss: 1.187393\n","Validation loss has decreased (1.188862 --> 1.187393).  Saving model ...\n","Epoch: 718 \tTraining Loss: 0.418111 \tValidation Loss: 1.198946\n","Epoch: 719 \tTraining Loss: 0.451850 \tValidation Loss: 1.207754\n","Epoch: 720 \tTraining Loss: 0.434092 \tValidation Loss: 1.183783\n","Validation loss has decreased (1.187393 --> 1.183783).  Saving model ...\n","Epoch: 721 \tTraining Loss: 0.418883 \tValidation Loss: 1.191741\n","Epoch: 722 \tTraining Loss: 0.433041 \tValidation Loss: 1.202420\n","Epoch: 723 \tTraining Loss: 0.452145 \tValidation Loss: 1.190829\n","Epoch: 724 \tTraining Loss: 0.428452 \tValidation Loss: 1.188962\n","Epoch: 725 \tTraining Loss: 0.412648 \tValidation Loss: 1.193019\n","Epoch: 726 \tTraining Loss: 0.431612 \tValidation Loss: 1.184929\n","Epoch: 727 \tTraining Loss: 0.412440 \tValidation Loss: 1.184931\n","Epoch: 728 \tTraining Loss: 0.427542 \tValidation Loss: 1.185678\n","Epoch: 729 \tTraining Loss: 0.439467 \tValidation Loss: 1.184201\n","Epoch: 730 \tTraining Loss: 0.417736 \tValidation Loss: 1.190594\n","Epoch: 731 \tTraining Loss: 0.411203 \tValidation Loss: 1.196295\n","Epoch: 732 \tTraining Loss: 0.416745 \tValidation Loss: 1.187491\n","Epoch: 733 \tTraining Loss: 0.472305 \tValidation Loss: 1.182043\n","Validation loss has decreased (1.183783 --> 1.182043).  Saving model ...\n","Epoch: 734 \tTraining Loss: 0.405051 \tValidation Loss: 1.170326\n","Validation loss has decreased (1.182043 --> 1.170326).  Saving model ...\n","Epoch: 735 \tTraining Loss: 0.410108 \tValidation Loss: 1.180468\n","Epoch: 736 \tTraining Loss: 0.430797 \tValidation Loss: 1.177445\n","Epoch: 737 \tTraining Loss: 0.361288 \tValidation Loss: 1.193864\n","Epoch: 738 \tTraining Loss: 0.397491 \tValidation Loss: 1.182583\n","Epoch: 739 \tTraining Loss: 0.413505 \tValidation Loss: 1.181189\n","Epoch: 740 \tTraining Loss: 0.424386 \tValidation Loss: 1.185532\n","Epoch: 741 \tTraining Loss: 0.421003 \tValidation Loss: 1.171013\n","Epoch: 742 \tTraining Loss: 0.407559 \tValidation Loss: 1.175827\n","Epoch: 743 \tTraining Loss: 0.431695 \tValidation Loss: 1.178219\n","Epoch: 744 \tTraining Loss: 0.430975 \tValidation Loss: 1.175809\n","Epoch: 745 \tTraining Loss: 0.387183 \tValidation Loss: 1.182583\n","Epoch: 746 \tTraining Loss: 0.414322 \tValidation Loss: 1.190462\n","Epoch: 747 \tTraining Loss: 0.386159 \tValidation Loss: 1.172428\n","Epoch: 748 \tTraining Loss: 0.398337 \tValidation Loss: 1.181697\n","Epoch: 749 \tTraining Loss: 0.410654 \tValidation Loss: 1.170965\n","Epoch: 750 \tTraining Loss: 0.403533 \tValidation Loss: 1.170662\n","Epoch: 751 \tTraining Loss: 0.362540 \tValidation Loss: 1.180853\n","Epoch: 752 \tTraining Loss: 0.399542 \tValidation Loss: 1.169824\n","Validation loss has decreased (1.170326 --> 1.169824).  Saving model ...\n","Epoch: 753 \tTraining Loss: 0.367699 \tValidation Loss: 1.192950\n","Epoch: 754 \tTraining Loss: 0.434883 \tValidation Loss: 1.171375\n","Epoch: 755 \tTraining Loss: 0.396395 \tValidation Loss: 1.167962\n","Validation loss has decreased (1.169824 --> 1.167962).  Saving model ...\n","Epoch: 756 \tTraining Loss: 0.396392 \tValidation Loss: 1.174003\n","Epoch: 757 \tTraining Loss: 0.429750 \tValidation Loss: 1.163389\n","Validation loss has decreased (1.167962 --> 1.163389).  Saving model ...\n","Epoch: 758 \tTraining Loss: 0.383804 \tValidation Loss: 1.170175\n","Epoch: 759 \tTraining Loss: 0.371058 \tValidation Loss: 1.173302\n","Epoch: 760 \tTraining Loss: 0.392740 \tValidation Loss: 1.166455\n","Epoch: 761 \tTraining Loss: 0.384543 \tValidation Loss: 1.177931\n","Epoch: 762 \tTraining Loss: 0.420358 \tValidation Loss: 1.173472\n","Epoch: 763 \tTraining Loss: 0.398233 \tValidation Loss: 1.174009\n","Epoch: 764 \tTraining Loss: 0.376514 \tValidation Loss: 1.172141\n","Epoch: 765 \tTraining Loss: 0.402882 \tValidation Loss: 1.153890\n","Validation loss has decreased (1.163389 --> 1.153890).  Saving model ...\n","Epoch: 766 \tTraining Loss: 0.392216 \tValidation Loss: 1.159698\n","Epoch: 767 \tTraining Loss: 0.378426 \tValidation Loss: 1.152979\n","Validation loss has decreased (1.153890 --> 1.152979).  Saving model ...\n","Epoch: 768 \tTraining Loss: 0.403553 \tValidation Loss: 1.169490\n","Epoch: 769 \tTraining Loss: 0.372872 \tValidation Loss: 1.161847\n","Epoch: 770 \tTraining Loss: 0.377692 \tValidation Loss: 1.152400\n","Validation loss has decreased (1.152979 --> 1.152400).  Saving model ...\n","Epoch: 771 \tTraining Loss: 0.360230 \tValidation Loss: 1.175032\n","Epoch: 772 \tTraining Loss: 0.395823 \tValidation Loss: 1.162153\n","Epoch: 773 \tTraining Loss: 0.355161 \tValidation Loss: 1.170493\n","Epoch: 774 \tTraining Loss: 0.427757 \tValidation Loss: 1.167713\n","Epoch: 775 \tTraining Loss: 0.341018 \tValidation Loss: 1.159478\n","Epoch: 776 \tTraining Loss: 0.408411 \tValidation Loss: 1.161799\n","Epoch: 777 \tTraining Loss: 0.362694 \tValidation Loss: 1.159362\n","Epoch: 778 \tTraining Loss: 0.364493 \tValidation Loss: 1.148594\n","Validation loss has decreased (1.152400 --> 1.148594).  Saving model ...\n","Epoch: 779 \tTraining Loss: 0.394896 \tValidation Loss: 1.153303\n","Epoch: 780 \tTraining Loss: 0.393207 \tValidation Loss: 1.160400\n","Epoch: 781 \tTraining Loss: 0.390064 \tValidation Loss: 1.152714\n","Epoch: 782 \tTraining Loss: 0.390534 \tValidation Loss: 1.153924\n","Epoch: 783 \tTraining Loss: 0.392405 \tValidation Loss: 1.158256\n","Epoch: 784 \tTraining Loss: 0.365270 \tValidation Loss: 1.145591\n","Validation loss has decreased (1.148594 --> 1.145591).  Saving model ...\n","Epoch: 785 \tTraining Loss: 0.391664 \tValidation Loss: 1.152398\n","Epoch: 786 \tTraining Loss: 0.352145 \tValidation Loss: 1.160606\n","Epoch: 787 \tTraining Loss: 0.346427 \tValidation Loss: 1.159821\n","Epoch: 788 \tTraining Loss: 0.362216 \tValidation Loss: 1.149840\n","Epoch: 789 \tTraining Loss: 0.372034 \tValidation Loss: 1.154165\n","Epoch: 790 \tTraining Loss: 0.339162 \tValidation Loss: 1.167351\n","Epoch: 791 \tTraining Loss: 0.378901 \tValidation Loss: 1.158214\n","Epoch: 792 \tTraining Loss: 0.373588 \tValidation Loss: 1.157521\n","Epoch: 793 \tTraining Loss: 0.357938 \tValidation Loss: 1.163975\n","Epoch: 794 \tTraining Loss: 0.362788 \tValidation Loss: 1.151899\n","Epoch: 795 \tTraining Loss: 0.356013 \tValidation Loss: 1.147877\n","Epoch: 796 \tTraining Loss: 0.324039 \tValidation Loss: 1.152134\n","Epoch: 797 \tTraining Loss: 0.369522 \tValidation Loss: 1.153677\n","Epoch: 798 \tTraining Loss: 0.360888 \tValidation Loss: 1.139987\n","Validation loss has decreased (1.145591 --> 1.139987).  Saving model ...\n","Epoch: 799 \tTraining Loss: 0.351025 \tValidation Loss: 1.140780\n","Epoch: 800 \tTraining Loss: 0.357780 \tValidation Loss: 1.141730\n","Epoch: 801 \tTraining Loss: 0.364969 \tValidation Loss: 1.142267\n","Epoch: 802 \tTraining Loss: 0.344456 \tValidation Loss: 1.137743\n","Validation loss has decreased (1.139987 --> 1.137743).  Saving model ...\n","Epoch: 803 \tTraining Loss: 0.373658 \tValidation Loss: 1.149127\n","Epoch: 804 \tTraining Loss: 0.371078 \tValidation Loss: 1.142062\n","Epoch: 805 \tTraining Loss: 0.379067 \tValidation Loss: 1.144246\n","Epoch: 806 \tTraining Loss: 0.355767 \tValidation Loss: 1.127075\n","Validation loss has decreased (1.137743 --> 1.127075).  Saving model ...\n","Epoch: 807 \tTraining Loss: 0.341935 \tValidation Loss: 1.161596\n","Epoch: 808 \tTraining Loss: 0.341651 \tValidation Loss: 1.153117\n","Epoch: 809 \tTraining Loss: 0.346720 \tValidation Loss: 1.147825\n","Epoch: 810 \tTraining Loss: 0.345682 \tValidation Loss: 1.144544\n","Epoch: 811 \tTraining Loss: 0.341567 \tValidation Loss: 1.147556\n","Epoch: 812 \tTraining Loss: 0.322364 \tValidation Loss: 1.131520\n","Epoch: 813 \tTraining Loss: 0.330159 \tValidation Loss: 1.145107\n","Epoch: 814 \tTraining Loss: 0.326042 \tValidation Loss: 1.139882\n","Epoch: 815 \tTraining Loss: 0.328063 \tValidation Loss: 1.143186\n","Epoch: 816 \tTraining Loss: 0.372407 \tValidation Loss: 1.138813\n","Epoch: 817 \tTraining Loss: 0.368694 \tValidation Loss: 1.146224\n","Epoch: 818 \tTraining Loss: 0.342197 \tValidation Loss: 1.134461\n","Epoch: 819 \tTraining Loss: 0.341705 \tValidation Loss: 1.127060\n","Validation loss has decreased (1.127075 --> 1.127060).  Saving model ...\n","Epoch: 820 \tTraining Loss: 0.352303 \tValidation Loss: 1.149035\n","Epoch: 821 \tTraining Loss: 0.346485 \tValidation Loss: 1.133769\n","Epoch: 822 \tTraining Loss: 0.347850 \tValidation Loss: 1.139845\n","Epoch: 823 \tTraining Loss: 0.345812 \tValidation Loss: 1.131236\n","Epoch: 824 \tTraining Loss: 0.300891 \tValidation Loss: 1.137432\n","Epoch: 825 \tTraining Loss: 0.326218 \tValidation Loss: 1.142630\n","Epoch: 826 \tTraining Loss: 0.312253 \tValidation Loss: 1.143726\n","Epoch: 827 \tTraining Loss: 0.336447 \tValidation Loss: 1.159198\n","Epoch: 828 \tTraining Loss: 0.326048 \tValidation Loss: 1.137970\n","Epoch: 829 \tTraining Loss: 0.357148 \tValidation Loss: 1.139152\n","Epoch: 830 \tTraining Loss: 0.339742 \tValidation Loss: 1.116240\n","Validation loss has decreased (1.127060 --> 1.116240).  Saving model ...\n","Epoch: 831 \tTraining Loss: 0.309803 \tValidation Loss: 1.129193\n","Epoch: 832 \tTraining Loss: 0.347350 \tValidation Loss: 1.118322\n","Epoch: 833 \tTraining Loss: 0.336296 \tValidation Loss: 1.131186\n","Epoch: 834 \tTraining Loss: 0.347820 \tValidation Loss: 1.140219\n","Epoch: 835 \tTraining Loss: 0.370344 \tValidation Loss: 1.127964\n","Epoch: 836 \tTraining Loss: 0.327706 \tValidation Loss: 1.135784\n","Epoch: 837 \tTraining Loss: 0.323865 \tValidation Loss: 1.133817\n","Epoch: 838 \tTraining Loss: 0.322065 \tValidation Loss: 1.130012\n","Epoch: 839 \tTraining Loss: 0.317681 \tValidation Loss: 1.130351\n","Epoch: 840 \tTraining Loss: 0.316439 \tValidation Loss: 1.138309\n","Epoch: 841 \tTraining Loss: 0.313108 \tValidation Loss: 1.113304\n","Validation loss has decreased (1.116240 --> 1.113304).  Saving model ...\n","Epoch: 842 \tTraining Loss: 0.326465 \tValidation Loss: 1.120443\n","Epoch: 843 \tTraining Loss: 0.324507 \tValidation Loss: 1.126251\n","Epoch: 844 \tTraining Loss: 0.311342 \tValidation Loss: 1.116242\n","Epoch: 845 \tTraining Loss: 0.329817 \tValidation Loss: 1.123619\n","Epoch: 846 \tTraining Loss: 0.317973 \tValidation Loss: 1.130632\n","Epoch: 847 \tTraining Loss: 0.311242 \tValidation Loss: 1.118307\n","Epoch: 848 \tTraining Loss: 0.341638 \tValidation Loss: 1.116176\n","Epoch: 849 \tTraining Loss: 0.318213 \tValidation Loss: 1.112505\n","Validation loss has decreased (1.113304 --> 1.112505).  Saving model ...\n","Epoch: 850 \tTraining Loss: 0.312113 \tValidation Loss: 1.108424\n","Validation loss has decreased (1.112505 --> 1.108424).  Saving model ...\n","Epoch: 851 \tTraining Loss: 0.334766 \tValidation Loss: 1.116750\n","Epoch: 852 \tTraining Loss: 0.319116 \tValidation Loss: 1.119820\n","Epoch: 853 \tTraining Loss: 0.312473 \tValidation Loss: 1.131995\n","Epoch: 854 \tTraining Loss: 0.322080 \tValidation Loss: 1.140133\n","Epoch: 855 \tTraining Loss: 0.333209 \tValidation Loss: 1.111814\n","Epoch: 856 \tTraining Loss: 0.326776 \tValidation Loss: 1.115332\n","Epoch: 857 \tTraining Loss: 0.276991 \tValidation Loss: 1.128154\n","Epoch: 858 \tTraining Loss: 0.331786 \tValidation Loss: 1.112489\n","Epoch: 859 \tTraining Loss: 0.329428 \tValidation Loss: 1.120312\n","Epoch: 860 \tTraining Loss: 0.299609 \tValidation Loss: 1.122580\n","Epoch: 861 \tTraining Loss: 0.311821 \tValidation Loss: 1.113323\n","Epoch: 862 \tTraining Loss: 0.304180 \tValidation Loss: 1.119472\n","Epoch: 863 \tTraining Loss: 0.317473 \tValidation Loss: 1.122383\n","Epoch: 864 \tTraining Loss: 0.339895 \tValidation Loss: 1.101447\n","Validation loss has decreased (1.108424 --> 1.101447).  Saving model ...\n","Epoch: 865 \tTraining Loss: 0.282733 \tValidation Loss: 1.133085\n","Epoch: 866 \tTraining Loss: 0.295983 \tValidation Loss: 1.125257\n","Epoch: 867 \tTraining Loss: 0.281295 \tValidation Loss: 1.106708\n","Epoch: 868 \tTraining Loss: 0.288804 \tValidation Loss: 1.113136\n","Epoch: 869 \tTraining Loss: 0.275685 \tValidation Loss: 1.129865\n","Epoch: 870 \tTraining Loss: 0.307401 \tValidation Loss: 1.101367\n","Validation loss has decreased (1.101447 --> 1.101367).  Saving model ...\n","Epoch: 871 \tTraining Loss: 0.285803 \tValidation Loss: 1.115642\n","Epoch: 872 \tTraining Loss: 0.335984 \tValidation Loss: 1.105838\n","Epoch: 873 \tTraining Loss: 0.321252 \tValidation Loss: 1.103585\n","Epoch: 874 \tTraining Loss: 0.280276 \tValidation Loss: 1.115077\n","Epoch: 875 \tTraining Loss: 0.319121 \tValidation Loss: 1.099834\n","Validation loss has decreased (1.101367 --> 1.099834).  Saving model ...\n","Epoch: 876 \tTraining Loss: 0.314888 \tValidation Loss: 1.111865\n","Epoch: 877 \tTraining Loss: 0.287931 \tValidation Loss: 1.106302\n","Epoch: 878 \tTraining Loss: 0.297190 \tValidation Loss: 1.117433\n","Epoch: 879 \tTraining Loss: 0.309370 \tValidation Loss: 1.129388\n","Epoch: 880 \tTraining Loss: 0.312539 \tValidation Loss: 1.110690\n","Epoch: 881 \tTraining Loss: 0.286134 \tValidation Loss: 1.103066\n","Epoch: 882 \tTraining Loss: 0.287265 \tValidation Loss: 1.104846\n","Epoch: 883 \tTraining Loss: 0.292786 \tValidation Loss: 1.114302\n","Epoch: 884 \tTraining Loss: 0.314823 \tValidation Loss: 1.118398\n","Epoch: 885 \tTraining Loss: 0.272455 \tValidation Loss: 1.116108\n","Epoch: 886 \tTraining Loss: 0.296374 \tValidation Loss: 1.106041\n","Epoch: 887 \tTraining Loss: 0.258688 \tValidation Loss: 1.106929\n","Epoch: 888 \tTraining Loss: 0.297533 \tValidation Loss: 1.105641\n","Epoch: 889 \tTraining Loss: 0.265957 \tValidation Loss: 1.097752\n","Validation loss has decreased (1.099834 --> 1.097752).  Saving model ...\n","Epoch: 890 \tTraining Loss: 0.291398 \tValidation Loss: 1.117272\n","Epoch: 891 \tTraining Loss: 0.291569 \tValidation Loss: 1.112637\n","Epoch: 892 \tTraining Loss: 0.269545 \tValidation Loss: 1.098418\n","Epoch: 893 \tTraining Loss: 0.285618 \tValidation Loss: 1.095062\n","Validation loss has decreased (1.097752 --> 1.095062).  Saving model ...\n","Epoch: 894 \tTraining Loss: 0.271819 \tValidation Loss: 1.127548\n","Epoch: 895 \tTraining Loss: 0.283118 \tValidation Loss: 1.115122\n","Epoch: 896 \tTraining Loss: 0.284742 \tValidation Loss: 1.095414\n","Epoch: 897 \tTraining Loss: 0.296808 \tValidation Loss: 1.095240\n","Epoch: 898 \tTraining Loss: 0.263566 \tValidation Loss: 1.097827\n","Epoch: 899 \tTraining Loss: 0.294807 \tValidation Loss: 1.105998\n","Epoch: 900 \tTraining Loss: 0.267392 \tValidation Loss: 1.098407\n","Epoch: 901 \tTraining Loss: 0.310440 \tValidation Loss: 1.110574\n","Epoch: 902 \tTraining Loss: 0.268892 \tValidation Loss: 1.111348\n","Epoch: 903 \tTraining Loss: 0.310441 \tValidation Loss: 1.103952\n","Epoch: 904 \tTraining Loss: 0.286410 \tValidation Loss: 1.107937\n","Epoch: 905 \tTraining Loss: 0.267223 \tValidation Loss: 1.096461\n","Epoch: 906 \tTraining Loss: 0.285332 \tValidation Loss: 1.095442\n","Epoch: 907 \tTraining Loss: 0.289831 \tValidation Loss: 1.105474\n","Epoch: 908 \tTraining Loss: 0.275760 \tValidation Loss: 1.080315\n","Validation loss has decreased (1.095062 --> 1.080315).  Saving model ...\n","Epoch: 909 \tTraining Loss: 0.287682 \tValidation Loss: 1.102429\n","Epoch: 910 \tTraining Loss: 0.271178 \tValidation Loss: 1.091951\n","Epoch: 911 \tTraining Loss: 0.287021 \tValidation Loss: 1.106884\n","Epoch: 912 \tTraining Loss: 0.277512 \tValidation Loss: 1.111414\n","Epoch: 913 \tTraining Loss: 0.275897 \tValidation Loss: 1.096238\n","Epoch: 914 \tTraining Loss: 0.283960 \tValidation Loss: 1.119239\n","Epoch: 915 \tTraining Loss: 0.280502 \tValidation Loss: 1.098884\n","Epoch: 916 \tTraining Loss: 0.302792 \tValidation Loss: 1.088944\n","Epoch: 917 \tTraining Loss: 0.252700 \tValidation Loss: 1.108289\n","Epoch: 918 \tTraining Loss: 0.278597 \tValidation Loss: 1.104950\n","Epoch: 919 \tTraining Loss: 0.254938 \tValidation Loss: 1.105583\n","Epoch: 920 \tTraining Loss: 0.279098 \tValidation Loss: 1.083248\n","Epoch: 921 \tTraining Loss: 0.232257 \tValidation Loss: 1.110222\n","Epoch: 922 \tTraining Loss: 0.289299 \tValidation Loss: 1.102728\n","Epoch: 923 \tTraining Loss: 0.282631 \tValidation Loss: 1.111352\n","Epoch: 924 \tTraining Loss: 0.265313 \tValidation Loss: 1.099495\n","Epoch: 925 \tTraining Loss: 0.258922 \tValidation Loss: 1.118465\n","Epoch: 926 \tTraining Loss: 0.265652 \tValidation Loss: 1.088595\n","Epoch: 927 \tTraining Loss: 0.268115 \tValidation Loss: 1.107323\n","Epoch: 928 \tTraining Loss: 0.257637 \tValidation Loss: 1.104740\n","Epoch: 929 \tTraining Loss: 0.253067 \tValidation Loss: 1.101339\n","Epoch: 930 \tTraining Loss: 0.228492 \tValidation Loss: 1.101467\n","Epoch: 931 \tTraining Loss: 0.266975 \tValidation Loss: 1.090358\n","Epoch: 932 \tTraining Loss: 0.270980 \tValidation Loss: 1.080963\n","Epoch: 933 \tTraining Loss: 0.260203 \tValidation Loss: 1.096511\n","Epoch: 934 \tTraining Loss: 0.248824 \tValidation Loss: 1.090559\n","Epoch: 935 \tTraining Loss: 0.275767 \tValidation Loss: 1.090933\n","Epoch: 936 \tTraining Loss: 0.254048 \tValidation Loss: 1.097766\n","Epoch: 937 \tTraining Loss: 0.263624 \tValidation Loss: 1.113126\n","Epoch: 938 \tTraining Loss: 0.268096 \tValidation Loss: 1.090171\n","Epoch: 939 \tTraining Loss: 0.261454 \tValidation Loss: 1.090110\n","Epoch: 940 \tTraining Loss: 0.282396 \tValidation Loss: 1.099435\n","Epoch: 941 \tTraining Loss: 0.249634 \tValidation Loss: 1.087168\n","Epoch: 942 \tTraining Loss: 0.279277 \tValidation Loss: 1.085216\n","Epoch: 943 \tTraining Loss: 0.251977 \tValidation Loss: 1.111750\n","Epoch: 944 \tTraining Loss: 0.255133 \tValidation Loss: 1.080255\n","Validation loss has decreased (1.080315 --> 1.080255).  Saving model ...\n","Epoch: 945 \tTraining Loss: 0.274091 \tValidation Loss: 1.097501\n","Epoch: 946 \tTraining Loss: 0.239159 \tValidation Loss: 1.085281\n","Epoch: 947 \tTraining Loss: 0.253612 \tValidation Loss: 1.091074\n","Epoch: 948 \tTraining Loss: 0.242765 \tValidation Loss: 1.071458\n","Validation loss has decreased (1.080255 --> 1.071458).  Saving model ...\n","Epoch: 949 \tTraining Loss: 0.245222 \tValidation Loss: 1.088927\n","Epoch: 950 \tTraining Loss: 0.232203 \tValidation Loss: 1.107294\n","Epoch: 951 \tTraining Loss: 0.243217 \tValidation Loss: 1.088252\n","Epoch: 952 \tTraining Loss: 0.249322 \tValidation Loss: 1.080678\n","Epoch: 953 \tTraining Loss: 0.266879 \tValidation Loss: 1.088000\n","Epoch: 954 \tTraining Loss: 0.245458 \tValidation Loss: 1.099425\n","Epoch: 955 \tTraining Loss: 0.284119 \tValidation Loss: 1.087574\n","Epoch: 956 \tTraining Loss: 0.207938 \tValidation Loss: 1.083281\n","Epoch: 957 \tTraining Loss: 0.241490 \tValidation Loss: 1.087470\n","Epoch: 958 \tTraining Loss: 0.277768 \tValidation Loss: 1.088132\n","Epoch: 959 \tTraining Loss: 0.237485 \tValidation Loss: 1.087639\n","Epoch: 960 \tTraining Loss: 0.253021 \tValidation Loss: 1.082010\n","Epoch: 961 \tTraining Loss: 0.223673 \tValidation Loss: 1.075208\n","Epoch: 962 \tTraining Loss: 0.235646 \tValidation Loss: 1.065766\n","Validation loss has decreased (1.071458 --> 1.065766).  Saving model ...\n","Epoch: 963 \tTraining Loss: 0.238143 \tValidation Loss: 1.068654\n","Epoch: 964 \tTraining Loss: 0.250754 \tValidation Loss: 1.093559\n","Epoch: 965 \tTraining Loss: 0.212813 \tValidation Loss: 1.096084\n","Epoch: 966 \tTraining Loss: 0.245211 \tValidation Loss: 1.086670\n","Epoch: 967 \tTraining Loss: 0.229359 \tValidation Loss: 1.086314\n","Epoch: 968 \tTraining Loss: 0.233858 \tValidation Loss: 1.082107\n","Epoch: 969 \tTraining Loss: 0.229327 \tValidation Loss: 1.079436\n","Epoch: 970 \tTraining Loss: 0.217294 \tValidation Loss: 1.094679\n","Epoch: 971 \tTraining Loss: 0.226462 \tValidation Loss: 1.089197\n","Epoch: 972 \tTraining Loss: 0.213637 \tValidation Loss: 1.074061\n","Epoch: 973 \tTraining Loss: 0.254553 \tValidation Loss: 1.072058\n","Epoch: 974 \tTraining Loss: 0.246952 \tValidation Loss: 1.082003\n","Epoch: 975 \tTraining Loss: 0.246524 \tValidation Loss: 1.104393\n","Epoch: 976 \tTraining Loss: 0.255049 \tValidation Loss: 1.087695\n","Epoch: 977 \tTraining Loss: 0.240854 \tValidation Loss: 1.085043\n","Epoch: 978 \tTraining Loss: 0.234504 \tValidation Loss: 1.080529\n","Epoch: 979 \tTraining Loss: 0.220611 \tValidation Loss: 1.087124\n","Epoch: 980 \tTraining Loss: 0.233003 \tValidation Loss: 1.074152\n","Epoch: 981 \tTraining Loss: 0.246224 \tValidation Loss: 1.072756\n","Epoch: 982 \tTraining Loss: 0.237021 \tValidation Loss: 1.084742\n","Epoch: 983 \tTraining Loss: 0.234995 \tValidation Loss: 1.069585\n","Epoch: 984 \tTraining Loss: 0.249119 \tValidation Loss: 1.077355\n","Epoch: 985 \tTraining Loss: 0.215518 \tValidation Loss: 1.079081\n","Epoch: 986 \tTraining Loss: 0.213149 \tValidation Loss: 1.092246\n","Epoch: 987 \tTraining Loss: 0.244595 \tValidation Loss: 1.081801\n","Epoch: 988 \tTraining Loss: 0.222859 \tValidation Loss: 1.084348\n","Epoch: 989 \tTraining Loss: 0.264654 \tValidation Loss: 1.103552\n","Epoch: 990 \tTraining Loss: 0.236198 \tValidation Loss: 1.082801\n","Epoch: 991 \tTraining Loss: 0.244684 \tValidation Loss: 1.082732\n","Epoch: 992 \tTraining Loss: 0.232895 \tValidation Loss: 1.068323\n","Epoch: 993 \tTraining Loss: 0.221363 \tValidation Loss: 1.086340\n","Epoch: 994 \tTraining Loss: 0.221455 \tValidation Loss: 1.084591\n","Epoch: 995 \tTraining Loss: 0.203062 \tValidation Loss: 1.072837\n","Epoch: 996 \tTraining Loss: 0.238499 \tValidation Loss: 1.075796\n","Epoch: 997 \tTraining Loss: 0.217405 \tValidation Loss: 1.081246\n","Epoch: 998 \tTraining Loss: 0.238523 \tValidation Loss: 1.069100\n","Epoch: 999 \tTraining Loss: 0.241835 \tValidation Loss: 1.061085\n","Validation loss has decreased (1.065766 --> 1.061085).  Saving model ...\n","Epoch: 1000 \tTraining Loss: 0.211263 \tValidation Loss: 1.079586\n","Epoch: 1001 \tTraining Loss: 0.209628 \tValidation Loss: 1.088051\n","Epoch: 1002 \tTraining Loss: 0.257897 \tValidation Loss: 1.061628\n","Epoch: 1003 \tTraining Loss: 0.192920 \tValidation Loss: 1.093169\n","Epoch: 1004 \tTraining Loss: 0.241047 \tValidation Loss: 1.074571\n","Epoch: 1005 \tTraining Loss: 0.229087 \tValidation Loss: 1.072510\n","Epoch: 1006 \tTraining Loss: 0.247194 \tValidation Loss: 1.083549\n","Epoch: 1007 \tTraining Loss: 0.258659 \tValidation Loss: 1.081967\n","Epoch: 1008 \tTraining Loss: 0.251020 \tValidation Loss: 1.077676\n","Epoch: 1009 \tTraining Loss: 0.217838 \tValidation Loss: 1.062830\n","Epoch: 1010 \tTraining Loss: 0.244382 \tValidation Loss: 1.059555\n","Validation loss has decreased (1.061085 --> 1.059555).  Saving model ...\n","Epoch: 1011 \tTraining Loss: 0.223378 \tValidation Loss: 1.067445\n","Epoch: 1012 \tTraining Loss: 0.208480 \tValidation Loss: 1.076383\n","Epoch: 1013 \tTraining Loss: 0.205355 \tValidation Loss: 1.079564\n","Epoch: 1014 \tTraining Loss: 0.222557 \tValidation Loss: 1.073761\n","Epoch: 1015 \tTraining Loss: 0.222328 \tValidation Loss: 1.069965\n","Epoch: 1016 \tTraining Loss: 0.223301 \tValidation Loss: 1.071508\n","Epoch: 1017 \tTraining Loss: 0.214033 \tValidation Loss: 1.071543\n","Epoch: 1018 \tTraining Loss: 0.242440 \tValidation Loss: 1.092895\n","Epoch: 1019 \tTraining Loss: 0.242518 \tValidation Loss: 1.078083\n","Epoch: 1020 \tTraining Loss: 0.207114 \tValidation Loss: 1.054731\n","Validation loss has decreased (1.059555 --> 1.054731).  Saving model ...\n","Epoch: 1021 \tTraining Loss: 0.207730 \tValidation Loss: 1.068328\n","Epoch: 1022 \tTraining Loss: 0.195896 \tValidation Loss: 1.075096\n","Epoch: 1023 \tTraining Loss: 0.215263 \tValidation Loss: 1.082198\n","Epoch: 1024 \tTraining Loss: 0.186756 \tValidation Loss: 1.063967\n","Epoch: 1025 \tTraining Loss: 0.223104 \tValidation Loss: 1.060706\n","Epoch: 1026 \tTraining Loss: 0.193726 \tValidation Loss: 1.076765\n","Epoch: 1027 \tTraining Loss: 0.223676 \tValidation Loss: 1.071559\n","Epoch: 1028 \tTraining Loss: 0.225492 \tValidation Loss: 1.068547\n","Epoch: 1029 \tTraining Loss: 0.184947 \tValidation Loss: 1.080192\n","Epoch: 1030 \tTraining Loss: 0.220661 \tValidation Loss: 1.066287\n","Epoch: 1031 \tTraining Loss: 0.208712 \tValidation Loss: 1.062467\n","Epoch: 1032 \tTraining Loss: 0.213718 \tValidation Loss: 1.067836\n","Epoch: 1033 \tTraining Loss: 0.204730 \tValidation Loss: 1.066936\n","Epoch: 1034 \tTraining Loss: 0.236669 \tValidation Loss: 1.065895\n","Epoch: 1035 \tTraining Loss: 0.201311 \tValidation Loss: 1.077771\n","Epoch: 1036 \tTraining Loss: 0.199085 \tValidation Loss: 1.071199\n","Epoch: 1037 \tTraining Loss: 0.213613 \tValidation Loss: 1.080221\n","Epoch: 1038 \tTraining Loss: 0.211241 \tValidation Loss: 1.070081\n","Epoch: 1039 \tTraining Loss: 0.177007 \tValidation Loss: 1.057885\n","Epoch: 1040 \tTraining Loss: 0.215186 \tValidation Loss: 1.087353\n","Epoch: 1041 \tTraining Loss: 0.226065 \tValidation Loss: 1.065939\n","Epoch: 1042 \tTraining Loss: 0.205655 \tValidation Loss: 1.059900\n","Epoch: 1043 \tTraining Loss: 0.191037 \tValidation Loss: 1.055632\n","Epoch: 1044 \tTraining Loss: 0.206948 \tValidation Loss: 1.072356\n","Epoch: 1045 \tTraining Loss: 0.203379 \tValidation Loss: 1.068559\n","Epoch: 1046 \tTraining Loss: 0.217546 \tValidation Loss: 1.090985\n","Epoch: 1047 \tTraining Loss: 0.207337 \tValidation Loss: 1.066968\n","Epoch: 1048 \tTraining Loss: 0.188414 \tValidation Loss: 1.071973\n","Epoch: 1049 \tTraining Loss: 0.220112 \tValidation Loss: 1.077264\n","Epoch: 1050 \tTraining Loss: 0.230902 \tValidation Loss: 1.070565\n","Epoch: 1051 \tTraining Loss: 0.209041 \tValidation Loss: 1.067305\n","Epoch: 1052 \tTraining Loss: 0.189933 \tValidation Loss: 1.059258\n","Epoch: 1053 \tTraining Loss: 0.209965 \tValidation Loss: 1.056035\n","Epoch: 1054 \tTraining Loss: 0.222604 \tValidation Loss: 1.081217\n","Epoch: 1055 \tTraining Loss: 0.206491 \tValidation Loss: 1.080304\n","Epoch: 1056 \tTraining Loss: 0.211768 \tValidation Loss: 1.077351\n","Epoch: 1057 \tTraining Loss: 0.208914 \tValidation Loss: 1.080787\n","Epoch: 1058 \tTraining Loss: 0.211581 \tValidation Loss: 1.065693\n","Epoch: 1059 \tTraining Loss: 0.185907 \tValidation Loss: 1.065208\n","Epoch: 1060 \tTraining Loss: 0.198559 \tValidation Loss: 1.067254\n","Epoch: 1061 \tTraining Loss: 0.200719 \tValidation Loss: 1.068571\n","Epoch: 1062 \tTraining Loss: 0.211596 \tValidation Loss: 1.070121\n","Epoch: 1063 \tTraining Loss: 0.224633 \tValidation Loss: 1.080209\n","Epoch: 1064 \tTraining Loss: 0.198076 \tValidation Loss: 1.069201\n","Epoch: 1065 \tTraining Loss: 0.199591 \tValidation Loss: 1.061254\n","Epoch: 1066 \tTraining Loss: 0.211288 \tValidation Loss: 1.068851\n","Epoch: 1067 \tTraining Loss: 0.208331 \tValidation Loss: 1.074158\n","Epoch: 1068 \tTraining Loss: 0.170955 \tValidation Loss: 1.061459\n","Epoch: 1069 \tTraining Loss: 0.212865 \tValidation Loss: 1.053305\n","Validation loss has decreased (1.054731 --> 1.053305).  Saving model ...\n","Epoch: 1070 \tTraining Loss: 0.235485 \tValidation Loss: 1.055568\n","Epoch: 1071 \tTraining Loss: 0.192867 \tValidation Loss: 1.065172\n","Epoch: 1072 \tTraining Loss: 0.219571 \tValidation Loss: 1.060149\n","Epoch: 1073 \tTraining Loss: 0.181742 \tValidation Loss: 1.079674\n","Epoch: 1074 \tTraining Loss: 0.218640 \tValidation Loss: 1.059010\n","Epoch: 1075 \tTraining Loss: 0.245309 \tValidation Loss: 1.049213\n","Validation loss has decreased (1.053305 --> 1.049213).  Saving model ...\n","Epoch: 1076 \tTraining Loss: 0.209089 \tValidation Loss: 1.054616\n","Epoch: 1077 \tTraining Loss: 0.199161 \tValidation Loss: 1.068898\n","Epoch: 1078 \tTraining Loss: 0.191341 \tValidation Loss: 1.052246\n","Epoch: 1079 \tTraining Loss: 0.200280 \tValidation Loss: 1.057629\n","Epoch: 1080 \tTraining Loss: 0.184124 \tValidation Loss: 1.072884\n","Epoch: 1081 \tTraining Loss: 0.212819 \tValidation Loss: 1.069460\n","Epoch: 1082 \tTraining Loss: 0.192646 \tValidation Loss: 1.060370\n","Epoch: 1083 \tTraining Loss: 0.193841 \tValidation Loss: 1.064143\n","Epoch: 1084 \tTraining Loss: 0.211385 \tValidation Loss: 1.048155\n","Validation loss has decreased (1.049213 --> 1.048155).  Saving model ...\n","Epoch: 1085 \tTraining Loss: 0.202339 \tValidation Loss: 1.037903\n","Validation loss has decreased (1.048155 --> 1.037903).  Saving model ...\n","Epoch: 1086 \tTraining Loss: 0.209954 \tValidation Loss: 1.049710\n","Epoch: 1087 \tTraining Loss: 0.202219 \tValidation Loss: 1.045987\n","Epoch: 1088 \tTraining Loss: 0.160810 \tValidation Loss: 1.079139\n","Epoch: 1089 \tTraining Loss: 0.201208 \tValidation Loss: 1.070802\n","Epoch: 1090 \tTraining Loss: 0.181467 \tValidation Loss: 1.058232\n","Epoch: 1091 \tTraining Loss: 0.222283 \tValidation Loss: 1.072334\n","Epoch: 1092 \tTraining Loss: 0.152213 \tValidation Loss: 1.055702\n","Epoch: 1093 \tTraining Loss: 0.208405 \tValidation Loss: 1.074674\n","Epoch: 1094 \tTraining Loss: 0.177993 \tValidation Loss: 1.063892\n","Epoch: 1095 \tTraining Loss: 0.194920 \tValidation Loss: 1.056174\n","Epoch: 1096 \tTraining Loss: 0.207531 \tValidation Loss: 1.053778\n","Epoch: 1097 \tTraining Loss: 0.215837 \tValidation Loss: 1.064368\n","Epoch: 1098 \tTraining Loss: 0.218956 \tValidation Loss: 1.059962\n","Epoch: 1099 \tTraining Loss: 0.181624 \tValidation Loss: 1.068819\n","Epoch: 1100 \tTraining Loss: 0.194321 \tValidation Loss: 1.061551\n","Epoch: 1101 \tTraining Loss: 0.187050 \tValidation Loss: 1.038820\n","Epoch: 1102 \tTraining Loss: 0.198689 \tValidation Loss: 1.040935\n","Epoch: 1103 \tTraining Loss: 0.194464 \tValidation Loss: 1.074782\n","Epoch: 1104 \tTraining Loss: 0.193089 \tValidation Loss: 1.061115\n","Epoch: 1105 \tTraining Loss: 0.193417 \tValidation Loss: 1.069993\n","Epoch: 1106 \tTraining Loss: 0.189221 \tValidation Loss: 1.065175\n","Epoch: 1107 \tTraining Loss: 0.191400 \tValidation Loss: 1.059973\n","Epoch: 1108 \tTraining Loss: 0.175325 \tValidation Loss: 1.066692\n","Epoch: 1109 \tTraining Loss: 0.197909 \tValidation Loss: 1.068749\n","Epoch: 1110 \tTraining Loss: 0.187228 \tValidation Loss: 1.046032\n","Epoch: 1111 \tTraining Loss: 0.186419 \tValidation Loss: 1.063227\n","Epoch: 1112 \tTraining Loss: 0.222445 \tValidation Loss: 1.048031\n","Epoch: 1113 \tTraining Loss: 0.166793 \tValidation Loss: 1.049963\n","Epoch: 1114 \tTraining Loss: 0.168054 \tValidation Loss: 1.063185\n","Epoch: 1115 \tTraining Loss: 0.179028 \tValidation Loss: 1.060201\n","Epoch: 1116 \tTraining Loss: 0.178707 \tValidation Loss: 1.053192\n","Epoch: 1117 \tTraining Loss: 0.182073 \tValidation Loss: 1.052887\n","Epoch: 1118 \tTraining Loss: 0.174908 \tValidation Loss: 1.055240\n","Epoch: 1119 \tTraining Loss: 0.182655 \tValidation Loss: 1.060079\n","Epoch: 1120 \tTraining Loss: 0.159852 \tValidation Loss: 1.047435\n","Epoch: 1121 \tTraining Loss: 0.180793 \tValidation Loss: 1.065719\n","Epoch: 1122 \tTraining Loss: 0.170244 \tValidation Loss: 1.052289\n","Epoch: 1123 \tTraining Loss: 0.170989 \tValidation Loss: 1.043145\n","Epoch: 1124 \tTraining Loss: 0.168221 \tValidation Loss: 1.064353\n","Epoch: 1125 \tTraining Loss: 0.216133 \tValidation Loss: 1.053168\n","Epoch: 1126 \tTraining Loss: 0.193517 \tValidation Loss: 1.061536\n","Epoch: 1127 \tTraining Loss: 0.207217 \tValidation Loss: 1.054682\n","Epoch: 1128 \tTraining Loss: 0.159964 \tValidation Loss: 1.068555\n","Epoch: 1129 \tTraining Loss: 0.178761 \tValidation Loss: 1.036830\n","Validation loss has decreased (1.037903 --> 1.036830).  Saving model ...\n","Epoch: 1130 \tTraining Loss: 0.187202 \tValidation Loss: 1.056275\n","Epoch: 1131 \tTraining Loss: 0.199337 \tValidation Loss: 1.047684\n","Epoch: 1132 \tTraining Loss: 0.202070 \tValidation Loss: 1.062009\n","Epoch: 1133 \tTraining Loss: 0.207307 \tValidation Loss: 1.051389\n","Epoch: 1134 \tTraining Loss: 0.190263 \tValidation Loss: 1.054573\n","Epoch: 1135 \tTraining Loss: 0.221886 \tValidation Loss: 1.055858\n","Epoch: 1136 \tTraining Loss: 0.172961 \tValidation Loss: 1.071100\n","Epoch: 1137 \tTraining Loss: 0.180209 \tValidation Loss: 1.071874\n","Epoch: 1138 \tTraining Loss: 0.191981 \tValidation Loss: 1.054969\n","Epoch: 1139 \tTraining Loss: 0.163253 \tValidation Loss: 1.045625\n","Epoch: 1140 \tTraining Loss: 0.173239 \tValidation Loss: 1.055078\n","Epoch: 1141 \tTraining Loss: 0.164349 \tValidation Loss: 1.045581\n","Epoch: 1142 \tTraining Loss: 0.173853 \tValidation Loss: 1.062967\n","Epoch: 1143 \tTraining Loss: 0.191348 \tValidation Loss: 1.058341\n","Epoch: 1144 \tTraining Loss: 0.183247 \tValidation Loss: 1.069693\n","Epoch: 1145 \tTraining Loss: 0.187263 \tValidation Loss: 1.060961\n","Epoch: 1146 \tTraining Loss: 0.157323 \tValidation Loss: 1.053508\n","Epoch: 1147 \tTraining Loss: 0.162812 \tValidation Loss: 1.060551\n","Epoch: 1148 \tTraining Loss: 0.165322 \tValidation Loss: 1.038299\n","Epoch: 1149 \tTraining Loss: 0.192577 \tValidation Loss: 1.051880\n","Epoch: 1150 \tTraining Loss: 0.163986 \tValidation Loss: 1.074920\n","Epoch: 1151 \tTraining Loss: 0.164136 \tValidation Loss: 1.047414\n","Epoch: 1152 \tTraining Loss: 0.178087 \tValidation Loss: 1.037100\n","Epoch: 1153 \tTraining Loss: 0.176307 \tValidation Loss: 1.053275\n","Epoch: 1154 \tTraining Loss: 0.173615 \tValidation Loss: 1.057477\n","Epoch: 1155 \tTraining Loss: 0.176530 \tValidation Loss: 1.041023\n","Epoch: 1156 \tTraining Loss: 0.186154 \tValidation Loss: 1.040794\n","Epoch: 1157 \tTraining Loss: 0.188030 \tValidation Loss: 1.034769\n","Validation loss has decreased (1.036830 --> 1.034769).  Saving model ...\n","Epoch: 1158 \tTraining Loss: 0.165822 \tValidation Loss: 1.065891\n","Epoch: 1159 \tTraining Loss: 0.172801 \tValidation Loss: 1.048275\n","Epoch: 1160 \tTraining Loss: 0.164037 \tValidation Loss: 1.051507\n","Epoch: 1161 \tTraining Loss: 0.159815 \tValidation Loss: 1.046519\n","Epoch: 1162 \tTraining Loss: 0.197716 \tValidation Loss: 1.050256\n","Epoch: 1163 \tTraining Loss: 0.151728 \tValidation Loss: 1.051709\n","Epoch: 1164 \tTraining Loss: 0.153843 \tValidation Loss: 1.049085\n","Epoch: 1165 \tTraining Loss: 0.161362 \tValidation Loss: 1.057387\n","Epoch: 1166 \tTraining Loss: 0.183028 \tValidation Loss: 1.031837\n","Validation loss has decreased (1.034769 --> 1.031837).  Saving model ...\n","Epoch: 1167 \tTraining Loss: 0.169503 \tValidation Loss: 1.030526\n","Validation loss has decreased (1.031837 --> 1.030526).  Saving model ...\n","Epoch: 1168 \tTraining Loss: 0.196277 \tValidation Loss: 1.051646\n","Epoch: 1169 \tTraining Loss: 0.185524 \tValidation Loss: 1.039739\n","Epoch: 1170 \tTraining Loss: 0.158616 \tValidation Loss: 1.034771\n","Epoch: 1171 \tTraining Loss: 0.166837 \tValidation Loss: 1.033190\n","Epoch: 1172 \tTraining Loss: 0.178004 \tValidation Loss: 1.050035\n","Epoch: 1173 \tTraining Loss: 0.160771 \tValidation Loss: 1.065809\n","Epoch: 1174 \tTraining Loss: 0.193662 \tValidation Loss: 1.048176\n","Epoch: 1175 \tTraining Loss: 0.172956 \tValidation Loss: 1.034426\n","Epoch: 1176 \tTraining Loss: 0.170366 \tValidation Loss: 1.053444\n","Epoch: 1177 \tTraining Loss: 0.172203 \tValidation Loss: 1.052485\n","Epoch: 1178 \tTraining Loss: 0.172940 \tValidation Loss: 1.039565\n","Epoch: 1179 \tTraining Loss: 0.163672 \tValidation Loss: 1.064613\n","Epoch: 1180 \tTraining Loss: 0.140442 \tValidation Loss: 1.043706\n","Epoch: 1181 \tTraining Loss: 0.163696 \tValidation Loss: 1.056627\n","Epoch: 1182 \tTraining Loss: 0.161080 \tValidation Loss: 1.037997\n","Epoch: 1183 \tTraining Loss: 0.171682 \tValidation Loss: 1.063106\n","Epoch: 1184 \tTraining Loss: 0.156737 \tValidation Loss: 1.048989\n","Epoch: 1185 \tTraining Loss: 0.183081 \tValidation Loss: 1.044579\n","Epoch: 1186 \tTraining Loss: 0.176176 \tValidation Loss: 1.048507\n","Epoch: 1187 \tTraining Loss: 0.167849 \tValidation Loss: 1.056123\n","Epoch: 1188 \tTraining Loss: 0.190964 \tValidation Loss: 1.070514\n","Epoch: 1189 \tTraining Loss: 0.162711 \tValidation Loss: 1.047792\n","Epoch: 1190 \tTraining Loss: 0.182150 \tValidation Loss: 1.041527\n","Epoch: 1191 \tTraining Loss: 0.160847 \tValidation Loss: 1.045665\n","Epoch: 1192 \tTraining Loss: 0.147847 \tValidation Loss: 1.042564\n","Epoch: 1193 \tTraining Loss: 0.124678 \tValidation Loss: 1.040931\n","Epoch: 1194 \tTraining Loss: 0.154424 \tValidation Loss: 1.036984\n","Epoch: 1195 \tTraining Loss: 0.165888 \tValidation Loss: 1.052187\n","Epoch: 1196 \tTraining Loss: 0.143639 \tValidation Loss: 1.024433\n","Validation loss has decreased (1.030526 --> 1.024433).  Saving model ...\n","Epoch: 1197 \tTraining Loss: 0.174075 \tValidation Loss: 1.043380\n","Epoch: 1198 \tTraining Loss: 0.162800 \tValidation Loss: 1.061104\n","Epoch: 1199 \tTraining Loss: 0.184402 \tValidation Loss: 1.051314\n","Epoch: 1200 \tTraining Loss: 0.147689 \tValidation Loss: 1.056890\n","Epoch: 1201 \tTraining Loss: 0.147661 \tValidation Loss: 1.034465\n","Epoch: 1202 \tTraining Loss: 0.145952 \tValidation Loss: 1.048370\n","Epoch: 1203 \tTraining Loss: 0.136425 \tValidation Loss: 1.052159\n","Epoch: 1204 \tTraining Loss: 0.171969 \tValidation Loss: 1.065874\n","Epoch: 1205 \tTraining Loss: 0.170791 \tValidation Loss: 1.032507\n","Epoch: 1206 \tTraining Loss: 0.167519 \tValidation Loss: 1.048875\n","Epoch: 1207 \tTraining Loss: 0.148218 \tValidation Loss: 1.032355\n","Epoch: 1208 \tTraining Loss: 0.152056 \tValidation Loss: 1.036808\n","Epoch: 1209 \tTraining Loss: 0.164040 \tValidation Loss: 1.056126\n","Epoch: 1210 \tTraining Loss: 0.165036 \tValidation Loss: 1.038014\n","Epoch: 1211 \tTraining Loss: 0.151166 \tValidation Loss: 1.035729\n","Epoch: 1212 \tTraining Loss: 0.147985 \tValidation Loss: 1.056442\n","Epoch: 1213 \tTraining Loss: 0.178190 \tValidation Loss: 1.052690\n","Epoch: 1214 \tTraining Loss: 0.152112 \tValidation Loss: 1.031197\n","Epoch: 1215 \tTraining Loss: 0.166074 \tValidation Loss: 1.055529\n","Epoch: 1216 \tTraining Loss: 0.169349 \tValidation Loss: 1.030832\n","Epoch: 1217 \tTraining Loss: 0.157836 \tValidation Loss: 1.045438\n","Epoch: 1218 \tTraining Loss: 0.174372 \tValidation Loss: 1.040465\n","Epoch: 1219 \tTraining Loss: 0.137349 \tValidation Loss: 1.036200\n","Epoch: 1220 \tTraining Loss: 0.164193 \tValidation Loss: 1.027687\n","Epoch: 1221 \tTraining Loss: 0.179707 \tValidation Loss: 1.041146\n","Epoch: 1222 \tTraining Loss: 0.147891 \tValidation Loss: 1.025732\n","Epoch: 1223 \tTraining Loss: 0.159862 \tValidation Loss: 1.041787\n","Epoch: 1224 \tTraining Loss: 0.160210 \tValidation Loss: 1.054840\n","Epoch: 1225 \tTraining Loss: 0.158363 \tValidation Loss: 1.039501\n","Epoch: 1226 \tTraining Loss: 0.141624 \tValidation Loss: 1.049987\n","Epoch: 1227 \tTraining Loss: 0.174249 \tValidation Loss: 1.030167\n","Epoch: 1228 \tTraining Loss: 0.157778 \tValidation Loss: 1.038802\n","Epoch: 1229 \tTraining Loss: 0.139337 \tValidation Loss: 1.031120\n","Epoch: 1230 \tTraining Loss: 0.145312 \tValidation Loss: 1.057472\n","Epoch: 1231 \tTraining Loss: 0.162828 \tValidation Loss: 1.049071\n","Epoch: 1232 \tTraining Loss: 0.138980 \tValidation Loss: 1.042351\n","Epoch: 1233 \tTraining Loss: 0.181084 \tValidation Loss: 1.059119\n","Epoch: 1234 \tTraining Loss: 0.143035 \tValidation Loss: 1.060019\n","Epoch: 1235 \tTraining Loss: 0.149330 \tValidation Loss: 1.060633\n","Epoch: 1236 \tTraining Loss: 0.153395 \tValidation Loss: 1.026540\n","Epoch: 1237 \tTraining Loss: 0.138856 \tValidation Loss: 1.032180\n","Epoch: 1238 \tTraining Loss: 0.156620 \tValidation Loss: 1.053545\n","Epoch: 1239 \tTraining Loss: 0.169153 \tValidation Loss: 1.024050\n","Validation loss has decreased (1.024433 --> 1.024050).  Saving model ...\n","Epoch: 1240 \tTraining Loss: 0.140155 \tValidation Loss: 1.055967\n","Epoch: 1241 \tTraining Loss: 0.165294 \tValidation Loss: 1.033846\n","Epoch: 1242 \tTraining Loss: 0.174368 \tValidation Loss: 1.030937\n","Epoch: 1243 \tTraining Loss: 0.122957 \tValidation Loss: 1.038908\n","Epoch: 1244 \tTraining Loss: 0.139261 \tValidation Loss: 1.021049\n","Validation loss has decreased (1.024050 --> 1.021049).  Saving model ...\n","Epoch: 1245 \tTraining Loss: 0.147125 \tValidation Loss: 1.039257\n","Epoch: 1246 \tTraining Loss: 0.140641 \tValidation Loss: 1.056636\n","Epoch: 1247 \tTraining Loss: 0.115213 \tValidation Loss: 1.062044\n","Epoch: 1248 \tTraining Loss: 0.176459 \tValidation Loss: 1.040148\n","Epoch: 1249 \tTraining Loss: 0.149075 \tValidation Loss: 1.055221\n","Epoch: 1250 \tTraining Loss: 0.159597 \tValidation Loss: 1.037425\n","Epoch: 1251 \tTraining Loss: 0.135930 \tValidation Loss: 1.045681\n","Epoch: 1252 \tTraining Loss: 0.148376 \tValidation Loss: 1.027545\n","Epoch: 1253 \tTraining Loss: 0.135729 \tValidation Loss: 1.039751\n","Epoch: 1254 \tTraining Loss: 0.150972 \tValidation Loss: 1.050061\n","Epoch: 1255 \tTraining Loss: 0.156992 \tValidation Loss: 1.048341\n","Epoch: 1256 \tTraining Loss: 0.143507 \tValidation Loss: 1.027431\n","Epoch: 1257 \tTraining Loss: 0.147172 \tValidation Loss: 1.031126\n","Epoch: 1258 \tTraining Loss: 0.150071 \tValidation Loss: 1.044568\n","Epoch: 1259 \tTraining Loss: 0.145701 \tValidation Loss: 1.042711\n","Epoch: 1260 \tTraining Loss: 0.161362 \tValidation Loss: 1.032055\n","Epoch: 1261 \tTraining Loss: 0.158458 \tValidation Loss: 1.033181\n","Epoch: 1262 \tTraining Loss: 0.135903 \tValidation Loss: 1.028387\n","Epoch: 1263 \tTraining Loss: 0.172250 \tValidation Loss: 1.037998\n","Epoch: 1264 \tTraining Loss: 0.159901 \tValidation Loss: 1.057196\n","Epoch: 1265 \tTraining Loss: 0.148737 \tValidation Loss: 1.029439\n","Epoch: 1266 \tTraining Loss: 0.146129 \tValidation Loss: 1.029814\n","Epoch: 1267 \tTraining Loss: 0.158033 \tValidation Loss: 1.040717\n","Epoch: 1268 \tTraining Loss: 0.134007 \tValidation Loss: 1.050076\n","Epoch: 1269 \tTraining Loss: 0.144544 \tValidation Loss: 1.066417\n","Epoch: 1270 \tTraining Loss: 0.134147 \tValidation Loss: 1.035391\n","Epoch: 1271 \tTraining Loss: 0.126633 \tValidation Loss: 1.043874\n","Epoch: 1272 \tTraining Loss: 0.120601 \tValidation Loss: 1.057880\n","Epoch: 1273 \tTraining Loss: 0.157592 \tValidation Loss: 1.039147\n","Epoch: 1274 \tTraining Loss: 0.135994 \tValidation Loss: 1.049065\n","Epoch: 1275 \tTraining Loss: 0.160454 \tValidation Loss: 1.026281\n","Epoch: 1276 \tTraining Loss: 0.155593 \tValidation Loss: 1.030033\n","Epoch: 1277 \tTraining Loss: 0.130725 \tValidation Loss: 1.039798\n","Epoch: 1278 \tTraining Loss: 0.150479 \tValidation Loss: 1.053359\n","Epoch: 1279 \tTraining Loss: 0.137594 \tValidation Loss: 1.049165\n","Epoch: 1280 \tTraining Loss: 0.149559 \tValidation Loss: 1.037650\n","Epoch: 1281 \tTraining Loss: 0.142868 \tValidation Loss: 1.022217\n","Epoch: 1282 \tTraining Loss: 0.151063 \tValidation Loss: 1.043702\n","Epoch: 1283 \tTraining Loss: 0.170491 \tValidation Loss: 1.020297\n","Validation loss has decreased (1.021049 --> 1.020297).  Saving model ...\n","Epoch: 1284 \tTraining Loss: 0.156672 \tValidation Loss: 1.028277\n","Epoch: 1285 \tTraining Loss: 0.147464 \tValidation Loss: 1.024073\n","Epoch: 1286 \tTraining Loss: 0.132899 \tValidation Loss: 1.048841\n","Epoch: 1287 \tTraining Loss: 0.147439 \tValidation Loss: 1.056714\n","Epoch: 1288 \tTraining Loss: 0.175500 \tValidation Loss: 1.028690\n","Epoch: 1289 \tTraining Loss: 0.139760 \tValidation Loss: 1.042363\n","Epoch: 1290 \tTraining Loss: 0.125810 \tValidation Loss: 1.047334\n","Epoch: 1291 \tTraining Loss: 0.161461 \tValidation Loss: 1.064333\n","Epoch: 1292 \tTraining Loss: 0.139989 \tValidation Loss: 1.031733\n","Epoch: 1293 \tTraining Loss: 0.151067 \tValidation Loss: 1.034021\n","Epoch: 1294 \tTraining Loss: 0.178496 \tValidation Loss: 1.049915\n","Epoch: 1295 \tTraining Loss: 0.151982 \tValidation Loss: 1.035293\n","Epoch: 1296 \tTraining Loss: 0.140149 \tValidation Loss: 1.037485\n","Epoch: 1297 \tTraining Loss: 0.135113 \tValidation Loss: 1.022717\n","Epoch: 1298 \tTraining Loss: 0.128367 \tValidation Loss: 1.026697\n","Epoch: 1299 \tTraining Loss: 0.132236 \tValidation Loss: 1.017855\n","Validation loss has decreased (1.020297 --> 1.017855).  Saving model ...\n","Epoch: 1300 \tTraining Loss: 0.132382 \tValidation Loss: 1.029675\n","Epoch: 1301 \tTraining Loss: 0.124241 \tValidation Loss: 1.067006\n","Epoch: 1302 \tTraining Loss: 0.138087 \tValidation Loss: 1.045658\n","Epoch: 1303 \tTraining Loss: 0.139552 \tValidation Loss: 1.030322\n","Epoch: 1304 \tTraining Loss: 0.147892 \tValidation Loss: 1.039295\n","Epoch: 1305 \tTraining Loss: 0.145237 \tValidation Loss: 1.022526\n","Epoch: 1306 \tTraining Loss: 0.127549 \tValidation Loss: 1.033593\n","Epoch: 1307 \tTraining Loss: 0.145653 \tValidation Loss: 1.049488\n","Epoch: 1308 \tTraining Loss: 0.133653 \tValidation Loss: 1.022603\n","Epoch: 1309 \tTraining Loss: 0.142082 \tValidation Loss: 1.034166\n","Epoch: 1310 \tTraining Loss: 0.151902 \tValidation Loss: 1.020532\n","Epoch: 1311 \tTraining Loss: 0.126790 \tValidation Loss: 1.031559\n","Epoch: 1312 \tTraining Loss: 0.138930 \tValidation Loss: 1.054320\n","Epoch: 1313 \tTraining Loss: 0.147979 \tValidation Loss: 1.055879\n","Epoch: 1314 \tTraining Loss: 0.132527 \tValidation Loss: 1.042861\n","Epoch: 1315 \tTraining Loss: 0.148301 \tValidation Loss: 1.039764\n","Epoch: 1316 \tTraining Loss: 0.139016 \tValidation Loss: 1.029276\n","Epoch: 1317 \tTraining Loss: 0.131945 \tValidation Loss: 1.033246\n","Epoch: 1318 \tTraining Loss: 0.152236 \tValidation Loss: 1.027970\n","Epoch: 1319 \tTraining Loss: 0.155074 \tValidation Loss: 1.034976\n","Epoch: 1320 \tTraining Loss: 0.145606 \tValidation Loss: 1.035479\n","Epoch: 1321 \tTraining Loss: 0.137334 \tValidation Loss: 1.058017\n","Epoch: 1322 \tTraining Loss: 0.122367 \tValidation Loss: 1.039821\n","Epoch: 1323 \tTraining Loss: 0.115390 \tValidation Loss: 1.022594\n","Epoch: 1324 \tTraining Loss: 0.127884 \tValidation Loss: 1.053788\n","Epoch: 1325 \tTraining Loss: 0.122786 \tValidation Loss: 1.029217\n","Epoch: 1326 \tTraining Loss: 0.133163 \tValidation Loss: 1.030353\n","Epoch: 1327 \tTraining Loss: 0.141183 \tValidation Loss: 1.038494\n","Epoch: 1328 \tTraining Loss: 0.128891 \tValidation Loss: 1.022805\n","Epoch: 1329 \tTraining Loss: 0.146934 \tValidation Loss: 1.030429\n","Epoch: 1330 \tTraining Loss: 0.124709 \tValidation Loss: 1.037776\n","Epoch: 1331 \tTraining Loss: 0.126680 \tValidation Loss: 1.022127\n","Epoch: 1332 \tTraining Loss: 0.120852 \tValidation Loss: 1.036445\n","Epoch: 1333 \tTraining Loss: 0.104634 \tValidation Loss: 1.031241\n","Epoch: 1334 \tTraining Loss: 0.146237 \tValidation Loss: 1.057864\n","Epoch: 1335 \tTraining Loss: 0.130277 \tValidation Loss: 1.040284\n","Epoch: 1336 \tTraining Loss: 0.132584 \tValidation Loss: 1.030943\n","Epoch: 1337 \tTraining Loss: 0.130543 \tValidation Loss: 1.032054\n","Epoch: 1338 \tTraining Loss: 0.132868 \tValidation Loss: 1.029089\n","Epoch: 1339 \tTraining Loss: 0.110050 \tValidation Loss: 1.023113\n","Epoch: 1340 \tTraining Loss: 0.126158 \tValidation Loss: 1.035361\n","Epoch: 1341 \tTraining Loss: 0.118353 \tValidation Loss: 1.009877\n","Validation loss has decreased (1.017855 --> 1.009877).  Saving model ...\n","Epoch: 1342 \tTraining Loss: 0.142096 \tValidation Loss: 1.034699\n","Epoch: 1343 \tTraining Loss: 0.111530 \tValidation Loss: 1.031277\n","Epoch: 1344 \tTraining Loss: 0.133790 \tValidation Loss: 1.029044\n","Epoch: 1345 \tTraining Loss: 0.119515 \tValidation Loss: 1.029572\n","Epoch: 1346 \tTraining Loss: 0.112835 \tValidation Loss: 1.030174\n","Epoch: 1347 \tTraining Loss: 0.127194 \tValidation Loss: 1.041358\n","Epoch: 1348 \tTraining Loss: 0.101657 \tValidation Loss: 1.011551\n","Epoch: 1349 \tTraining Loss: 0.132129 \tValidation Loss: 1.032518\n","Epoch: 1350 \tTraining Loss: 0.137765 \tValidation Loss: 1.027403\n","Epoch: 1351 \tTraining Loss: 0.129958 \tValidation Loss: 1.043643\n","Epoch: 1352 \tTraining Loss: 0.124732 \tValidation Loss: 1.025861\n","Epoch: 1353 \tTraining Loss: 0.134346 \tValidation Loss: 1.018259\n","Epoch: 1354 \tTraining Loss: 0.146037 \tValidation Loss: 1.026752\n","Epoch: 1355 \tTraining Loss: 0.112407 \tValidation Loss: 1.045962\n","Epoch: 1356 \tTraining Loss: 0.141014 \tValidation Loss: 1.046312\n","Epoch: 1357 \tTraining Loss: 0.114780 \tValidation Loss: 1.047425\n","Epoch: 1358 \tTraining Loss: 0.134031 \tValidation Loss: 1.018543\n","Epoch: 1359 \tTraining Loss: 0.137896 \tValidation Loss: 1.030902\n","Epoch: 1360 \tTraining Loss: 0.115770 \tValidation Loss: 1.023813\n","Epoch: 1361 \tTraining Loss: 0.112078 \tValidation Loss: 1.033553\n","Epoch: 1362 \tTraining Loss: 0.138064 \tValidation Loss: 1.027236\n","Epoch: 1363 \tTraining Loss: 0.133989 \tValidation Loss: 1.059949\n","Epoch: 1364 \tTraining Loss: 0.106676 \tValidation Loss: 1.034422\n","Epoch: 1365 \tTraining Loss: 0.115992 \tValidation Loss: 1.039344\n","Epoch: 1366 \tTraining Loss: 0.144056 \tValidation Loss: 1.038474\n","Epoch: 1367 \tTraining Loss: 0.129416 \tValidation Loss: 1.034612\n","Epoch: 1368 \tTraining Loss: 0.129441 \tValidation Loss: 1.027130\n","Epoch: 1369 \tTraining Loss: 0.110632 \tValidation Loss: 1.028677\n","Epoch: 1370 \tTraining Loss: 0.119923 \tValidation Loss: 1.033054\n","Epoch: 1371 \tTraining Loss: 0.134134 \tValidation Loss: 1.038540\n","Epoch: 1372 \tTraining Loss: 0.124566 \tValidation Loss: 1.017519\n","Epoch: 1373 \tTraining Loss: 0.110261 \tValidation Loss: 1.039995\n","Epoch: 1374 \tTraining Loss: 0.142716 \tValidation Loss: 1.032008\n","Epoch: 1375 \tTraining Loss: 0.106289 \tValidation Loss: 1.046219\n","Epoch: 1376 \tTraining Loss: 0.140024 \tValidation Loss: 1.025010\n","Epoch: 1377 \tTraining Loss: 0.102213 \tValidation Loss: 1.026966\n","Epoch: 1378 \tTraining Loss: 0.139819 \tValidation Loss: 1.025819\n","Epoch: 1379 \tTraining Loss: 0.129474 \tValidation Loss: 1.038885\n","Epoch: 1380 \tTraining Loss: 0.108196 \tValidation Loss: 1.025941\n","Epoch: 1381 \tTraining Loss: 0.125075 \tValidation Loss: 1.027266\n","Epoch: 1382 \tTraining Loss: 0.104688 \tValidation Loss: 1.040011\n","Epoch: 1383 \tTraining Loss: 0.109505 \tValidation Loss: 1.046648\n","Epoch: 1384 \tTraining Loss: 0.112844 \tValidation Loss: 1.018953\n","Epoch: 1385 \tTraining Loss: 0.126340 \tValidation Loss: 1.036280\n","Epoch: 1386 \tTraining Loss: 0.129787 \tValidation Loss: 1.053109\n","Epoch: 1387 \tTraining Loss: 0.133268 \tValidation Loss: 1.059099\n","Epoch: 1388 \tTraining Loss: 0.110685 \tValidation Loss: 1.065714\n","Epoch: 1389 \tTraining Loss: 0.113294 \tValidation Loss: 1.037120\n","Epoch: 1390 \tTraining Loss: 0.118134 \tValidation Loss: 1.016971\n","Epoch: 1391 \tTraining Loss: 0.122694 \tValidation Loss: 1.018458\n","Epoch: 1392 \tTraining Loss: 0.134931 \tValidation Loss: 1.028120\n","Epoch: 1393 \tTraining Loss: 0.137316 \tValidation Loss: 1.029949\n","Epoch: 1394 \tTraining Loss: 0.140240 \tValidation Loss: 1.034774\n","Epoch: 1395 \tTraining Loss: 0.131026 \tValidation Loss: 1.034729\n","Epoch: 1396 \tTraining Loss: 0.096053 \tValidation Loss: 1.058139\n","Epoch: 1397 \tTraining Loss: 0.111308 \tValidation Loss: 1.028409\n","Epoch: 1398 \tTraining Loss: 0.138914 \tValidation Loss: 1.013334\n","Epoch: 1399 \tTraining Loss: 0.106550 \tValidation Loss: 1.007749\n","Validation loss has decreased (1.009877 --> 1.007749).  Saving model ...\n","Epoch: 1400 \tTraining Loss: 0.096819 \tValidation Loss: 1.035707\n","Epoch: 1401 \tTraining Loss: 0.144227 \tValidation Loss: 1.051631\n","Epoch: 1402 \tTraining Loss: 0.136080 \tValidation Loss: 1.029240\n","Epoch: 1403 \tTraining Loss: 0.127359 \tValidation Loss: 1.045674\n","Epoch: 1404 \tTraining Loss: 0.121551 \tValidation Loss: 1.040561\n","Epoch: 1405 \tTraining Loss: 0.128098 \tValidation Loss: 1.021661\n","Epoch: 1406 \tTraining Loss: 0.120885 \tValidation Loss: 1.021674\n","Epoch: 1407 \tTraining Loss: 0.118518 \tValidation Loss: 1.036240\n","Epoch: 1408 \tTraining Loss: 0.142393 \tValidation Loss: 1.020397\n","Epoch: 1409 \tTraining Loss: 0.110446 \tValidation Loss: 1.026236\n","Epoch: 1410 \tTraining Loss: 0.117992 \tValidation Loss: 1.054688\n","Epoch: 1411 \tTraining Loss: 0.126791 \tValidation Loss: 1.029133\n","Epoch: 1412 \tTraining Loss: 0.134872 \tValidation Loss: 1.017526\n","Epoch: 1413 \tTraining Loss: 0.126062 \tValidation Loss: 1.011243\n","Epoch: 1414 \tTraining Loss: 0.130188 \tValidation Loss: 1.033067\n","Epoch: 1415 \tTraining Loss: 0.131874 \tValidation Loss: 1.039575\n","Epoch: 1416 \tTraining Loss: 0.143081 \tValidation Loss: 1.043724\n","Epoch: 1417 \tTraining Loss: 0.106591 \tValidation Loss: 1.017135\n","Epoch: 1418 \tTraining Loss: 0.116258 \tValidation Loss: 1.024991\n","Epoch: 1419 \tTraining Loss: 0.114937 \tValidation Loss: 1.033706\n","Epoch: 1420 \tTraining Loss: 0.139846 \tValidation Loss: 1.034963\n","Epoch: 1421 \tTraining Loss: 0.126918 \tValidation Loss: 0.995772\n","Validation loss has decreased (1.007749 --> 0.995772).  Saving model ...\n","Epoch: 1422 \tTraining Loss: 0.114647 \tValidation Loss: 1.033439\n","Epoch: 1423 \tTraining Loss: 0.100972 \tValidation Loss: 1.036060\n","Epoch: 1424 \tTraining Loss: 0.109896 \tValidation Loss: 1.033964\n","Epoch: 1425 \tTraining Loss: 0.107287 \tValidation Loss: 1.025403\n","Epoch: 1426 \tTraining Loss: 0.145052 \tValidation Loss: 1.021038\n","Epoch: 1427 \tTraining Loss: 0.112955 \tValidation Loss: 1.032779\n","Epoch: 1428 \tTraining Loss: 0.120498 \tValidation Loss: 1.031845\n","Epoch: 1429 \tTraining Loss: 0.124847 \tValidation Loss: 1.018797\n","Epoch: 1430 \tTraining Loss: 0.108496 \tValidation Loss: 1.033506\n","Epoch: 1431 \tTraining Loss: 0.115433 \tValidation Loss: 1.025987\n","Epoch: 1432 \tTraining Loss: 0.115423 \tValidation Loss: 1.035817\n","Epoch: 1433 \tTraining Loss: 0.119491 \tValidation Loss: 1.020946\n","Epoch: 1434 \tTraining Loss: 0.113221 \tValidation Loss: 1.031733\n","Epoch: 1435 \tTraining Loss: 0.126463 \tValidation Loss: 1.032053\n","Epoch: 1436 \tTraining Loss: 0.127568 \tValidation Loss: 1.036884\n","Epoch: 1437 \tTraining Loss: 0.098399 \tValidation Loss: 1.016128\n","Epoch: 1438 \tTraining Loss: 0.108191 \tValidation Loss: 1.012455\n","Epoch: 1439 \tTraining Loss: 0.120625 \tValidation Loss: 1.011392\n","Epoch: 1440 \tTraining Loss: 0.118137 \tValidation Loss: 1.014586\n","Epoch: 1441 \tTraining Loss: 0.088881 \tValidation Loss: 1.037318\n","Epoch: 1442 \tTraining Loss: 0.129865 \tValidation Loss: 1.041067\n","Epoch: 1443 \tTraining Loss: 0.121873 \tValidation Loss: 1.009924\n","Epoch: 1444 \tTraining Loss: 0.133806 \tValidation Loss: 1.023978\n","Epoch: 1445 \tTraining Loss: 0.102497 \tValidation Loss: 1.030853\n","Epoch: 1446 \tTraining Loss: 0.105892 \tValidation Loss: 1.034360\n","Epoch: 1447 \tTraining Loss: 0.106217 \tValidation Loss: 1.029827\n","Epoch: 1448 \tTraining Loss: 0.121120 \tValidation Loss: 1.027215\n","Epoch: 1449 \tTraining Loss: 0.102901 \tValidation Loss: 1.043655\n","Epoch: 1450 \tTraining Loss: 0.138930 \tValidation Loss: 1.018259\n","Epoch: 1451 \tTraining Loss: 0.094141 \tValidation Loss: 1.015967\n","Epoch: 1452 \tTraining Loss: 0.109565 \tValidation Loss: 1.015667\n","Epoch: 1453 \tTraining Loss: 0.128715 \tValidation Loss: 1.037740\n","Epoch: 1454 \tTraining Loss: 0.110889 \tValidation Loss: 1.020958\n","Epoch: 1455 \tTraining Loss: 0.107724 \tValidation Loss: 1.027254\n","Epoch: 1456 \tTraining Loss: 0.117116 \tValidation Loss: 1.021043\n","Epoch: 1457 \tTraining Loss: 0.106558 \tValidation Loss: 1.022012\n","Epoch: 1458 \tTraining Loss: 0.123748 \tValidation Loss: 1.029366\n","Epoch: 1459 \tTraining Loss: 0.119573 \tValidation Loss: 1.031567\n","Epoch: 1460 \tTraining Loss: 0.101371 \tValidation Loss: 1.032236\n","Epoch: 1461 \tTraining Loss: 0.101569 \tValidation Loss: 1.042297\n","Epoch: 1462 \tTraining Loss: 0.131618 \tValidation Loss: 1.031834\n","Epoch: 1463 \tTraining Loss: 0.098643 \tValidation Loss: 1.028370\n","Epoch: 1464 \tTraining Loss: 0.127381 \tValidation Loss: 1.030870\n","Epoch: 1465 \tTraining Loss: 0.098239 \tValidation Loss: 1.012317\n","Epoch: 1466 \tTraining Loss: 0.100053 \tValidation Loss: 1.017448\n","Epoch: 1467 \tTraining Loss: 0.117789 \tValidation Loss: 1.014638\n","Epoch: 1468 \tTraining Loss: 0.091325 \tValidation Loss: 1.029832\n","Epoch: 1469 \tTraining Loss: 0.116780 \tValidation Loss: 1.044569\n","Epoch: 1470 \tTraining Loss: 0.118245 \tValidation Loss: 1.013239\n","Epoch: 1471 \tTraining Loss: 0.105048 \tValidation Loss: 1.038969\n","Epoch: 1472 \tTraining Loss: 0.127186 \tValidation Loss: 1.033622\n","Epoch: 1473 \tTraining Loss: 0.097507 \tValidation Loss: 1.017793\n","Epoch: 1474 \tTraining Loss: 0.125835 \tValidation Loss: 1.009657\n","Epoch: 1475 \tTraining Loss: 0.097988 \tValidation Loss: 1.016394\n","Epoch: 1476 \tTraining Loss: 0.099125 \tValidation Loss: 1.014463\n","Epoch: 1477 \tTraining Loss: 0.100912 \tValidation Loss: 1.027952\n","Epoch: 1478 \tTraining Loss: 0.092211 \tValidation Loss: 1.042215\n","Epoch: 1479 \tTraining Loss: 0.106581 \tValidation Loss: 1.044065\n","Epoch: 1480 \tTraining Loss: 0.090112 \tValidation Loss: 1.033527\n","Epoch: 1481 \tTraining Loss: 0.119516 \tValidation Loss: 1.012979\n","Epoch: 1482 \tTraining Loss: 0.123138 \tValidation Loss: 1.003628\n","Epoch: 1483 \tTraining Loss: 0.125513 \tValidation Loss: 1.015563\n","Epoch: 1484 \tTraining Loss: 0.141028 \tValidation Loss: 1.020240\n","Epoch: 1485 \tTraining Loss: 0.100086 \tValidation Loss: 1.023511\n","Epoch: 1486 \tTraining Loss: 0.098056 \tValidation Loss: 1.024935\n","Epoch: 1487 \tTraining Loss: 0.093091 \tValidation Loss: 1.029163\n","Epoch: 1488 \tTraining Loss: 0.103194 \tValidation Loss: 1.033253\n","Epoch: 1489 \tTraining Loss: 0.124218 \tValidation Loss: 1.016853\n","Epoch: 1490 \tTraining Loss: 0.108041 \tValidation Loss: 1.030378\n","Epoch: 1491 \tTraining Loss: 0.113216 \tValidation Loss: 1.007256\n","Epoch: 1492 \tTraining Loss: 0.103911 \tValidation Loss: 1.024283\n","Epoch: 1493 \tTraining Loss: 0.123485 \tValidation Loss: 1.026828\n","Epoch: 1494 \tTraining Loss: 0.105052 \tValidation Loss: 1.030425\n","Epoch: 1495 \tTraining Loss: 0.114587 \tValidation Loss: 1.004952\n","Epoch: 1496 \tTraining Loss: 0.110319 \tValidation Loss: 1.027278\n","Epoch: 1497 \tTraining Loss: 0.114663 \tValidation Loss: 1.027113\n","Epoch: 1498 \tTraining Loss: 0.106161 \tValidation Loss: 1.019942\n","Epoch: 1499 \tTraining Loss: 0.110413 \tValidation Loss: 1.026344\n","Epoch: 1500 \tTraining Loss: 0.107989 \tValidation Loss: 1.037264\n","Epoch: 1501 \tTraining Loss: 0.130053 \tValidation Loss: 1.037550\n","Epoch: 1502 \tTraining Loss: 0.115708 \tValidation Loss: 1.012890\n","Epoch: 1503 \tTraining Loss: 0.104703 \tValidation Loss: 1.021882\n","Epoch: 1504 \tTraining Loss: 0.093080 \tValidation Loss: 1.036893\n","Epoch: 1505 \tTraining Loss: 0.104308 \tValidation Loss: 1.021966\n","Epoch: 1506 \tTraining Loss: 0.096954 \tValidation Loss: 1.034254\n","Epoch: 1507 \tTraining Loss: 0.124163 \tValidation Loss: 1.035700\n","Epoch: 1508 \tTraining Loss: 0.112571 \tValidation Loss: 1.009217\n","Epoch: 1509 \tTraining Loss: 0.103746 \tValidation Loss: 1.015774\n","Epoch: 1510 \tTraining Loss: 0.122535 \tValidation Loss: 1.024329\n","Epoch: 1511 \tTraining Loss: 0.100239 \tValidation Loss: 1.027464\n","Epoch: 1512 \tTraining Loss: 0.101574 \tValidation Loss: 1.017493\n","Epoch: 1513 \tTraining Loss: 0.092690 \tValidation Loss: 1.042200\n","Epoch: 1514 \tTraining Loss: 0.113055 \tValidation Loss: 1.028549\n","Epoch: 1515 \tTraining Loss: 0.114751 \tValidation Loss: 1.028989\n","Epoch: 1516 \tTraining Loss: 0.096236 \tValidation Loss: 1.018329\n","Epoch: 1517 \tTraining Loss: 0.104627 \tValidation Loss: 1.042639\n","Epoch: 1518 \tTraining Loss: 0.093164 \tValidation Loss: 1.007489\n","Epoch: 1519 \tTraining Loss: 0.092924 \tValidation Loss: 1.037601\n","Epoch: 1520 \tTraining Loss: 0.097238 \tValidation Loss: 1.032878\n","Epoch: 1521 \tTraining Loss: 0.100383 \tValidation Loss: 1.022193\n","Epoch: 1522 \tTraining Loss: 0.100096 \tValidation Loss: 1.039860\n","Epoch: 1523 \tTraining Loss: 0.090852 \tValidation Loss: 1.051079\n","Epoch: 1524 \tTraining Loss: 0.117547 \tValidation Loss: 1.023513\n","Epoch: 1525 \tTraining Loss: 0.090906 \tValidation Loss: 1.020230\n","Epoch: 1526 \tTraining Loss: 0.109076 \tValidation Loss: 1.021366\n","Epoch: 1527 \tTraining Loss: 0.101507 \tValidation Loss: 1.013947\n","Epoch: 1528 \tTraining Loss: 0.104277 \tValidation Loss: 1.040008\n","Epoch: 1529 \tTraining Loss: 0.112051 \tValidation Loss: 1.029957\n","Epoch: 1530 \tTraining Loss: 0.103594 \tValidation Loss: 1.024518\n","Epoch: 1531 \tTraining Loss: 0.103706 \tValidation Loss: 1.016408\n","Epoch: 1532 \tTraining Loss: 0.128626 \tValidation Loss: 1.007407\n","Epoch: 1533 \tTraining Loss: 0.095695 \tValidation Loss: 0.998424\n","Epoch: 1534 \tTraining Loss: 0.096612 \tValidation Loss: 1.025500\n","Epoch: 1535 \tTraining Loss: 0.088220 \tValidation Loss: 1.007002\n","Epoch: 1536 \tTraining Loss: 0.105084 \tValidation Loss: 1.025493\n","Epoch: 1537 \tTraining Loss: 0.095739 \tValidation Loss: 1.021290\n","Epoch: 1538 \tTraining Loss: 0.099883 \tValidation Loss: 1.012020\n","Epoch: 1539 \tTraining Loss: 0.093657 \tValidation Loss: 1.023519\n","Epoch: 1540 \tTraining Loss: 0.106871 \tValidation Loss: 1.052880\n","Epoch: 1541 \tTraining Loss: 0.104535 \tValidation Loss: 1.023199\n","Epoch: 1542 \tTraining Loss: 0.091894 \tValidation Loss: 1.023957\n","Epoch: 1543 \tTraining Loss: 0.114210 \tValidation Loss: 1.030155\n","Epoch: 1544 \tTraining Loss: 0.096844 \tValidation Loss: 1.026758\n","Epoch: 1545 \tTraining Loss: 0.107166 \tValidation Loss: 1.015966\n","Epoch: 1546 \tTraining Loss: 0.112509 \tValidation Loss: 1.028386\n","Epoch: 1547 \tTraining Loss: 0.083102 \tValidation Loss: 1.023075\n","Epoch: 1548 \tTraining Loss: 0.099122 \tValidation Loss: 1.024258\n","Epoch: 1549 \tTraining Loss: 0.099276 \tValidation Loss: 1.028011\n","Epoch: 1550 \tTraining Loss: 0.108314 \tValidation Loss: 1.017111\n","Epoch: 1551 \tTraining Loss: 0.132848 \tValidation Loss: 1.050439\n","Epoch: 1552 \tTraining Loss: 0.099131 \tValidation Loss: 1.042562\n","Epoch: 1553 \tTraining Loss: 0.109416 \tValidation Loss: 1.020211\n","Epoch: 1554 \tTraining Loss: 0.107052 \tValidation Loss: 1.011205\n","Epoch: 1555 \tTraining Loss: 0.108121 \tValidation Loss: 1.035475\n","Epoch: 1556 \tTraining Loss: 0.095254 \tValidation Loss: 1.027975\n","Epoch: 1557 \tTraining Loss: 0.121040 \tValidation Loss: 1.023802\n","Epoch: 1558 \tTraining Loss: 0.095365 \tValidation Loss: 1.034297\n","Epoch: 1559 \tTraining Loss: 0.108952 \tValidation Loss: 1.018658\n","Epoch: 1560 \tTraining Loss: 0.104634 \tValidation Loss: 1.025142\n","Epoch: 1561 \tTraining Loss: 0.096712 \tValidation Loss: 1.017006\n","Epoch: 1562 \tTraining Loss: 0.097187 \tValidation Loss: 1.024420\n","Epoch: 1563 \tTraining Loss: 0.093284 \tValidation Loss: 1.036265\n","Epoch: 1564 \tTraining Loss: 0.101186 \tValidation Loss: 1.020176\n","Epoch: 1565 \tTraining Loss: 0.089964 \tValidation Loss: 1.041647\n","Epoch: 1566 \tTraining Loss: 0.092971 \tValidation Loss: 1.032405\n","Epoch: 1567 \tTraining Loss: 0.112695 \tValidation Loss: 1.023986\n","Epoch: 1568 \tTraining Loss: 0.093389 \tValidation Loss: 1.027396\n","Epoch: 1569 \tTraining Loss: 0.096613 \tValidation Loss: 1.019405\n","Epoch: 1570 \tTraining Loss: 0.112195 \tValidation Loss: 1.024852\n","Epoch: 1571 \tTraining Loss: 0.092865 \tValidation Loss: 1.036861\n","Epoch: 1572 \tTraining Loss: 0.118218 \tValidation Loss: 1.020865\n","Epoch: 1573 \tTraining Loss: 0.088621 \tValidation Loss: 1.032194\n","Epoch: 1574 \tTraining Loss: 0.093745 \tValidation Loss: 1.043666\n","Epoch: 1575 \tTraining Loss: 0.111429 \tValidation Loss: 1.022092\n","Epoch: 1576 \tTraining Loss: 0.092643 \tValidation Loss: 1.027181\n","Epoch: 1577 \tTraining Loss: 0.092203 \tValidation Loss: 1.022026\n","Epoch: 1578 \tTraining Loss: 0.101941 \tValidation Loss: 1.006662\n","Epoch: 1579 \tTraining Loss: 0.088230 \tValidation Loss: 1.030433\n","Epoch: 1580 \tTraining Loss: 0.078084 \tValidation Loss: 1.010903\n","Epoch: 1581 \tTraining Loss: 0.089505 \tValidation Loss: 1.017173\n","Epoch: 1582 \tTraining Loss: 0.085220 \tValidation Loss: 1.033345\n","Epoch: 1583 \tTraining Loss: 0.091154 \tValidation Loss: 1.006618\n","Epoch: 1584 \tTraining Loss: 0.107173 \tValidation Loss: 1.031061\n","Epoch: 1585 \tTraining Loss: 0.087679 \tValidation Loss: 1.043141\n","Epoch: 1586 \tTraining Loss: 0.123844 \tValidation Loss: 1.016038\n","Epoch: 1587 \tTraining Loss: 0.106287 \tValidation Loss: 1.023238\n","Epoch: 1588 \tTraining Loss: 0.096288 \tValidation Loss: 1.027291\n","Epoch: 1589 \tTraining Loss: 0.092047 \tValidation Loss: 1.008547\n","Epoch: 1590 \tTraining Loss: 0.119238 \tValidation Loss: 1.041279\n","Epoch: 1591 \tTraining Loss: 0.082570 \tValidation Loss: 1.035922\n","Epoch: 1592 \tTraining Loss: 0.098288 \tValidation Loss: 1.038292\n","Epoch: 1593 \tTraining Loss: 0.087172 \tValidation Loss: 1.038903\n","Epoch: 1594 \tTraining Loss: 0.118112 \tValidation Loss: 1.029563\n","Epoch: 1595 \tTraining Loss: 0.102757 \tValidation Loss: 1.036649\n","Epoch: 1596 \tTraining Loss: 0.099132 \tValidation Loss: 1.021151\n","Epoch: 1597 \tTraining Loss: 0.083413 \tValidation Loss: 1.033485\n","Epoch: 1598 \tTraining Loss: 0.090728 \tValidation Loss: 1.012652\n","Epoch: 1599 \tTraining Loss: 0.087196 \tValidation Loss: 1.024689\n","Epoch: 1600 \tTraining Loss: 0.083555 \tValidation Loss: 1.044616\n","Epoch: 1601 \tTraining Loss: 0.119996 \tValidation Loss: 1.027181\n","Epoch: 1602 \tTraining Loss: 0.098662 \tValidation Loss: 1.034981\n","Epoch: 1603 \tTraining Loss: 0.100430 \tValidation Loss: 1.003400\n","Epoch: 1604 \tTraining Loss: 0.114377 \tValidation Loss: 1.021319\n","Epoch: 1605 \tTraining Loss: 0.099352 \tValidation Loss: 1.030054\n","Epoch: 1606 \tTraining Loss: 0.093491 \tValidation Loss: 1.039771\n","Epoch: 1607 \tTraining Loss: 0.102221 \tValidation Loss: 1.006968\n","Epoch: 1608 \tTraining Loss: 0.094239 \tValidation Loss: 1.043117\n","Epoch: 1609 \tTraining Loss: 0.100570 \tValidation Loss: 1.039845\n","Epoch: 1610 \tTraining Loss: 0.095989 \tValidation Loss: 1.029195\n","Epoch: 1611 \tTraining Loss: 0.093952 \tValidation Loss: 1.017490\n","Epoch: 1612 \tTraining Loss: 0.085365 \tValidation Loss: 1.027347\n","Epoch: 1613 \tTraining Loss: 0.095967 \tValidation Loss: 1.023035\n","Epoch: 1614 \tTraining Loss: 0.076849 \tValidation Loss: 1.031407\n","Epoch: 1615 \tTraining Loss: 0.087935 \tValidation Loss: 1.046269\n","Epoch: 1616 \tTraining Loss: 0.098643 \tValidation Loss: 1.045613\n","Epoch: 1617 \tTraining Loss: 0.099988 \tValidation Loss: 1.023194\n","Epoch: 1618 \tTraining Loss: 0.085104 \tValidation Loss: 1.031894\n","Epoch: 1619 \tTraining Loss: 0.103094 \tValidation Loss: 1.008151\n","Epoch: 1620 \tTraining Loss: 0.081283 \tValidation Loss: 1.007534\n","Epoch: 1621 \tTraining Loss: 0.098158 \tValidation Loss: 1.031206\n","Epoch: 1622 \tTraining Loss: 0.112669 \tValidation Loss: 1.022775\n","Epoch: 1623 \tTraining Loss: 0.103310 \tValidation Loss: 1.027876\n","Epoch: 1624 \tTraining Loss: 0.133740 \tValidation Loss: 1.016311\n","Epoch: 1625 \tTraining Loss: 0.099911 \tValidation Loss: 1.017536\n","Epoch: 1626 \tTraining Loss: 0.086314 \tValidation Loss: 1.032217\n","Epoch: 1627 \tTraining Loss: 0.086875 \tValidation Loss: 1.013721\n","Epoch: 1628 \tTraining Loss: 0.091757 \tValidation Loss: 1.021206\n","Epoch: 1629 \tTraining Loss: 0.092961 \tValidation Loss: 1.014272\n","Epoch: 1630 \tTraining Loss: 0.116830 \tValidation Loss: 1.034004\n","Epoch: 1631 \tTraining Loss: 0.096576 \tValidation Loss: 1.032970\n","Epoch: 1632 \tTraining Loss: 0.081853 \tValidation Loss: 1.032413\n","Epoch: 1633 \tTraining Loss: 0.102509 \tValidation Loss: 1.020378\n","Epoch: 1634 \tTraining Loss: 0.087427 \tValidation Loss: 1.028911\n","Epoch: 1635 \tTraining Loss: 0.120486 \tValidation Loss: 1.000894\n","Epoch: 1636 \tTraining Loss: 0.082608 \tValidation Loss: 1.029419\n","Epoch: 1637 \tTraining Loss: 0.084103 \tValidation Loss: 1.014382\n","Epoch: 1638 \tTraining Loss: 0.088257 \tValidation Loss: 1.024209\n","Epoch: 1639 \tTraining Loss: 0.113216 \tValidation Loss: 1.021890\n","Epoch: 1640 \tTraining Loss: 0.085591 \tValidation Loss: 1.023425\n","Epoch: 1641 \tTraining Loss: 0.071977 \tValidation Loss: 1.028253\n","Epoch: 1642 \tTraining Loss: 0.098806 \tValidation Loss: 1.025457\n","Epoch: 1643 \tTraining Loss: 0.098544 \tValidation Loss: 1.022429\n","Epoch: 1644 \tTraining Loss: 0.104124 \tValidation Loss: 1.027354\n","Epoch: 1645 \tTraining Loss: 0.080577 \tValidation Loss: 1.011023\n","Epoch: 1646 \tTraining Loss: 0.086987 \tValidation Loss: 1.022926\n","Epoch: 1647 \tTraining Loss: 0.098890 \tValidation Loss: 1.009304\n","Epoch: 1648 \tTraining Loss: 0.096778 \tValidation Loss: 1.027881\n","Epoch: 1649 \tTraining Loss: 0.094348 \tValidation Loss: 1.023808\n","Epoch: 1650 \tTraining Loss: 0.088991 \tValidation Loss: 1.006421\n","Epoch: 1651 \tTraining Loss: 0.098412 \tValidation Loss: 1.015038\n","Epoch: 1652 \tTraining Loss: 0.084819 \tValidation Loss: 1.020186\n","Epoch: 1653 \tTraining Loss: 0.088403 \tValidation Loss: 1.007396\n","Epoch: 1654 \tTraining Loss: 0.079270 \tValidation Loss: 1.018433\n","Epoch: 1655 \tTraining Loss: 0.121015 \tValidation Loss: 1.058131\n","Epoch: 1656 \tTraining Loss: 0.098336 \tValidation Loss: 1.020055\n","Epoch: 1657 \tTraining Loss: 0.100603 \tValidation Loss: 1.031543\n","Epoch: 1658 \tTraining Loss: 0.081894 \tValidation Loss: 1.005878\n","Epoch: 1659 \tTraining Loss: 0.112229 \tValidation Loss: 1.012365\n","Epoch: 1660 \tTraining Loss: 0.088595 \tValidation Loss: 1.012106\n","Epoch: 1661 \tTraining Loss: 0.086467 \tValidation Loss: 1.017655\n","Epoch: 1662 \tTraining Loss: 0.091753 \tValidation Loss: 1.028841\n","Epoch: 1663 \tTraining Loss: 0.087624 \tValidation Loss: 1.030475\n","Epoch: 1664 \tTraining Loss: 0.101457 \tValidation Loss: 1.035842\n","Epoch: 1665 \tTraining Loss: 0.106809 \tValidation Loss: 1.003529\n","Epoch: 1666 \tTraining Loss: 0.087067 \tValidation Loss: 1.035904\n","Epoch: 1667 \tTraining Loss: 0.079797 \tValidation Loss: 1.035028\n","Epoch: 1668 \tTraining Loss: 0.093160 \tValidation Loss: 0.989500\n","Validation loss has decreased (0.995772 --> 0.989500).  Saving model ...\n","Epoch: 1669 \tTraining Loss: 0.093239 \tValidation Loss: 1.027961\n","Epoch: 1670 \tTraining Loss: 0.077991 \tValidation Loss: 1.034820\n","Epoch: 1671 \tTraining Loss: 0.079843 \tValidation Loss: 1.029512\n","Epoch: 1672 \tTraining Loss: 0.101446 \tValidation Loss: 1.021224\n","Epoch: 1673 \tTraining Loss: 0.081089 \tValidation Loss: 1.034785\n","Epoch: 1674 \tTraining Loss: 0.094580 \tValidation Loss: 1.025632\n","Epoch: 1675 \tTraining Loss: 0.084350 \tValidation Loss: 1.048490\n","Epoch: 1676 \tTraining Loss: 0.107270 \tValidation Loss: 1.019861\n","Epoch: 1677 \tTraining Loss: 0.095741 \tValidation Loss: 1.025650\n","Epoch: 1678 \tTraining Loss: 0.080331 \tValidation Loss: 1.021394\n","Epoch: 1679 \tTraining Loss: 0.096158 \tValidation Loss: 1.024414\n","Epoch: 1680 \tTraining Loss: 0.080351 \tValidation Loss: 1.031896\n","Epoch: 1681 \tTraining Loss: 0.077551 \tValidation Loss: 1.036623\n","Epoch: 1682 \tTraining Loss: 0.080311 \tValidation Loss: 1.007666\n","Epoch: 1683 \tTraining Loss: 0.096522 \tValidation Loss: 1.011484\n","Epoch: 1684 \tTraining Loss: 0.096403 \tValidation Loss: 1.031624\n","Epoch: 1685 \tTraining Loss: 0.066733 \tValidation Loss: 1.042083\n","Epoch: 1686 \tTraining Loss: 0.111278 \tValidation Loss: 1.024288\n","Epoch: 1687 \tTraining Loss: 0.075910 \tValidation Loss: 1.024430\n","Epoch: 1688 \tTraining Loss: 0.075935 \tValidation Loss: 1.020239\n","Epoch: 1689 \tTraining Loss: 0.090550 \tValidation Loss: 1.006295\n","Epoch: 1690 \tTraining Loss: 0.081033 \tValidation Loss: 1.029891\n","Epoch: 1691 \tTraining Loss: 0.092461 \tValidation Loss: 1.016920\n","Epoch: 1692 \tTraining Loss: 0.120350 \tValidation Loss: 1.028871\n","Epoch: 1693 \tTraining Loss: 0.103330 \tValidation Loss: 1.023982\n","Epoch: 1694 \tTraining Loss: 0.094888 \tValidation Loss: 1.030111\n","Epoch: 1695 \tTraining Loss: 0.084342 \tValidation Loss: 1.023964\n","Epoch: 1696 \tTraining Loss: 0.085209 \tValidation Loss: 1.022789\n","Epoch: 1697 \tTraining Loss: 0.104445 \tValidation Loss: 1.029591\n","Epoch: 1698 \tTraining Loss: 0.080333 \tValidation Loss: 1.040098\n","Epoch: 1699 \tTraining Loss: 0.083389 \tValidation Loss: 1.032923\n","Epoch: 1700 \tTraining Loss: 0.090162 \tValidation Loss: 1.039057\n","Epoch: 1701 \tTraining Loss: 0.092814 \tValidation Loss: 1.023526\n","Epoch: 1702 \tTraining Loss: 0.085889 \tValidation Loss: 1.018481\n","Epoch: 1703 \tTraining Loss: 0.090293 \tValidation Loss: 1.013211\n","Epoch: 1704 \tTraining Loss: 0.074092 \tValidation Loss: 1.018709\n","Epoch: 1705 \tTraining Loss: 0.079415 \tValidation Loss: 1.007905\n","Epoch: 1706 \tTraining Loss: 0.089488 \tValidation Loss: 1.029870\n","Epoch: 1707 \tTraining Loss: 0.083962 \tValidation Loss: 1.011934\n","Epoch: 1708 \tTraining Loss: 0.081268 \tValidation Loss: 1.026333\n","Epoch: 1709 \tTraining Loss: 0.084895 \tValidation Loss: 1.021098\n","Epoch: 1710 \tTraining Loss: 0.072553 \tValidation Loss: 1.035791\n","Epoch: 1711 \tTraining Loss: 0.078109 \tValidation Loss: 1.040678\n","Epoch: 1712 \tTraining Loss: 0.091499 \tValidation Loss: 1.022136\n","Epoch: 1713 \tTraining Loss: 0.076582 \tValidation Loss: 1.037456\n","Epoch: 1714 \tTraining Loss: 0.079745 \tValidation Loss: 1.036586\n","Epoch: 1715 \tTraining Loss: 0.091240 \tValidation Loss: 1.009742\n","Epoch: 1716 \tTraining Loss: 0.088467 \tValidation Loss: 1.038993\n","Epoch: 1717 \tTraining Loss: 0.089971 \tValidation Loss: 1.013085\n","Epoch: 1718 \tTraining Loss: 0.096063 \tValidation Loss: 1.020785\n","Epoch: 1719 \tTraining Loss: 0.089931 \tValidation Loss: 1.036878\n","Epoch: 1720 \tTraining Loss: 0.096739 \tValidation Loss: 1.008488\n","Epoch: 1721 \tTraining Loss: 0.071113 \tValidation Loss: 1.024132\n","Epoch: 1722 \tTraining Loss: 0.088457 \tValidation Loss: 1.003048\n","Epoch: 1723 \tTraining Loss: 0.088315 \tValidation Loss: 1.006163\n","Epoch: 1724 \tTraining Loss: 0.084745 \tValidation Loss: 1.035417\n","Epoch: 1725 \tTraining Loss: 0.087399 \tValidation Loss: 1.033909\n","Epoch: 1726 \tTraining Loss: 0.071296 \tValidation Loss: 1.014574\n","Epoch: 1727 \tTraining Loss: 0.072096 \tValidation Loss: 1.035528\n","Epoch: 1728 \tTraining Loss: 0.094845 \tValidation Loss: 1.019635\n","Epoch: 1729 \tTraining Loss: 0.081638 \tValidation Loss: 1.024343\n","Epoch: 1730 \tTraining Loss: 0.088488 \tValidation Loss: 1.021819\n","Epoch: 1731 \tTraining Loss: 0.075808 \tValidation Loss: 1.028679\n","Epoch: 1732 \tTraining Loss: 0.078016 \tValidation Loss: 1.001479\n","Epoch: 1733 \tTraining Loss: 0.100214 \tValidation Loss: 1.024212\n","Epoch: 1734 \tTraining Loss: 0.074833 \tValidation Loss: 1.045760\n","Epoch: 1735 \tTraining Loss: 0.077637 \tValidation Loss: 1.008925\n","Epoch: 1736 \tTraining Loss: 0.067086 \tValidation Loss: 1.010786\n","Epoch: 1737 \tTraining Loss: 0.088429 \tValidation Loss: 1.025518\n","Epoch: 1738 \tTraining Loss: 0.075892 \tValidation Loss: 1.028812\n","Epoch: 1739 \tTraining Loss: 0.081967 \tValidation Loss: 0.993485\n","Epoch: 1740 \tTraining Loss: 0.084136 \tValidation Loss: 1.013886\n","Epoch: 1741 \tTraining Loss: 0.073953 \tValidation Loss: 1.033741\n","Epoch: 1742 \tTraining Loss: 0.094674 \tValidation Loss: 1.023826\n","Epoch: 1743 \tTraining Loss: 0.083016 \tValidation Loss: 1.021381\n","Epoch: 1744 \tTraining Loss: 0.074453 \tValidation Loss: 1.019785\n","Epoch: 1745 \tTraining Loss: 0.063471 \tValidation Loss: 1.013189\n","Epoch: 1746 \tTraining Loss: 0.078410 \tValidation Loss: 1.019456\n","Epoch: 1747 \tTraining Loss: 0.081945 \tValidation Loss: 1.035266\n","Epoch: 1748 \tTraining Loss: 0.089924 \tValidation Loss: 1.013177\n","Epoch: 1749 \tTraining Loss: 0.070752 \tValidation Loss: 1.017134\n","Epoch: 1750 \tTraining Loss: 0.084314 \tValidation Loss: 1.004030\n","Epoch: 1751 \tTraining Loss: 0.087729 \tValidation Loss: 1.042193\n","Epoch: 1752 \tTraining Loss: 0.085179 \tValidation Loss: 1.021558\n","Epoch: 1753 \tTraining Loss: 0.097584 \tValidation Loss: 1.032308\n","Epoch: 1754 \tTraining Loss: 0.076455 \tValidation Loss: 1.037767\n","Epoch: 1755 \tTraining Loss: 0.088614 \tValidation Loss: 1.027401\n","Epoch: 1756 \tTraining Loss: 0.093784 \tValidation Loss: 1.028534\n","Epoch: 1757 \tTraining Loss: 0.081043 \tValidation Loss: 1.014531\n","Epoch: 1758 \tTraining Loss: 0.085452 \tValidation Loss: 1.009415\n","Epoch: 1759 \tTraining Loss: 0.082399 \tValidation Loss: 1.007069\n","Epoch: 1760 \tTraining Loss: 0.083310 \tValidation Loss: 1.018829\n","Epoch: 1761 \tTraining Loss: 0.085352 \tValidation Loss: 1.012659\n","Epoch: 1762 \tTraining Loss: 0.093110 \tValidation Loss: 1.007453\n","Epoch: 1763 \tTraining Loss: 0.077188 \tValidation Loss: 1.007681\n","Epoch: 1764 \tTraining Loss: 0.069147 \tValidation Loss: 0.994032\n","Epoch: 1765 \tTraining Loss: 0.088861 \tValidation Loss: 1.013528\n","Epoch: 1766 \tTraining Loss: 0.083875 \tValidation Loss: 1.013457\n","Epoch: 1767 \tTraining Loss: 0.076349 \tValidation Loss: 1.023062\n","Epoch: 1768 \tTraining Loss: 0.083734 \tValidation Loss: 1.012784\n","Epoch: 1769 \tTraining Loss: 0.092919 \tValidation Loss: 1.011796\n","Epoch: 1770 \tTraining Loss: 0.083901 \tValidation Loss: 1.008806\n","Epoch: 1771 \tTraining Loss: 0.095320 \tValidation Loss: 1.020964\n","Epoch: 1772 \tTraining Loss: 0.095418 \tValidation Loss: 1.021202\n","Epoch: 1773 \tTraining Loss: 0.066454 \tValidation Loss: 1.027539\n","Epoch: 1774 \tTraining Loss: 0.080121 \tValidation Loss: 1.029170\n","Epoch: 1775 \tTraining Loss: 0.070074 \tValidation Loss: 1.045875\n","Epoch: 1776 \tTraining Loss: 0.084370 \tValidation Loss: 1.034271\n","Epoch: 1777 \tTraining Loss: 0.093386 \tValidation Loss: 1.002566\n","Epoch: 1778 \tTraining Loss: 0.083424 \tValidation Loss: 1.017915\n","Epoch: 1779 \tTraining Loss: 0.075168 \tValidation Loss: 1.022498\n","Epoch: 1780 \tTraining Loss: 0.066833 \tValidation Loss: 1.013253\n","Epoch: 1781 \tTraining Loss: 0.082421 \tValidation Loss: 1.008688\n","Epoch: 1782 \tTraining Loss: 0.085691 \tValidation Loss: 1.014293\n","Epoch: 1783 \tTraining Loss: 0.081865 \tValidation Loss: 1.047213\n","Epoch: 1784 \tTraining Loss: 0.066054 \tValidation Loss: 1.036835\n","Epoch: 1785 \tTraining Loss: 0.076848 \tValidation Loss: 1.047815\n","Epoch: 1786 \tTraining Loss: 0.075396 \tValidation Loss: 1.020549\n","Epoch: 1787 \tTraining Loss: 0.083756 \tValidation Loss: 1.036586\n","Epoch: 1788 \tTraining Loss: 0.079113 \tValidation Loss: 1.020226\n","Epoch: 1789 \tTraining Loss: 0.073687 \tValidation Loss: 1.016188\n","Epoch: 1790 \tTraining Loss: 0.065674 \tValidation Loss: 1.014999\n","Epoch: 1791 \tTraining Loss: 0.065820 \tValidation Loss: 1.017052\n","Epoch: 1792 \tTraining Loss: 0.073193 \tValidation Loss: 1.040001\n","Epoch: 1793 \tTraining Loss: 0.075539 \tValidation Loss: 1.025497\n","Epoch: 1794 \tTraining Loss: 0.068045 \tValidation Loss: 1.038596\n","Epoch: 1795 \tTraining Loss: 0.061463 \tValidation Loss: 1.042157\n","Epoch: 1796 \tTraining Loss: 0.073810 \tValidation Loss: 1.057198\n","Epoch: 1797 \tTraining Loss: 0.082671 \tValidation Loss: 1.008233\n","Epoch: 1798 \tTraining Loss: 0.083043 \tValidation Loss: 1.033896\n","Epoch: 1799 \tTraining Loss: 0.075588 \tValidation Loss: 1.012622\n","Epoch: 1800 \tTraining Loss: 0.085931 \tValidation Loss: 1.024767\n","Epoch: 1801 \tTraining Loss: 0.088926 \tValidation Loss: 1.020914\n","Epoch: 1802 \tTraining Loss: 0.074529 \tValidation Loss: 1.016154\n","Epoch: 1803 \tTraining Loss: 0.087757 \tValidation Loss: 1.025479\n","Epoch: 1804 \tTraining Loss: 0.070048 \tValidation Loss: 1.022913\n","Epoch: 1805 \tTraining Loss: 0.081592 \tValidation Loss: 1.037888\n","Epoch: 1806 \tTraining Loss: 0.085553 \tValidation Loss: 1.023522\n","Epoch: 1807 \tTraining Loss: 0.074404 \tValidation Loss: 1.012395\n","Epoch: 1808 \tTraining Loss: 0.080606 \tValidation Loss: 1.029315\n","Epoch: 1809 \tTraining Loss: 0.060241 \tValidation Loss: 1.028333\n","Epoch: 1810 \tTraining Loss: 0.080425 \tValidation Loss: 1.019650\n","Epoch: 1811 \tTraining Loss: 0.085764 \tValidation Loss: 1.033723\n","Epoch: 1812 \tTraining Loss: 0.082381 \tValidation Loss: 1.045649\n","Epoch: 1813 \tTraining Loss: 0.073518 \tValidation Loss: 1.014538\n","Epoch: 1814 \tTraining Loss: 0.072709 \tValidation Loss: 1.021332\n","Epoch: 1815 \tTraining Loss: 0.078698 \tValidation Loss: 1.043418\n","Epoch: 1816 \tTraining Loss: 0.078785 \tValidation Loss: 1.023897\n","Epoch: 1817 \tTraining Loss: 0.081108 \tValidation Loss: 1.028403\n","Epoch: 1818 \tTraining Loss: 0.065965 \tValidation Loss: 1.025387\n","Epoch: 1819 \tTraining Loss: 0.071783 \tValidation Loss: 1.035284\n","Epoch: 1820 \tTraining Loss: 0.071097 \tValidation Loss: 1.028050\n","Epoch: 1821 \tTraining Loss: 0.081031 \tValidation Loss: 1.026389\n","Epoch: 1822 \tTraining Loss: 0.085326 \tValidation Loss: 1.036249\n","Epoch: 1823 \tTraining Loss: 0.090810 \tValidation Loss: 1.033165\n","Epoch: 1824 \tTraining Loss: 0.066722 \tValidation Loss: 1.043285\n","Epoch: 1825 \tTraining Loss: 0.077461 \tValidation Loss: 1.002670\n","Epoch: 1826 \tTraining Loss: 0.074718 \tValidation Loss: 1.035787\n","Epoch: 1827 \tTraining Loss: 0.075817 \tValidation Loss: 1.045254\n","Epoch: 1828 \tTraining Loss: 0.070637 \tValidation Loss: 1.019618\n","Epoch: 1829 \tTraining Loss: 0.077222 \tValidation Loss: 1.026921\n","Epoch: 1830 \tTraining Loss: 0.064800 \tValidation Loss: 1.016787\n","Epoch: 1831 \tTraining Loss: 0.078231 \tValidation Loss: 1.017810\n","Epoch: 1832 \tTraining Loss: 0.074538 \tValidation Loss: 1.016064\n","Epoch: 1833 \tTraining Loss: 0.065557 \tValidation Loss: 1.022189\n","Epoch: 1834 \tTraining Loss: 0.076335 \tValidation Loss: 1.025483\n","Epoch: 1835 \tTraining Loss: 0.082682 \tValidation Loss: 1.024993\n","Epoch: 1836 \tTraining Loss: 0.077306 \tValidation Loss: 1.028191\n","Epoch: 1837 \tTraining Loss: 0.090347 \tValidation Loss: 1.009422\n","Epoch: 1838 \tTraining Loss: 0.078070 \tValidation Loss: 1.034012\n","Epoch: 1839 \tTraining Loss: 0.079448 \tValidation Loss: 1.016013\n","Epoch: 1840 \tTraining Loss: 0.078607 \tValidation Loss: 1.027541\n","Epoch: 1841 \tTraining Loss: 0.077965 \tValidation Loss: 1.010876\n","Epoch: 1842 \tTraining Loss: 0.078405 \tValidation Loss: 1.005102\n","Epoch: 1843 \tTraining Loss: 0.088296 \tValidation Loss: 1.015392\n","Epoch: 1844 \tTraining Loss: 0.080176 \tValidation Loss: 1.044057\n","Epoch: 1845 \tTraining Loss: 0.086383 \tValidation Loss: 1.021170\n","Epoch: 1846 \tTraining Loss: 0.100474 \tValidation Loss: 1.024960\n","Epoch: 1847 \tTraining Loss: 0.071852 \tValidation Loss: 1.007864\n","Epoch: 1848 \tTraining Loss: 0.095396 \tValidation Loss: 1.015180\n","Epoch: 1849 \tTraining Loss: 0.069097 \tValidation Loss: 1.013143\n","Epoch: 1850 \tTraining Loss: 0.080909 \tValidation Loss: 1.015738\n","Epoch: 1851 \tTraining Loss: 0.087423 \tValidation Loss: 1.032453\n","Epoch: 1852 \tTraining Loss: 0.083736 \tValidation Loss: 1.010226\n","Epoch: 1853 \tTraining Loss: 0.084305 \tValidation Loss: 1.011624\n","Epoch: 1854 \tTraining Loss: 0.064082 \tValidation Loss: 1.013224\n","Epoch: 1855 \tTraining Loss: 0.074301 \tValidation Loss: 1.026909\n","Epoch: 1856 \tTraining Loss: 0.081687 \tValidation Loss: 1.037874\n","Epoch: 1857 \tTraining Loss: 0.088274 \tValidation Loss: 1.036457\n","Epoch: 1858 \tTraining Loss: 0.083846 \tValidation Loss: 1.023651\n","Epoch: 1859 \tTraining Loss: 0.061992 \tValidation Loss: 1.038308\n","Epoch: 1860 \tTraining Loss: 0.069010 \tValidation Loss: 1.016737\n","Epoch: 1861 \tTraining Loss: 0.081023 \tValidation Loss: 1.031076\n","Epoch: 1862 \tTraining Loss: 0.077520 \tValidation Loss: 1.041359\n","Epoch: 1863 \tTraining Loss: 0.084722 \tValidation Loss: 1.019342\n","Epoch: 1864 \tTraining Loss: 0.091964 \tValidation Loss: 1.025673\n","Epoch: 1865 \tTraining Loss: 0.064932 \tValidation Loss: 1.030599\n","Epoch: 1866 \tTraining Loss: 0.065403 \tValidation Loss: 1.038775\n","Epoch: 1867 \tTraining Loss: 0.069917 \tValidation Loss: 1.042961\n","Epoch: 1868 \tTraining Loss: 0.071372 \tValidation Loss: 1.049477\n","Epoch: 1869 \tTraining Loss: 0.062369 \tValidation Loss: 1.039413\n","Epoch: 1870 \tTraining Loss: 0.068223 \tValidation Loss: 1.022073\n","Epoch: 1871 \tTraining Loss: 0.080747 \tValidation Loss: 1.035189\n","Epoch: 1872 \tTraining Loss: 0.081045 \tValidation Loss: 1.039999\n","Epoch: 1873 \tTraining Loss: 0.077741 \tValidation Loss: 1.062201\n","Epoch: 1874 \tTraining Loss: 0.067244 \tValidation Loss: 1.035756\n","Epoch: 1875 \tTraining Loss: 0.076031 \tValidation Loss: 1.023814\n","Epoch: 1876 \tTraining Loss: 0.085383 \tValidation Loss: 0.998827\n","Epoch: 1877 \tTraining Loss: 0.086768 \tValidation Loss: 1.009484\n","Epoch: 1878 \tTraining Loss: 0.071536 \tValidation Loss: 1.021614\n","Epoch: 1879 \tTraining Loss: 0.085094 \tValidation Loss: 1.022880\n","Epoch: 1880 \tTraining Loss: 0.077292 \tValidation Loss: 1.021825\n","Epoch: 1881 \tTraining Loss: 0.060744 \tValidation Loss: 1.025393\n","Epoch: 1882 \tTraining Loss: 0.083940 \tValidation Loss: 1.024972\n","Epoch: 1883 \tTraining Loss: 0.073126 \tValidation Loss: 1.034614\n","Epoch: 1884 \tTraining Loss: 0.077860 \tValidation Loss: 1.018703\n","Epoch: 1885 \tTraining Loss: 0.066951 \tValidation Loss: 1.037416\n","Epoch: 1886 \tTraining Loss: 0.073240 \tValidation Loss: 1.034339\n","Epoch: 1887 \tTraining Loss: 0.078502 \tValidation Loss: 1.012849\n","Epoch: 1888 \tTraining Loss: 0.083861 \tValidation Loss: 1.009185\n","Epoch: 1889 \tTraining Loss: 0.094819 \tValidation Loss: 1.054510\n","Epoch: 1890 \tTraining Loss: 0.061443 \tValidation Loss: 1.010367\n","Epoch: 1891 \tTraining Loss: 0.069668 \tValidation Loss: 1.022120\n","Epoch: 1892 \tTraining Loss: 0.100152 \tValidation Loss: 1.024612\n","Epoch: 1893 \tTraining Loss: 0.075979 \tValidation Loss: 1.039361\n","Epoch: 1894 \tTraining Loss: 0.081181 \tValidation Loss: 1.051642\n","Epoch: 1895 \tTraining Loss: 0.077770 \tValidation Loss: 1.019547\n","Epoch: 1896 \tTraining Loss: 0.075152 \tValidation Loss: 1.019405\n","Epoch: 1897 \tTraining Loss: 0.086670 \tValidation Loss: 1.027475\n","Epoch: 1898 \tTraining Loss: 0.069349 \tValidation Loss: 1.027406\n","Epoch: 1899 \tTraining Loss: 0.071494 \tValidation Loss: 1.044679\n","Epoch: 1900 \tTraining Loss: 0.073234 \tValidation Loss: 1.031138\n","Epoch: 1901 \tTraining Loss: 0.054030 \tValidation Loss: 1.030204\n","Epoch: 1902 \tTraining Loss: 0.066161 \tValidation Loss: 1.038151\n","Epoch: 1903 \tTraining Loss: 0.074363 \tValidation Loss: 1.013812\n","Epoch: 1904 \tTraining Loss: 0.080972 \tValidation Loss: 1.013754\n","Epoch: 1905 \tTraining Loss: 0.075729 \tValidation Loss: 1.022674\n","Epoch: 1906 \tTraining Loss: 0.070285 \tValidation Loss: 1.043115\n","Epoch: 1907 \tTraining Loss: 0.074604 \tValidation Loss: 1.039462\n","Epoch: 1908 \tTraining Loss: 0.082876 \tValidation Loss: 1.044952\n","Epoch: 1909 \tTraining Loss: 0.071071 \tValidation Loss: 1.041195\n","Epoch: 1910 \tTraining Loss: 0.076062 \tValidation Loss: 1.009003\n","Epoch: 1911 \tTraining Loss: 0.063804 \tValidation Loss: 1.029919\n","Epoch: 1912 \tTraining Loss: 0.063165 \tValidation Loss: 1.038937\n","Epoch: 1913 \tTraining Loss: 0.077987 \tValidation Loss: 1.036534\n","Epoch: 1914 \tTraining Loss: 0.050472 \tValidation Loss: 1.021274\n","Epoch: 1915 \tTraining Loss: 0.081498 \tValidation Loss: 1.044181\n","Epoch: 1916 \tTraining Loss: 0.067140 \tValidation Loss: 1.013749\n","Epoch: 1917 \tTraining Loss: 0.068380 \tValidation Loss: 1.043458\n","Epoch: 1918 \tTraining Loss: 0.076641 \tValidation Loss: 1.024699\n","Epoch: 1919 \tTraining Loss: 0.058116 \tValidation Loss: 1.038108\n","Epoch: 1920 \tTraining Loss: 0.075712 \tValidation Loss: 1.015702\n","Epoch: 1921 \tTraining Loss: 0.069845 \tValidation Loss: 1.013632\n","Epoch: 1922 \tTraining Loss: 0.074185 \tValidation Loss: 1.037877\n","Epoch: 1923 \tTraining Loss: 0.067216 \tValidation Loss: 1.020671\n","Epoch: 1924 \tTraining Loss: 0.052881 \tValidation Loss: 1.014480\n","Epoch: 1925 \tTraining Loss: 0.080500 \tValidation Loss: 1.034485\n","Epoch: 1926 \tTraining Loss: 0.071228 \tValidation Loss: 1.026324\n","Epoch: 1927 \tTraining Loss: 0.059243 \tValidation Loss: 1.022242\n","Epoch: 1928 \tTraining Loss: 0.078288 \tValidation Loss: 1.018769\n","Epoch: 1929 \tTraining Loss: 0.073518 \tValidation Loss: 1.012457\n","Epoch: 1930 \tTraining Loss: 0.065858 \tValidation Loss: 1.009273\n","Epoch: 1931 \tTraining Loss: 0.067596 \tValidation Loss: 1.023779\n","Epoch: 1932 \tTraining Loss: 0.066967 \tValidation Loss: 1.019608\n","Epoch: 1933 \tTraining Loss: 0.100339 \tValidation Loss: 1.017870\n","Epoch: 1934 \tTraining Loss: 0.073909 \tValidation Loss: 1.009430\n","Epoch: 1935 \tTraining Loss: 0.069740 \tValidation Loss: 1.020663\n","Epoch: 1936 \tTraining Loss: 0.073636 \tValidation Loss: 1.013072\n","Epoch: 1937 \tTraining Loss: 0.060822 \tValidation Loss: 1.016915\n","Epoch: 1938 \tTraining Loss: 0.072557 \tValidation Loss: 1.011919\n","Epoch: 1939 \tTraining Loss: 0.067365 \tValidation Loss: 1.047738\n","Epoch: 1940 \tTraining Loss: 0.066903 \tValidation Loss: 1.028204\n","Epoch: 1941 \tTraining Loss: 0.078041 \tValidation Loss: 1.024821\n","Epoch: 1942 \tTraining Loss: 0.059692 \tValidation Loss: 1.007208\n","Epoch: 1943 \tTraining Loss: 0.073913 \tValidation Loss: 1.071087\n","Epoch: 1944 \tTraining Loss: 0.071484 \tValidation Loss: 1.027862\n","Epoch: 1945 \tTraining Loss: 0.076746 \tValidation Loss: 1.026389\n","Epoch: 1946 \tTraining Loss: 0.060510 \tValidation Loss: 1.020557\n","Epoch: 1947 \tTraining Loss: 0.070839 \tValidation Loss: 1.034603\n","Epoch: 1948 \tTraining Loss: 0.072242 \tValidation Loss: 1.004109\n","Epoch: 1949 \tTraining Loss: 0.080185 \tValidation Loss: 1.041880\n","Epoch: 1950 \tTraining Loss: 0.072456 \tValidation Loss: 1.009734\n","Epoch: 1951 \tTraining Loss: 0.078041 \tValidation Loss: 1.036952\n","Epoch: 1952 \tTraining Loss: 0.057311 \tValidation Loss: 1.041417\n","Epoch: 1953 \tTraining Loss: 0.085542 \tValidation Loss: 1.020097\n","Epoch: 1954 \tTraining Loss: 0.070355 \tValidation Loss: 1.043290\n","Epoch: 1955 \tTraining Loss: 0.070706 \tValidation Loss: 1.008651\n","Epoch: 1956 \tTraining Loss: 0.063311 \tValidation Loss: 1.038573\n","Epoch: 1957 \tTraining Loss: 0.063894 \tValidation Loss: 1.022048\n","Epoch: 1958 \tTraining Loss: 0.069812 \tValidation Loss: 1.024397\n","Epoch: 1959 \tTraining Loss: 0.072067 \tValidation Loss: 1.022319\n","Epoch: 1960 \tTraining Loss: 0.062175 \tValidation Loss: 1.017054\n","Epoch: 1961 \tTraining Loss: 0.063506 \tValidation Loss: 1.034397\n","Epoch: 1962 \tTraining Loss: 0.062706 \tValidation Loss: 1.029759\n","Epoch: 1963 \tTraining Loss: 0.082132 \tValidation Loss: 1.012836\n","Epoch: 1964 \tTraining Loss: 0.061266 \tValidation Loss: 1.016927\n","Epoch: 1965 \tTraining Loss: 0.065526 \tValidation Loss: 1.021332\n","Epoch: 1966 \tTraining Loss: 0.071151 \tValidation Loss: 1.028146\n","Epoch: 1967 \tTraining Loss: 0.072920 \tValidation Loss: 1.024418\n","Epoch: 1968 \tTraining Loss: 0.058986 \tValidation Loss: 1.047805\n","Epoch: 1969 \tTraining Loss: 0.071349 \tValidation Loss: 1.018807\n","Epoch: 1970 \tTraining Loss: 0.060428 \tValidation Loss: 1.029015\n","Epoch: 1971 \tTraining Loss: 0.077639 \tValidation Loss: 1.026543\n","Epoch: 1972 \tTraining Loss: 0.054990 \tValidation Loss: 1.023298\n","Epoch: 1973 \tTraining Loss: 0.076362 \tValidation Loss: 1.025136\n","Epoch: 1974 \tTraining Loss: 0.065065 \tValidation Loss: 1.006201\n","Epoch: 1975 \tTraining Loss: 0.065186 \tValidation Loss: 1.022759\n","Epoch: 1976 \tTraining Loss: 0.068592 \tValidation Loss: 1.019768\n","Epoch: 1977 \tTraining Loss: 0.079389 \tValidation Loss: 1.033271\n","Epoch: 1978 \tTraining Loss: 0.073768 \tValidation Loss: 1.020576\n","Epoch: 1979 \tTraining Loss: 0.059393 \tValidation Loss: 1.025650\n","Epoch: 1980 \tTraining Loss: 0.053133 \tValidation Loss: 1.017200\n","Epoch: 1981 \tTraining Loss: 0.076112 \tValidation Loss: 1.018702\n","Epoch: 1982 \tTraining Loss: 0.083690 \tValidation Loss: 1.034379\n","Epoch: 1983 \tTraining Loss: 0.068740 \tValidation Loss: 1.037900\n","Epoch: 1984 \tTraining Loss: 0.078828 \tValidation Loss: 1.015922\n","Epoch: 1985 \tTraining Loss: 0.072164 \tValidation Loss: 1.039034\n","Epoch: 1986 \tTraining Loss: 0.076152 \tValidation Loss: 1.022296\n","Epoch: 1987 \tTraining Loss: 0.054599 \tValidation Loss: 1.022742\n","Epoch: 1988 \tTraining Loss: 0.065972 \tValidation Loss: 1.035619\n","Epoch: 1989 \tTraining Loss: 0.069306 \tValidation Loss: 1.037501\n","Epoch: 1990 \tTraining Loss: 0.055254 \tValidation Loss: 1.040363\n","Epoch: 1991 \tTraining Loss: 0.068261 \tValidation Loss: 1.031624\n","Epoch: 1992 \tTraining Loss: 0.077895 \tValidation Loss: 1.005406\n","Epoch: 1993 \tTraining Loss: 0.052211 \tValidation Loss: 0.995344\n","Epoch: 1994 \tTraining Loss: 0.053167 \tValidation Loss: 1.043951\n","Epoch: 1995 \tTraining Loss: 0.058794 \tValidation Loss: 1.017557\n","Epoch: 1996 \tTraining Loss: 0.052087 \tValidation Loss: 1.030135\n","Epoch: 1997 \tTraining Loss: 0.063636 \tValidation Loss: 1.034731\n","Epoch: 1998 \tTraining Loss: 0.062657 \tValidation Loss: 1.016129\n","Epoch: 1999 \tTraining Loss: 0.062911 \tValidation Loss: 1.022104\n","Epoch: 2000 \tTraining Loss: 0.068882 \tValidation Loss: 1.014840\n","Epoch: 2001 \tTraining Loss: 0.064140 \tValidation Loss: 1.013013\n","Epoch: 2002 \tTraining Loss: 0.064659 \tValidation Loss: 1.042832\n","Epoch: 2003 \tTraining Loss: 0.061556 \tValidation Loss: 1.036259\n","Epoch: 2004 \tTraining Loss: 0.090295 \tValidation Loss: 1.025937\n","Epoch: 2005 \tTraining Loss: 0.047844 \tValidation Loss: 1.031815\n","Epoch: 2006 \tTraining Loss: 0.057066 \tValidation Loss: 1.039566\n","Epoch: 2007 \tTraining Loss: 0.074352 \tValidation Loss: 1.024317\n","Epoch: 2008 \tTraining Loss: 0.066856 \tValidation Loss: 1.046125\n","Epoch: 2009 \tTraining Loss: 0.052050 \tValidation Loss: 1.054820\n","Epoch: 2010 \tTraining Loss: 0.068322 \tValidation Loss: 1.039697\n","Epoch: 2011 \tTraining Loss: 0.058134 \tValidation Loss: 1.020193\n","Epoch: 2012 \tTraining Loss: 0.066337 \tValidation Loss: 1.025628\n","Epoch: 2013 \tTraining Loss: 0.074412 \tValidation Loss: 1.009900\n","Epoch: 2014 \tTraining Loss: 0.076735 \tValidation Loss: 1.038595\n","Epoch: 2015 \tTraining Loss: 0.086682 \tValidation Loss: 1.002683\n","Epoch: 2016 \tTraining Loss: 0.076724 \tValidation Loss: 1.019645\n","Epoch: 2017 \tTraining Loss: 0.062002 \tValidation Loss: 1.031695\n","Epoch: 2018 \tTraining Loss: 0.060974 \tValidation Loss: 1.045472\n","Epoch: 2019 \tTraining Loss: 0.070871 \tValidation Loss: 1.038687\n","Epoch: 2020 \tTraining Loss: 0.061926 \tValidation Loss: 1.024237\n","Epoch: 2021 \tTraining Loss: 0.062028 \tValidation Loss: 1.036143\n","Epoch: 2022 \tTraining Loss: 0.051096 \tValidation Loss: 1.003595\n","Epoch: 2023 \tTraining Loss: 0.054679 \tValidation Loss: 1.041909\n","Epoch: 2024 \tTraining Loss: 0.066894 \tValidation Loss: 1.012833\n","Epoch: 2025 \tTraining Loss: 0.054302 \tValidation Loss: 1.037452\n","Epoch: 2026 \tTraining Loss: 0.068134 \tValidation Loss: 1.048887\n","Epoch: 2027 \tTraining Loss: 0.072501 \tValidation Loss: 1.032876\n","Epoch: 2028 \tTraining Loss: 0.073379 \tValidation Loss: 1.013375\n","Epoch: 2029 \tTraining Loss: 0.069428 \tValidation Loss: 1.025387\n","Epoch: 2030 \tTraining Loss: 0.051485 \tValidation Loss: 1.033061\n","Epoch: 2031 \tTraining Loss: 0.067585 \tValidation Loss: 1.033777\n","Epoch: 2032 \tTraining Loss: 0.057976 \tValidation Loss: 1.032634\n","Epoch: 2033 \tTraining Loss: 0.064280 \tValidation Loss: 1.014303\n","Epoch: 2034 \tTraining Loss: 0.065421 \tValidation Loss: 1.047212\n","Epoch: 2035 \tTraining Loss: 0.054524 \tValidation Loss: 1.006968\n","Epoch: 2036 \tTraining Loss: 0.065177 \tValidation Loss: 1.045988\n","Epoch: 2037 \tTraining Loss: 0.062428 \tValidation Loss: 1.030159\n","Epoch: 2038 \tTraining Loss: 0.078996 \tValidation Loss: 1.014994\n","Epoch: 2039 \tTraining Loss: 0.059920 \tValidation Loss: 1.055097\n","Epoch: 2040 \tTraining Loss: 0.068318 \tValidation Loss: 1.039436\n","Epoch: 2041 \tTraining Loss: 0.060369 \tValidation Loss: 1.041222\n","Epoch: 2042 \tTraining Loss: 0.065485 \tValidation Loss: 1.035255\n","Epoch: 2043 \tTraining Loss: 0.068625 \tValidation Loss: 1.023555\n","Epoch: 2044 \tTraining Loss: 0.060069 \tValidation Loss: 1.024513\n","Epoch: 2045 \tTraining Loss: 0.053826 \tValidation Loss: 1.010935\n","Epoch: 2046 \tTraining Loss: 0.067187 \tValidation Loss: 1.007384\n","Epoch: 2047 \tTraining Loss: 0.079617 \tValidation Loss: 0.998991\n","Epoch: 2048 \tTraining Loss: 0.066914 \tValidation Loss: 1.015097\n","Epoch: 2049 \tTraining Loss: 0.097513 \tValidation Loss: 1.000259\n","Epoch: 2050 \tTraining Loss: 0.064070 \tValidation Loss: 1.025482\n","Epoch: 2051 \tTraining Loss: 0.053057 \tValidation Loss: 1.012972\n","Epoch: 2052 \tTraining Loss: 0.079293 \tValidation Loss: 1.009209\n","Epoch: 2053 \tTraining Loss: 0.057241 \tValidation Loss: 1.034976\n","Epoch: 2054 \tTraining Loss: 0.053487 \tValidation Loss: 1.015075\n","Epoch: 2055 \tTraining Loss: 0.053890 \tValidation Loss: 0.999905\n","Epoch: 2056 \tTraining Loss: 0.057682 \tValidation Loss: 1.036022\n","Epoch: 2057 \tTraining Loss: 0.066187 \tValidation Loss: 1.027790\n","Epoch: 2058 \tTraining Loss: 0.054601 \tValidation Loss: 1.040184\n","Epoch: 2059 \tTraining Loss: 0.071027 \tValidation Loss: 1.018875\n","Epoch: 2060 \tTraining Loss: 0.058579 \tValidation Loss: 1.020406\n","Epoch: 2061 \tTraining Loss: 0.063441 \tValidation Loss: 1.036361\n","Epoch: 2062 \tTraining Loss: 0.068626 \tValidation Loss: 1.017625\n","Epoch: 2063 \tTraining Loss: 0.067409 \tValidation Loss: 1.029488\n","Epoch: 2064 \tTraining Loss: 0.069773 \tValidation Loss: 1.019904\n","Epoch: 2065 \tTraining Loss: 0.067373 \tValidation Loss: 1.041775\n","Epoch: 2066 \tTraining Loss: 0.062932 \tValidation Loss: 1.015283\n","Epoch: 2067 \tTraining Loss: 0.073351 \tValidation Loss: 1.025055\n","Epoch: 2068 \tTraining Loss: 0.067159 \tValidation Loss: 1.021711\n","Epoch: 2069 \tTraining Loss: 0.080268 \tValidation Loss: 1.020377\n","Epoch: 2070 \tTraining Loss: 0.067143 \tValidation Loss: 1.025290\n","Epoch: 2071 \tTraining Loss: 0.046964 \tValidation Loss: 1.017705\n","Epoch: 2072 \tTraining Loss: 0.050249 \tValidation Loss: 1.003217\n","Epoch: 2073 \tTraining Loss: 0.052211 \tValidation Loss: 1.035011\n","Epoch: 2074 \tTraining Loss: 0.055894 \tValidation Loss: 1.055012\n","Epoch: 2075 \tTraining Loss: 0.072240 \tValidation Loss: 1.033923\n","Epoch: 2076 \tTraining Loss: 0.071472 \tValidation Loss: 1.018706\n","Epoch: 2077 \tTraining Loss: 0.085624 \tValidation Loss: 1.019426\n","Epoch: 2078 \tTraining Loss: 0.064348 \tValidation Loss: 1.024206\n","Epoch: 2079 \tTraining Loss: 0.063898 \tValidation Loss: 1.024222\n","Epoch: 2080 \tTraining Loss: 0.060965 \tValidation Loss: 1.024237\n","Epoch: 2081 \tTraining Loss: 0.052713 \tValidation Loss: 1.052535\n","Epoch: 2082 \tTraining Loss: 0.065069 \tValidation Loss: 1.029265\n","Epoch: 2083 \tTraining Loss: 0.058472 \tValidation Loss: 1.016969\n","Epoch: 2084 \tTraining Loss: 0.060888 \tValidation Loss: 1.028686\n","Epoch: 2085 \tTraining Loss: 0.071072 \tValidation Loss: 1.025321\n","Epoch: 2086 \tTraining Loss: 0.057830 \tValidation Loss: 1.027008\n","Epoch: 2087 \tTraining Loss: 0.052977 \tValidation Loss: 1.010004\n","Epoch: 2088 \tTraining Loss: 0.060892 \tValidation Loss: 1.035322\n","Epoch: 2089 \tTraining Loss: 0.064115 \tValidation Loss: 1.040674\n","Epoch: 2090 \tTraining Loss: 0.058972 \tValidation Loss: 1.031779\n","Epoch: 2091 \tTraining Loss: 0.068109 \tValidation Loss: 1.003834\n","Epoch: 2092 \tTraining Loss: 0.046480 \tValidation Loss: 1.033414\n","Epoch: 2093 \tTraining Loss: 0.042726 \tValidation Loss: 1.035707\n","Epoch: 2094 \tTraining Loss: 0.060380 \tValidation Loss: 1.027059\n","Epoch: 2095 \tTraining Loss: 0.084447 \tValidation Loss: 1.051510\n","Epoch: 2096 \tTraining Loss: 0.062036 \tValidation Loss: 1.030432\n","Epoch: 2097 \tTraining Loss: 0.061987 \tValidation Loss: 1.020496\n","Epoch: 2098 \tTraining Loss: 0.070107 \tValidation Loss: 1.023118\n","Epoch: 2099 \tTraining Loss: 0.051540 \tValidation Loss: 1.008168\n","Epoch: 2100 \tTraining Loss: 0.065604 \tValidation Loss: 1.033984\n","Epoch: 2101 \tTraining Loss: 0.067632 \tValidation Loss: 1.025487\n","Epoch: 2102 \tTraining Loss: 0.062708 \tValidation Loss: 1.039328\n","Epoch: 2103 \tTraining Loss: 0.061181 \tValidation Loss: 1.029002\n","Epoch: 2104 \tTraining Loss: 0.058778 \tValidation Loss: 1.021474\n","Epoch: 2105 \tTraining Loss: 0.050763 \tValidation Loss: 1.041731\n","Epoch: 2106 \tTraining Loss: 0.101160 \tValidation Loss: 1.031517\n","Epoch: 2107 \tTraining Loss: 0.069983 \tValidation Loss: 1.016698\n","Epoch: 2108 \tTraining Loss: 0.055320 \tValidation Loss: 1.032793\n","Epoch: 2109 \tTraining Loss: 0.058491 \tValidation Loss: 1.025169\n","Epoch: 2110 \tTraining Loss: 0.066924 \tValidation Loss: 1.021214\n","Epoch: 2111 \tTraining Loss: 0.066895 \tValidation Loss: 1.029315\n","Epoch: 2112 \tTraining Loss: 0.064024 \tValidation Loss: 1.019726\n","Epoch: 2113 \tTraining Loss: 0.054487 \tValidation Loss: 1.032583\n","Epoch: 2114 \tTraining Loss: 0.075179 \tValidation Loss: 1.019363\n","Epoch: 2115 \tTraining Loss: 0.057783 \tValidation Loss: 1.017056\n","Epoch: 2116 \tTraining Loss: 0.075063 \tValidation Loss: 1.021083\n","Epoch: 2117 \tTraining Loss: 0.055093 \tValidation Loss: 1.046546\n","Epoch: 2118 \tTraining Loss: 0.051912 \tValidation Loss: 1.041077\n","Epoch: 2119 \tTraining Loss: 0.053629 \tValidation Loss: 1.039843\n","Epoch: 2120 \tTraining Loss: 0.074264 \tValidation Loss: 1.038541\n","Epoch: 2121 \tTraining Loss: 0.071875 \tValidation Loss: 1.036815\n","Epoch: 2122 \tTraining Loss: 0.055695 \tValidation Loss: 1.007299\n","Epoch: 2123 \tTraining Loss: 0.073871 \tValidation Loss: 1.040681\n","Epoch: 2124 \tTraining Loss: 0.070506 \tValidation Loss: 1.035831\n","Epoch: 2125 \tTraining Loss: 0.052623 \tValidation Loss: 1.031614\n","Epoch: 2126 \tTraining Loss: 0.056001 \tValidation Loss: 1.018650\n","Epoch: 2127 \tTraining Loss: 0.056035 \tValidation Loss: 1.024596\n","Epoch: 2128 \tTraining Loss: 0.059157 \tValidation Loss: 1.043733\n","Epoch: 2129 \tTraining Loss: 0.072846 \tValidation Loss: 1.007801\n","Epoch: 2130 \tTraining Loss: 0.048101 \tValidation Loss: 1.019553\n","Epoch: 2131 \tTraining Loss: 0.055538 \tValidation Loss: 1.030065\n","Epoch: 2132 \tTraining Loss: 0.047642 \tValidation Loss: 1.022765\n","Epoch: 2133 \tTraining Loss: 0.063816 \tValidation Loss: 1.027192\n","Epoch: 2134 \tTraining Loss: 0.072382 \tValidation Loss: 1.012252\n","Epoch: 2135 \tTraining Loss: 0.070463 \tValidation Loss: 1.039154\n","Epoch: 2136 \tTraining Loss: 0.056468 \tValidation Loss: 1.032800\n","Epoch: 2137 \tTraining Loss: 0.052322 \tValidation Loss: 1.038027\n","Epoch: 2138 \tTraining Loss: 0.057549 \tValidation Loss: 1.018853\n","Epoch: 2139 \tTraining Loss: 0.053689 \tValidation Loss: 1.031651\n","Epoch: 2140 \tTraining Loss: 0.055016 \tValidation Loss: 1.025552\n","Epoch: 2141 \tTraining Loss: 0.056855 \tValidation Loss: 1.025812\n","Epoch: 2142 \tTraining Loss: 0.070458 \tValidation Loss: 1.047398\n","Epoch: 2143 \tTraining Loss: 0.066987 \tValidation Loss: 1.034904\n","Epoch: 2144 \tTraining Loss: 0.062858 \tValidation Loss: 1.034903\n","Epoch: 2145 \tTraining Loss: 0.059161 \tValidation Loss: 1.014694\n","Epoch: 2146 \tTraining Loss: 0.064136 \tValidation Loss: 1.015889\n","Epoch: 2147 \tTraining Loss: 0.047773 \tValidation Loss: 1.021119\n","Epoch: 2148 \tTraining Loss: 0.066304 \tValidation Loss: 1.031785\n","Epoch: 2149 \tTraining Loss: 0.061664 \tValidation Loss: 1.058612\n","Epoch: 2150 \tTraining Loss: 0.071634 \tValidation Loss: 1.018784\n","Epoch: 2151 \tTraining Loss: 0.076200 \tValidation Loss: 1.025669\n","Epoch: 2152 \tTraining Loss: 0.051872 \tValidation Loss: 1.032832\n","Epoch: 2153 \tTraining Loss: 0.060533 \tValidation Loss: 1.041994\n","Epoch: 2154 \tTraining Loss: 0.074222 \tValidation Loss: 1.037339\n","Epoch: 2155 \tTraining Loss: 0.064881 \tValidation Loss: 1.024255\n","Epoch: 2156 \tTraining Loss: 0.050857 \tValidation Loss: 1.054335\n","Epoch: 2157 \tTraining Loss: 0.054406 \tValidation Loss: 1.036225\n","Epoch: 2158 \tTraining Loss: 0.065132 \tValidation Loss: 1.036628\n","Epoch: 2159 \tTraining Loss: 0.068193 \tValidation Loss: 1.024597\n","Epoch: 2160 \tTraining Loss: 0.061905 \tValidation Loss: 1.012980\n","Epoch: 2161 \tTraining Loss: 0.066318 \tValidation Loss: 1.020380\n","Epoch: 2162 \tTraining Loss: 0.042359 \tValidation Loss: 1.028844\n","Epoch: 2163 \tTraining Loss: 0.055135 \tValidation Loss: 1.011843\n","Epoch: 2164 \tTraining Loss: 0.090231 \tValidation Loss: 1.030061\n","Epoch: 2165 \tTraining Loss: 0.054800 \tValidation Loss: 1.019807\n","Epoch: 2166 \tTraining Loss: 0.065006 \tValidation Loss: 1.042601\n","Epoch: 2167 \tTraining Loss: 0.047451 \tValidation Loss: 1.027212\n","Epoch: 2168 \tTraining Loss: 0.061518 \tValidation Loss: 1.023671\n","Epoch: 2169 \tTraining Loss: 0.071174 \tValidation Loss: 1.003478\n","Epoch: 2170 \tTraining Loss: 0.069455 \tValidation Loss: 1.050972\n","Epoch: 2171 \tTraining Loss: 0.054016 \tValidation Loss: 1.024484\n","Epoch: 2172 \tTraining Loss: 0.062915 \tValidation Loss: 1.028623\n","Epoch: 2173 \tTraining Loss: 0.086398 \tValidation Loss: 1.015092\n","Epoch: 2174 \tTraining Loss: 0.052660 \tValidation Loss: 1.038996\n","Epoch: 2175 \tTraining Loss: 0.056745 \tValidation Loss: 1.022592\n","Epoch: 2176 \tTraining Loss: 0.057616 \tValidation Loss: 1.043488\n","Epoch: 2177 \tTraining Loss: 0.053427 \tValidation Loss: 1.039139\n","Epoch: 2178 \tTraining Loss: 0.064354 \tValidation Loss: 1.019263\n","Epoch: 2179 \tTraining Loss: 0.048969 \tValidation Loss: 1.012699\n","Epoch: 2180 \tTraining Loss: 0.055003 \tValidation Loss: 1.028396\n","Epoch: 2181 \tTraining Loss: 0.052559 \tValidation Loss: 1.013549\n","Epoch: 2182 \tTraining Loss: 0.054591 \tValidation Loss: 1.044858\n","Epoch: 2183 \tTraining Loss: 0.057172 \tValidation Loss: 1.036126\n","Epoch: 2184 \tTraining Loss: 0.059198 \tValidation Loss: 1.048387\n","Epoch: 2185 \tTraining Loss: 0.058535 \tValidation Loss: 1.026752\n","Epoch: 2186 \tTraining Loss: 0.048939 \tValidation Loss: 1.031341\n","Epoch: 2187 \tTraining Loss: 0.062262 \tValidation Loss: 1.029230\n","Epoch: 2188 \tTraining Loss: 0.057968 \tValidation Loss: 1.034837\n","Epoch: 2189 \tTraining Loss: 0.060898 \tValidation Loss: 1.044279\n","Epoch: 2190 \tTraining Loss: 0.112416 \tValidation Loss: 1.012526\n","Epoch: 2191 \tTraining Loss: 0.060875 \tValidation Loss: 1.030276\n","Epoch: 2192 \tTraining Loss: 0.067964 \tValidation Loss: 1.025596\n","Epoch: 2193 \tTraining Loss: 0.066015 \tValidation Loss: 1.017269\n","Epoch: 2194 \tTraining Loss: 0.056710 \tValidation Loss: 1.017986\n","Epoch: 2195 \tTraining Loss: 0.060029 \tValidation Loss: 1.043806\n","Epoch: 2196 \tTraining Loss: 0.053907 \tValidation Loss: 1.009020\n","Epoch: 2197 \tTraining Loss: 0.064602 \tValidation Loss: 1.016810\n","Epoch: 2198 \tTraining Loss: 0.063029 \tValidation Loss: 1.038138\n","Epoch: 2199 \tTraining Loss: 0.058929 \tValidation Loss: 1.025396\n","Epoch: 2200 \tTraining Loss: 0.060784 \tValidation Loss: 1.019593\n","Epoch: 2201 \tTraining Loss: 0.060482 \tValidation Loss: 1.039742\n","Epoch: 2202 \tTraining Loss: 0.065670 \tValidation Loss: 1.029598\n","Epoch: 2203 \tTraining Loss: 0.062607 \tValidation Loss: 1.014598\n","Epoch: 2204 \tTraining Loss: 0.062295 \tValidation Loss: 1.015675\n","Epoch: 2205 \tTraining Loss: 0.051668 \tValidation Loss: 1.033719\n","Epoch: 2206 \tTraining Loss: 0.045796 \tValidation Loss: 1.010299\n","Epoch: 2207 \tTraining Loss: 0.057678 \tValidation Loss: 1.026876\n","Epoch: 2208 \tTraining Loss: 0.054994 \tValidation Loss: 1.026069\n","Epoch: 2209 \tTraining Loss: 0.066495 \tValidation Loss: 1.028279\n","Epoch: 2210 \tTraining Loss: 0.056632 \tValidation Loss: 1.021680\n","Epoch: 2211 \tTraining Loss: 0.059704 \tValidation Loss: 1.015870\n","Epoch: 2212 \tTraining Loss: 0.054694 \tValidation Loss: 1.015179\n","Epoch: 2213 \tTraining Loss: 0.055421 \tValidation Loss: 1.022498\n","Epoch: 2214 \tTraining Loss: 0.067828 \tValidation Loss: 1.044957\n","Epoch: 2215 \tTraining Loss: 0.058814 \tValidation Loss: 1.031155\n","Epoch: 2216 \tTraining Loss: 0.049693 \tValidation Loss: 1.036882\n","Epoch: 2217 \tTraining Loss: 0.050317 \tValidation Loss: 1.018459\n","Epoch: 2218 \tTraining Loss: 0.059640 \tValidation Loss: 1.021729\n","Epoch: 2219 \tTraining Loss: 0.054836 \tValidation Loss: 1.031196\n","Epoch: 2220 \tTraining Loss: 0.050556 \tValidation Loss: 1.057585\n","Epoch: 2221 \tTraining Loss: 0.065091 \tValidation Loss: 1.037246\n","Epoch: 2222 \tTraining Loss: 0.065966 \tValidation Loss: 1.018271\n","Epoch: 2223 \tTraining Loss: 0.051766 \tValidation Loss: 1.040237\n","Epoch: 2224 \tTraining Loss: 0.053730 \tValidation Loss: 1.039824\n","Epoch: 2225 \tTraining Loss: 0.046083 \tValidation Loss: 1.014048\n","Epoch: 2226 \tTraining Loss: 0.069278 \tValidation Loss: 1.027359\n","Epoch: 2227 \tTraining Loss: 0.059801 \tValidation Loss: 1.028577\n","Epoch: 2228 \tTraining Loss: 0.051468 \tValidation Loss: 1.033598\n","Epoch: 2229 \tTraining Loss: 0.062313 \tValidation Loss: 1.016727\n","Epoch: 2230 \tTraining Loss: 0.056721 \tValidation Loss: 1.029340\n","Epoch: 2231 \tTraining Loss: 0.055702 \tValidation Loss: 1.021276\n","Epoch: 2232 \tTraining Loss: 0.059864 \tValidation Loss: 1.039176\n","Epoch: 2233 \tTraining Loss: 0.056875 \tValidation Loss: 1.036579\n","Epoch: 2234 \tTraining Loss: 0.046108 \tValidation Loss: 1.042717\n","Epoch: 2235 \tTraining Loss: 0.053390 \tValidation Loss: 1.021627\n","Epoch: 2236 \tTraining Loss: 0.059440 \tValidation Loss: 1.029096\n","Epoch: 2237 \tTraining Loss: 0.050271 \tValidation Loss: 1.014299\n","Epoch: 2238 \tTraining Loss: 0.066541 \tValidation Loss: 1.011808\n","Epoch: 2239 \tTraining Loss: 0.057212 \tValidation Loss: 1.040228\n","Epoch: 2240 \tTraining Loss: 0.049803 \tValidation Loss: 1.046779\n","Epoch: 2241 \tTraining Loss: 0.052393 \tValidation Loss: 1.037703\n","Epoch: 2242 \tTraining Loss: 0.062662 \tValidation Loss: 1.023451\n","Epoch: 2243 \tTraining Loss: 0.042492 \tValidation Loss: 1.027735\n","Epoch: 2244 \tTraining Loss: 0.051402 \tValidation Loss: 1.024364\n","Epoch: 2245 \tTraining Loss: 0.063693 \tValidation Loss: 1.043160\n","Epoch: 2246 \tTraining Loss: 0.060460 \tValidation Loss: 1.011065\n","Epoch: 2247 \tTraining Loss: 0.061139 \tValidation Loss: 1.019615\n","Epoch: 2248 \tTraining Loss: 0.054076 \tValidation Loss: 1.030570\n","Epoch: 2249 \tTraining Loss: 0.059877 \tValidation Loss: 1.032624\n","Epoch: 2250 \tTraining Loss: 0.054657 \tValidation Loss: 1.029568\n","Epoch: 2251 \tTraining Loss: 0.063717 \tValidation Loss: 1.030610\n","Epoch: 2252 \tTraining Loss: 0.058109 \tValidation Loss: 1.023810\n","Epoch: 2253 \tTraining Loss: 0.055010 \tValidation Loss: 1.040310\n","Epoch: 2254 \tTraining Loss: 0.066122 \tValidation Loss: 1.041653\n","Epoch: 2255 \tTraining Loss: 0.070829 \tValidation Loss: 1.034917\n","Epoch: 2256 \tTraining Loss: 0.057726 \tValidation Loss: 1.028151\n","Epoch: 2257 \tTraining Loss: 0.050999 \tValidation Loss: 1.022382\n","Epoch: 2258 \tTraining Loss: 0.058679 \tValidation Loss: 1.013255\n","Epoch: 2259 \tTraining Loss: 0.044761 \tValidation Loss: 1.041519\n","Epoch: 2260 \tTraining Loss: 0.052336 \tValidation Loss: 1.023091\n","Epoch: 2261 \tTraining Loss: 0.062795 \tValidation Loss: 1.024762\n","Epoch: 2262 \tTraining Loss: 0.064277 \tValidation Loss: 1.003547\n","Epoch: 2263 \tTraining Loss: 0.051122 \tValidation Loss: 1.025563\n","Epoch: 2264 \tTraining Loss: 0.055641 \tValidation Loss: 1.036507\n","Epoch: 2265 \tTraining Loss: 0.050511 \tValidation Loss: 1.030201\n","Epoch: 2266 \tTraining Loss: 0.046211 \tValidation Loss: 1.028023\n","Epoch: 2267 \tTraining Loss: 0.051041 \tValidation Loss: 1.043044\n","Epoch: 2268 \tTraining Loss: 0.053395 \tValidation Loss: 1.023319\n","Epoch: 2269 \tTraining Loss: 0.055120 \tValidation Loss: 1.026851\n","Epoch: 2270 \tTraining Loss: 0.052836 \tValidation Loss: 1.047732\n","Epoch: 2271 \tTraining Loss: 0.048343 \tValidation Loss: 1.035701\n","Epoch: 2272 \tTraining Loss: 0.055281 \tValidation Loss: 1.021539\n","Epoch: 2273 \tTraining Loss: 0.058046 \tValidation Loss: 1.021815\n","Epoch: 2274 \tTraining Loss: 0.050916 \tValidation Loss: 1.050695\n","Epoch: 2275 \tTraining Loss: 0.057712 \tValidation Loss: 1.012353\n","Epoch: 2276 \tTraining Loss: 0.055047 \tValidation Loss: 1.036022\n","Epoch: 2277 \tTraining Loss: 0.055082 \tValidation Loss: 1.003564\n","Epoch: 2278 \tTraining Loss: 0.047568 \tValidation Loss: 1.031682\n","Epoch: 2279 \tTraining Loss: 0.039052 \tValidation Loss: 1.037180\n","Epoch: 2280 \tTraining Loss: 0.057500 \tValidation Loss: 1.050177\n","Epoch: 2281 \tTraining Loss: 0.055521 \tValidation Loss: 1.057291\n","Epoch: 2282 \tTraining Loss: 0.068357 \tValidation Loss: 1.063722\n","Epoch: 2283 \tTraining Loss: 0.062147 \tValidation Loss: 1.039311\n","Epoch: 2284 \tTraining Loss: 0.061058 \tValidation Loss: 1.015997\n","Epoch: 2285 \tTraining Loss: 0.051909 \tValidation Loss: 1.025606\n","Epoch: 2286 \tTraining Loss: 0.048377 \tValidation Loss: 1.041050\n","Epoch: 2287 \tTraining Loss: 0.052607 \tValidation Loss: 1.028859\n","Epoch: 2288 \tTraining Loss: 0.041531 \tValidation Loss: 1.034702\n","Epoch: 2289 \tTraining Loss: 0.047224 \tValidation Loss: 1.025038\n","Epoch: 2290 \tTraining Loss: 0.061129 \tValidation Loss: 1.024274\n","Epoch: 2291 \tTraining Loss: 0.051350 \tValidation Loss: 1.039817\n","Epoch: 2292 \tTraining Loss: 0.064680 \tValidation Loss: 1.024944\n","Epoch: 2293 \tTraining Loss: 0.052933 \tValidation Loss: 1.043723\n","Epoch: 2294 \tTraining Loss: 0.058952 \tValidation Loss: 1.031093\n","Epoch: 2295 \tTraining Loss: 0.069561 \tValidation Loss: 1.024110\n","Epoch: 2296 \tTraining Loss: 0.066135 \tValidation Loss: 1.020635\n","Epoch: 2297 \tTraining Loss: 0.046538 \tValidation Loss: 1.048016\n","Epoch: 2298 \tTraining Loss: 0.057840 \tValidation Loss: 1.004087\n","Epoch: 2299 \tTraining Loss: 0.054000 \tValidation Loss: 1.005677\n","Epoch: 2300 \tTraining Loss: 0.044770 \tValidation Loss: 1.014675\n","Epoch: 2301 \tTraining Loss: 0.055768 \tValidation Loss: 1.032916\n","Epoch: 2302 \tTraining Loss: 0.043105 \tValidation Loss: 1.029641\n","Epoch: 2303 \tTraining Loss: 0.041395 \tValidation Loss: 1.037212\n","Epoch: 2304 \tTraining Loss: 0.055846 \tValidation Loss: 1.028693\n","Epoch: 2305 \tTraining Loss: 0.052768 \tValidation Loss: 1.033314\n","Epoch: 2306 \tTraining Loss: 0.055721 \tValidation Loss: 1.013075\n","Epoch: 2307 \tTraining Loss: 0.054426 \tValidation Loss: 1.033311\n","Epoch: 2308 \tTraining Loss: 0.060775 \tValidation Loss: 1.048407\n","Epoch: 2309 \tTraining Loss: 0.057440 \tValidation Loss: 1.044477\n","Epoch: 2310 \tTraining Loss: 0.048101 \tValidation Loss: 1.036183\n","Epoch: 2311 \tTraining Loss: 0.070817 \tValidation Loss: 1.032173\n","Epoch: 2312 \tTraining Loss: 0.055649 \tValidation Loss: 1.029202\n","Epoch: 2313 \tTraining Loss: 0.059449 \tValidation Loss: 1.045582\n","Epoch: 2314 \tTraining Loss: 0.049377 \tValidation Loss: 1.019695\n","Epoch: 2315 \tTraining Loss: 0.053853 \tValidation Loss: 1.019579\n","Epoch: 2316 \tTraining Loss: 0.039232 \tValidation Loss: 1.053476\n","Epoch: 2317 \tTraining Loss: 0.050180 \tValidation Loss: 1.021555\n","Epoch: 2318 \tTraining Loss: 0.040055 \tValidation Loss: 1.030550\n","Epoch: 2319 \tTraining Loss: 0.060556 \tValidation Loss: 1.034235\n","Epoch: 2320 \tTraining Loss: 0.048322 \tValidation Loss: 1.023549\n","Epoch: 2321 \tTraining Loss: 0.048996 \tValidation Loss: 1.015607\n","Epoch: 2322 \tTraining Loss: 0.050272 \tValidation Loss: 1.044363\n","Epoch: 2323 \tTraining Loss: 0.036712 \tValidation Loss: 1.030617\n","Epoch: 2324 \tTraining Loss: 0.052921 \tValidation Loss: 1.039953\n","Epoch: 2325 \tTraining Loss: 0.042730 \tValidation Loss: 1.030895\n","Epoch: 2326 \tTraining Loss: 0.060359 \tValidation Loss: 1.023642\n","Epoch: 2327 \tTraining Loss: 0.053739 \tValidation Loss: 1.028642\n","Epoch: 2328 \tTraining Loss: 0.053488 \tValidation Loss: 1.017501\n","Epoch: 2329 \tTraining Loss: 0.052033 \tValidation Loss: 1.029660\n","Epoch: 2330 \tTraining Loss: 0.056229 \tValidation Loss: 1.023747\n","Epoch: 2331 \tTraining Loss: 0.070624 \tValidation Loss: 1.029879\n","Epoch: 2332 \tTraining Loss: 0.045535 \tValidation Loss: 1.038965\n","Epoch: 2333 \tTraining Loss: 0.046050 \tValidation Loss: 1.040794\n","Epoch: 2334 \tTraining Loss: 0.052785 \tValidation Loss: 1.029703\n","Epoch: 2335 \tTraining Loss: 0.060567 \tValidation Loss: 1.024453\n","Epoch: 2336 \tTraining Loss: 0.052613 \tValidation Loss: 1.037010\n","Epoch: 2337 \tTraining Loss: 0.035579 \tValidation Loss: 1.037218\n","Epoch: 2338 \tTraining Loss: 0.051434 \tValidation Loss: 1.031661\n","Epoch: 2339 \tTraining Loss: 0.047259 \tValidation Loss: 1.026575\n","Epoch: 2340 \tTraining Loss: 0.056809 \tValidation Loss: 1.025966\n","Epoch: 2341 \tTraining Loss: 0.048801 \tValidation Loss: 1.035551\n","Epoch: 2342 \tTraining Loss: 0.055945 \tValidation Loss: 1.033519\n","Epoch: 2343 \tTraining Loss: 0.039861 \tValidation Loss: 1.035601\n","Epoch: 2344 \tTraining Loss: 0.063195 \tValidation Loss: 1.046110\n","Epoch: 2345 \tTraining Loss: 0.056201 \tValidation Loss: 1.031741\n","Epoch: 2346 \tTraining Loss: 0.054586 \tValidation Loss: 1.022259\n","Epoch: 2347 \tTraining Loss: 0.059829 \tValidation Loss: 1.043639\n","Epoch: 2348 \tTraining Loss: 0.047979 \tValidation Loss: 1.026248\n","Epoch: 2349 \tTraining Loss: 0.050731 \tValidation Loss: 1.042592\n","Epoch: 2350 \tTraining Loss: 0.054671 \tValidation Loss: 1.028583\n","Epoch: 2351 \tTraining Loss: 0.048198 \tValidation Loss: 1.035206\n","Epoch: 2352 \tTraining Loss: 0.039381 \tValidation Loss: 1.017522\n","Epoch: 2353 \tTraining Loss: 0.048346 \tValidation Loss: 1.029245\n","Epoch: 2354 \tTraining Loss: 0.047490 \tValidation Loss: 1.042047\n","Epoch: 2355 \tTraining Loss: 0.047715 \tValidation Loss: 1.021472\n","Epoch: 2356 \tTraining Loss: 0.051926 \tValidation Loss: 1.015920\n","Epoch: 2357 \tTraining Loss: 0.042617 \tValidation Loss: 1.058233\n","Epoch: 2358 \tTraining Loss: 0.061428 \tValidation Loss: 1.034873\n","Epoch: 2359 \tTraining Loss: 0.045107 \tValidation Loss: 1.021724\n","Epoch: 2360 \tTraining Loss: 0.065062 \tValidation Loss: 1.044142\n","Epoch: 2361 \tTraining Loss: 0.043970 \tValidation Loss: 1.025488\n","Epoch: 2362 \tTraining Loss: 0.047060 \tValidation Loss: 1.073340\n","Epoch: 2363 \tTraining Loss: 0.045554 \tValidation Loss: 1.029375\n","Epoch: 2364 \tTraining Loss: 0.058546 \tValidation Loss: 1.013963\n","Epoch: 2365 \tTraining Loss: 0.045930 \tValidation Loss: 1.016732\n","Epoch: 2366 \tTraining Loss: 0.041064 \tValidation Loss: 1.045149\n","Epoch: 2367 \tTraining Loss: 0.047266 \tValidation Loss: 1.040886\n","Epoch: 2368 \tTraining Loss: 0.052618 \tValidation Loss: 1.033173\n","Epoch: 2369 \tTraining Loss: 0.055824 \tValidation Loss: 1.035282\n","Epoch: 2370 \tTraining Loss: 0.045494 \tValidation Loss: 1.023309\n","Epoch: 2371 \tTraining Loss: 0.049645 \tValidation Loss: 1.021494\n","Epoch: 2372 \tTraining Loss: 0.043841 \tValidation Loss: 1.005872\n","Epoch: 2373 \tTraining Loss: 0.051905 \tValidation Loss: 1.047632\n","Epoch: 2374 \tTraining Loss: 0.046599 \tValidation Loss: 1.036459\n","Epoch: 2375 \tTraining Loss: 0.059278 \tValidation Loss: 1.036006\n","Epoch: 2376 \tTraining Loss: 0.057506 \tValidation Loss: 1.053199\n","Epoch: 2377 \tTraining Loss: 0.046193 \tValidation Loss: 1.037755\n","Epoch: 2378 \tTraining Loss: 0.053079 \tValidation Loss: 1.021444\n","Epoch: 2379 \tTraining Loss: 0.049263 \tValidation Loss: 1.032422\n","Epoch: 2380 \tTraining Loss: 0.053093 \tValidation Loss: 1.047711\n","Epoch: 2381 \tTraining Loss: 0.043416 \tValidation Loss: 1.053843\n","Epoch: 2382 \tTraining Loss: 0.056508 \tValidation Loss: 1.018952\n","Epoch: 2383 \tTraining Loss: 0.051155 \tValidation Loss: 1.021132\n","Epoch: 2384 \tTraining Loss: 0.055698 \tValidation Loss: 1.037237\n","Epoch: 2385 \tTraining Loss: 0.054383 \tValidation Loss: 1.034046\n","Epoch: 2386 \tTraining Loss: 0.054637 \tValidation Loss: 1.037868\n","Epoch: 2387 \tTraining Loss: 0.054816 \tValidation Loss: 1.055428\n","Epoch: 2388 \tTraining Loss: 0.064544 \tValidation Loss: 1.030824\n","Epoch: 2389 \tTraining Loss: 0.045305 \tValidation Loss: 1.034093\n","Epoch: 2390 \tTraining Loss: 0.052771 \tValidation Loss: 1.030983\n","Epoch: 2391 \tTraining Loss: 0.058560 \tValidation Loss: 1.041351\n","Epoch: 2392 \tTraining Loss: 0.058082 \tValidation Loss: 1.028860\n","Epoch: 2393 \tTraining Loss: 0.037112 \tValidation Loss: 1.022165\n","Epoch: 2394 \tTraining Loss: 0.040517 \tValidation Loss: 1.060463\n","Epoch: 2395 \tTraining Loss: 0.048067 \tValidation Loss: 1.030196\n","Epoch: 2396 \tTraining Loss: 0.043376 \tValidation Loss: 1.038688\n","Epoch: 2397 \tTraining Loss: 0.068502 \tValidation Loss: 1.052354\n","Epoch: 2398 \tTraining Loss: 0.055146 \tValidation Loss: 1.030882\n","Epoch: 2399 \tTraining Loss: 0.040110 \tValidation Loss: 1.044398\n","Epoch: 2400 \tTraining Loss: 0.053352 \tValidation Loss: 1.034890\n","Epoch: 2401 \tTraining Loss: 0.056842 \tValidation Loss: 1.037604\n","Epoch: 2402 \tTraining Loss: 0.047415 \tValidation Loss: 1.042679\n","Epoch: 2403 \tTraining Loss: 0.055203 \tValidation Loss: 1.042498\n","Epoch: 2404 \tTraining Loss: 0.051828 \tValidation Loss: 1.042433\n","Epoch: 2405 \tTraining Loss: 0.032411 \tValidation Loss: 1.038955\n","Epoch: 2406 \tTraining Loss: 0.041656 \tValidation Loss: 1.046211\n","Epoch: 2407 \tTraining Loss: 0.043760 \tValidation Loss: 1.034308\n","Epoch: 2408 \tTraining Loss: 0.043348 \tValidation Loss: 1.034958\n","Epoch: 2409 \tTraining Loss: 0.063089 \tValidation Loss: 1.046328\n","Epoch: 2410 \tTraining Loss: 0.048735 \tValidation Loss: 1.050169\n","Epoch: 2411 \tTraining Loss: 0.078836 \tValidation Loss: 1.038546\n","Epoch: 2412 \tTraining Loss: 0.044654 \tValidation Loss: 1.011377\n","Epoch: 2413 \tTraining Loss: 0.052517 \tValidation Loss: 1.035243\n","Epoch: 2414 \tTraining Loss: 0.049684 \tValidation Loss: 1.035016\n","Epoch: 2415 \tTraining Loss: 0.053902 \tValidation Loss: 1.039201\n","Epoch: 2416 \tTraining Loss: 0.043339 \tValidation Loss: 1.044496\n","Epoch: 2417 \tTraining Loss: 0.065317 \tValidation Loss: 1.017516\n","Epoch: 2418 \tTraining Loss: 0.054143 \tValidation Loss: 1.057585\n","Epoch: 2419 \tTraining Loss: 0.051687 \tValidation Loss: 1.028180\n","Epoch: 2420 \tTraining Loss: 0.047422 \tValidation Loss: 1.013604\n","Epoch: 2421 \tTraining Loss: 0.043534 \tValidation Loss: 1.037060\n","Epoch: 2422 \tTraining Loss: 0.056316 \tValidation Loss: 1.018283\n","Epoch: 2423 \tTraining Loss: 0.056693 \tValidation Loss: 1.050006\n","Epoch: 2424 \tTraining Loss: 0.044599 \tValidation Loss: 1.045816\n","Epoch: 2425 \tTraining Loss: 0.041950 \tValidation Loss: 1.057813\n","Epoch: 2426 \tTraining Loss: 0.049602 \tValidation Loss: 1.027640\n","Epoch: 2427 \tTraining Loss: 0.041428 \tValidation Loss: 1.030123\n","Epoch: 2428 \tTraining Loss: 0.051924 \tValidation Loss: 1.024789\n","Epoch: 2429 \tTraining Loss: 0.065819 \tValidation Loss: 1.055630\n","Epoch: 2430 \tTraining Loss: 0.049755 \tValidation Loss: 1.036808\n","Epoch: 2431 \tTraining Loss: 0.061785 \tValidation Loss: 1.033243\n","Epoch: 2432 \tTraining Loss: 0.047063 \tValidation Loss: 1.032252\n","Epoch: 2433 \tTraining Loss: 0.050818 \tValidation Loss: 1.043284\n","Epoch: 2434 \tTraining Loss: 0.040806 \tValidation Loss: 1.031831\n","Epoch: 2435 \tTraining Loss: 0.055432 \tValidation Loss: 1.025102\n","Epoch: 2436 \tTraining Loss: 0.051730 \tValidation Loss: 1.026358\n","Epoch: 2437 \tTraining Loss: 0.044524 \tValidation Loss: 1.047580\n","Epoch: 2438 \tTraining Loss: 0.049282 \tValidation Loss: 1.042582\n","Epoch: 2439 \tTraining Loss: 0.042753 \tValidation Loss: 1.034090\n","Epoch: 2440 \tTraining Loss: 0.046946 \tValidation Loss: 1.019005\n","Epoch: 2441 \tTraining Loss: 0.042938 \tValidation Loss: 1.029106\n","Epoch: 2442 \tTraining Loss: 0.055217 \tValidation Loss: 1.027458\n","Epoch: 2443 \tTraining Loss: 0.047978 \tValidation Loss: 1.042872\n","Epoch: 2444 \tTraining Loss: 0.042098 \tValidation Loss: 1.023894\n","Epoch: 2445 \tTraining Loss: 0.073389 \tValidation Loss: 1.046679\n","Epoch: 2446 \tTraining Loss: 0.046212 \tValidation Loss: 1.040154\n","Epoch: 2447 \tTraining Loss: 0.046239 \tValidation Loss: 1.022025\n","Epoch: 2448 \tTraining Loss: 0.042217 \tValidation Loss: 1.054935\n","Epoch: 2449 \tTraining Loss: 0.049629 \tValidation Loss: 1.046132\n","Epoch: 2450 \tTraining Loss: 0.047817 \tValidation Loss: 1.031724\n","Epoch: 2451 \tTraining Loss: 0.038166 \tValidation Loss: 1.033766\n","Epoch: 2452 \tTraining Loss: 0.063119 \tValidation Loss: 1.033774\n","Epoch: 2453 \tTraining Loss: 0.054692 \tValidation Loss: 1.054884\n","Epoch: 2454 \tTraining Loss: 0.055961 \tValidation Loss: 1.020312\n","Epoch: 2455 \tTraining Loss: 0.047850 \tValidation Loss: 1.032346\n","Epoch: 2456 \tTraining Loss: 0.049473 \tValidation Loss: 1.045314\n","Epoch: 2457 \tTraining Loss: 0.058127 \tValidation Loss: 1.023096\n","Epoch: 2458 \tTraining Loss: 0.057878 \tValidation Loss: 1.045442\n","Epoch: 2459 \tTraining Loss: 0.044043 \tValidation Loss: 1.017645\n","Epoch: 2460 \tTraining Loss: 0.057975 \tValidation Loss: 1.033058\n","Epoch: 2461 \tTraining Loss: 0.055409 \tValidation Loss: 1.035724\n","Epoch: 2462 \tTraining Loss: 0.053323 \tValidation Loss: 1.036570\n","Epoch: 2463 \tTraining Loss: 0.045546 \tValidation Loss: 1.059828\n","Epoch: 2464 \tTraining Loss: 0.048210 \tValidation Loss: 1.064627\n","Epoch: 2465 \tTraining Loss: 0.034380 \tValidation Loss: 1.031930\n","Epoch: 2466 \tTraining Loss: 0.042905 \tValidation Loss: 1.033002\n","Epoch: 2467 \tTraining Loss: 0.058188 \tValidation Loss: 1.025883\n","Epoch: 2468 \tTraining Loss: 0.041223 \tValidation Loss: 1.027015\n","Epoch: 2469 \tTraining Loss: 0.063875 \tValidation Loss: 1.002627\n","Epoch: 2470 \tTraining Loss: 0.036733 \tValidation Loss: 1.028523\n","Epoch: 2471 \tTraining Loss: 0.046597 \tValidation Loss: 1.029111\n","Epoch: 2472 \tTraining Loss: 0.063404 \tValidation Loss: 1.055640\n","Epoch: 2473 \tTraining Loss: 0.059671 \tValidation Loss: 1.053391\n","Epoch: 2474 \tTraining Loss: 0.036834 \tValidation Loss: 1.020263\n","Epoch: 2475 \tTraining Loss: 0.063649 \tValidation Loss: 1.052875\n","Epoch: 2476 \tTraining Loss: 0.045738 \tValidation Loss: 1.003446\n","Epoch: 2477 \tTraining Loss: 0.049337 \tValidation Loss: 1.025950\n","Epoch: 2478 \tTraining Loss: 0.050122 \tValidation Loss: 1.045998\n","Epoch: 2479 \tTraining Loss: 0.044754 \tValidation Loss: 1.051759\n","Epoch: 2480 \tTraining Loss: 0.052526 \tValidation Loss: 1.034609\n","Epoch: 2481 \tTraining Loss: 0.046826 \tValidation Loss: 1.032503\n","Epoch: 2482 \tTraining Loss: 0.056678 \tValidation Loss: 1.032412\n","Epoch: 2483 \tTraining Loss: 0.047033 \tValidation Loss: 1.009972\n","Epoch: 2484 \tTraining Loss: 0.044318 \tValidation Loss: 1.033667\n","Epoch: 2485 \tTraining Loss: 0.063879 \tValidation Loss: 1.016839\n","Epoch: 2486 \tTraining Loss: 0.054615 \tValidation Loss: 1.051843\n","Epoch: 2487 \tTraining Loss: 0.035723 \tValidation Loss: 1.024575\n","Epoch: 2488 \tTraining Loss: 0.044560 \tValidation Loss: 1.039122\n","Epoch: 2489 \tTraining Loss: 0.036341 \tValidation Loss: 1.027129\n","Epoch: 2490 \tTraining Loss: 0.040003 \tValidation Loss: 1.040089\n","Epoch: 2491 \tTraining Loss: 0.051572 \tValidation Loss: 1.052273\n","Epoch: 2492 \tTraining Loss: 0.050996 \tValidation Loss: 1.036608\n","Epoch: 2493 \tTraining Loss: 0.042497 \tValidation Loss: 1.038557\n","Epoch: 2494 \tTraining Loss: 0.036485 \tValidation Loss: 1.046902\n","Epoch: 2495 \tTraining Loss: 0.060917 \tValidation Loss: 1.037025\n","Epoch: 2496 \tTraining Loss: 0.049611 \tValidation Loss: 1.040751\n","Epoch: 2497 \tTraining Loss: 0.046772 \tValidation Loss: 1.033616\n","Epoch: 2498 \tTraining Loss: 0.033426 \tValidation Loss: 1.030934\n","Epoch: 2499 \tTraining Loss: 0.036186 \tValidation Loss: 1.050038\n","Epoch: 2500 \tTraining Loss: 0.041915 \tValidation Loss: 1.025143\n","Epoch: 2501 \tTraining Loss: 0.066675 \tValidation Loss: 1.021028\n","Epoch: 2502 \tTraining Loss: 0.048090 \tValidation Loss: 1.032122\n","Epoch: 2503 \tTraining Loss: 0.050330 \tValidation Loss: 1.037865\n","Epoch: 2504 \tTraining Loss: 0.039731 \tValidation Loss: 1.045623\n","Epoch: 2505 \tTraining Loss: 0.061840 \tValidation Loss: 1.020384\n","Epoch: 2506 \tTraining Loss: 0.045771 \tValidation Loss: 1.042400\n","Epoch: 2507 \tTraining Loss: 0.046175 \tValidation Loss: 1.021278\n","Epoch: 2508 \tTraining Loss: 0.042008 \tValidation Loss: 1.012766\n","Epoch: 2509 \tTraining Loss: 0.042102 \tValidation Loss: 1.040590\n","Epoch: 2510 \tTraining Loss: 0.028878 \tValidation Loss: 1.057305\n","Epoch: 2511 \tTraining Loss: 0.058261 \tValidation Loss: 1.033639\n","Epoch: 2512 \tTraining Loss: 0.042112 \tValidation Loss: 1.019820\n","Epoch: 2513 \tTraining Loss: 0.046397 \tValidation Loss: 1.043709\n","Epoch: 2514 \tTraining Loss: 0.055361 \tValidation Loss: 1.049407\n","Epoch: 2515 \tTraining Loss: 0.049638 \tValidation Loss: 1.036852\n","Epoch: 2516 \tTraining Loss: 0.049331 \tValidation Loss: 1.024720\n","Epoch: 2517 \tTraining Loss: 0.042834 \tValidation Loss: 1.035993\n","Epoch: 2518 \tTraining Loss: 0.050549 \tValidation Loss: 1.028714\n","Epoch: 2519 \tTraining Loss: 0.047327 \tValidation Loss: 1.021482\n","Epoch: 2520 \tTraining Loss: 0.055529 \tValidation Loss: 1.026216\n","Epoch: 2521 \tTraining Loss: 0.055075 \tValidation Loss: 1.042396\n","Epoch: 2522 \tTraining Loss: 0.062781 \tValidation Loss: 1.016950\n","Epoch: 2523 \tTraining Loss: 0.040864 \tValidation Loss: 1.024994\n","Epoch: 2524 \tTraining Loss: 0.048327 \tValidation Loss: 1.021141\n","Epoch: 2525 \tTraining Loss: 0.041925 \tValidation Loss: 1.021836\n","Epoch: 2526 \tTraining Loss: 0.057015 \tValidation Loss: 1.042034\n","Epoch: 2527 \tTraining Loss: 0.047791 \tValidation Loss: 1.041258\n","Epoch: 2528 \tTraining Loss: 0.036098 \tValidation Loss: 1.025111\n","Epoch: 2529 \tTraining Loss: 0.045901 \tValidation Loss: 1.022008\n","Epoch: 2530 \tTraining Loss: 0.043841 \tValidation Loss: 1.047266\n","Epoch: 2531 \tTraining Loss: 0.049095 \tValidation Loss: 1.035825\n","Epoch: 2532 \tTraining Loss: 0.038031 \tValidation Loss: 1.045388\n","Epoch: 2533 \tTraining Loss: 0.041948 \tValidation Loss: 1.042076\n","Epoch: 2534 \tTraining Loss: 0.037564 \tValidation Loss: 1.032559\n","Epoch: 2535 \tTraining Loss: 0.039424 \tValidation Loss: 1.058132\n","Epoch: 2536 \tTraining Loss: 0.048208 \tValidation Loss: 1.041035\n","Epoch: 2537 \tTraining Loss: 0.033041 \tValidation Loss: 1.036859\n","Epoch: 2538 \tTraining Loss: 0.052244 \tValidation Loss: 1.025500\n","Epoch: 2539 \tTraining Loss: 0.040425 \tValidation Loss: 1.036385\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-46-e09774b43db4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m4000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#model_transfer.load_state_dict(torch.load(path+'bacteria_phylum_1.pt'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel_transfer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloaders_transfer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_transfer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_transfer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion_transfer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'bacteria_phylum_1.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-44-269c30be6d3e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(n_epochs, loader, model, optimizer, criterion, use_cuda, save_path)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;31m# calculate the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-13-0e31b3884c36>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-584d66d9163d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# apply a ReLu activation the outputs of the first layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# return a summed output, x + resnet_block(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mout_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_layer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_layer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;31m# TODO: if statement only here to tell the jit to skip emitting this when it is None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batches_tracked\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batches_tracked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batches_tracked\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmomentum\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# use cumulative moving average\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                     \u001b[0mexponential_average_factor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batches_tracked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"-bEltt5VQ2R_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1596096903962,"user_tz":-330,"elapsed":10497,"user":{"displayName":"Samyak Jain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gioocp3b6d0F-BZZ3D9c4FJz-F7ie6BPVSG0xZr=s64","userId":"08656034860933804166"}},"outputId":"3bb0b76e-fdfa-4c60-f0a5-755ee81594f0"},"source":["model.load_state_dict(torch.load(path+'bacteria_phylum_0.pt'))"],"execution_count":47,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":47}]},{"cell_type":"code","metadata":{"id":"9F4DDDa3Nmvq","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596095063320,"user_tz":-330,"elapsed":3199,"user":{"displayName":"Samyak Jain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gioocp3b6d0F-BZZ3D9c4FJz-F7ie6BPVSG0xZr=s64","userId":"08656034860933804166"}}},"source":["def test(loader, model, criterion, use_cuda):\n","\n","    # monitor test loss and accuracy\n","    test_loss = 0.\n","    correct = 0.\n","    total = 0.\n","\n","    model.eval()\n","    for batch_idx, (data, target) in enumerate(loader['test']):\n","        # move to GPU\n","        if use_cuda:\n","            data, target = data.cuda(), target.cuda()\n","        # forward pass: compute predicted outputs by passing inputs to the model\n","        data = data.unsqueeze(1)\n","        output = model(data)\n","        target = target.squeeze(1)\n","        \n","        # calculate the loss\n","        loss = criterion(output, target)\n","\n","        # update average test loss \n","        test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n","        # convert output probabilities to predicted class\n","        pred = output.data.max(1, keepdim=True)[1]\n","        \n","        # compare predictions to true label\n","        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n","        total += data.size(0)\n","            \n","    print('Test Loss: {:.6f}\\n'.format(test_loss))\n","\n","    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n","        100. * correct / total, correct, total))"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"16CtftLiN3ny","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"executionInfo":{"status":"ok","timestamp":1596096930949,"user_tz":-330,"elapsed":1249,"user":{"displayName":"Samyak Jain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gioocp3b6d0F-BZZ3D9c4FJz-F7ie6BPVSG0xZr=s64","userId":"08656034860933804166"}},"outputId":"12302dab-362b-43c7-c0e6-a7e608e75be6"},"source":["# call test function    \n","test(loaders_transfer, model_transfer, criterion_transfer, use_cuda)"],"execution_count":48,"outputs":[{"output_type":"stream","text":["Test Loss: 0.866355\n","\n","\n","Test Accuracy: 79% (19/24)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7mpcGUKMiJPO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":268},"executionInfo":{"status":"ok","timestamp":1596096409630,"user_tz":-330,"elapsed":8131,"user":{"displayName":"Samyak Jain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gioocp3b6d0F-BZZ3D9c4FJz-F7ie6BPVSG0xZr=s64","userId":"08656034860933804166"}},"outputId":"45c57ea6-0d8f-48e1-cda2-63b71f050dd4"},"source":["%matplotlib inline\n","%config InlineBackend.figure_format = 'retina'\n","import matplotlib.pyplot as plt\n","\n","plt.plot(train_losses, label='Training loss')\n","plt.plot(valid_losses, label='Validation loss')\n","plt.legend(frameon=False)\n","plt.show()"],"execution_count":40,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"image/png":{"width":380,"height":251},"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"THo1e2T-qbPc","colab_type":"code","colab":{}},"source":["model_transfer = GeNet()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y9JZrP6RfCZF","colab_type":"code","colab":{}},"source":["###  a function that takes a path to a read as input\n","### and returns the taxanomy as kingdom that is predicted by the model.\n","\n","model_transfer.load_state_dict(torch.load(path+'bacteria_phylum_1.pt'))\n","\n","# list of class names by index, i.e. a name can be accessed like class_names[0]\n","class_names = classes\n","\n","def predict_taxa_transfer(seq):\n","    \n","    # load the read and return the predicted taxa\n","    input=0\n","    \n","    read=np.zeros(2500)\n","    for x,y in enumerate(seq):\n","        read[x]=char_encode[y]\n","    read=read.reshape(50,50)\n","    input=read/20\n","\n","    tensor_r = torch.Tensor(input).unsqueeze(0).unsqueeze(0)\n","\n","    if use_cuda:\n","        tensor_r = tensor_r.cuda()\n","    \n","    model = model_transfer.cuda()\n","    model.eval()\n","    output = model(tensor_r)\n","    \n","    # convert output probabilities to predicted class\n","    _, pred_tensor = torch.max(output, 1)\n","    index = np.squeeze(pred_tensor.numpy()) if not use_cuda else np.squeeze(pred_tensor.cpu().numpy())\n","    return class_names[index]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TkFsu4Z0gS6R","colab_type":"code","colab":{}},"source":["### TODO: Write your algorithm.\n","### Feel free to use as many code cells as needed.\n","\n","def run_app(seq,taxa):\n","    \n","      prediction = predict_taxa_transfer(seq)\n","      print(f\"Input read: {seq}\")\n","      print(f\"Correct class: {taxa}\")\n","      print(f\"Predicted class: {prediction}\")  \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6w5YNVxbgaA0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":190},"executionInfo":{"status":"ok","timestamp":1595996100190,"user_tz":-330,"elapsed":9759,"user":{"displayName":"Samyak Jain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gioocp3b6d0F-BZZ3D9c4FJz-F7ie6BPVSG0xZr=s64","userId":"08656034860933804166"}},"outputId":"1e9a1da8-b19f-4d09-b3cc-655c32534a12"},"source":["inference_read = [\n","                  'GATGAACGCTGGCGGCGTGCCTAACACATGCAAGTCGAGCGGAGTTTAACTGGAAGCACTTGTGCGACCGGATAAACTTAGCGGCGGACGGGTGAGTAACACGTGAGCAACCTACCTATCGCAGGGGAACAACATTGGGAAACCAGTGCTAATACCGCATAACATCTTTTGGGGGCATCCCCGGAAGATCAAAGGATTTCGATCCGGCGACAGATGGGCTCGCGTCCGATTAGCTAGTTGGTAAGGTAAAAGCTTACCAAGGCAACGATCGGTAGCCGAACTGAGAGGTTGATCGGCCACATTGGGACTGAGACACGGCCCAGGCTCCTACGGGAGGCAGCAGTGGGGAATATTGGGCAATGGGGGAAACCCTGACCCAGCAACGCCGCGTGAAGGAAGAAGGCCTTCGGGTTGTAAACTTCTTTGATCAGGGACGAAACAAATGACGGTACCTGAAGAACAAGTCACGGCTAACTACGTGCCAGCAGCCGCGGTAATACGTAGGTGACAAGCGTTATCCGGATTTACTGGGTGTAAAGGGCGTGTAGGCGGTTTCGTAAGTTGGATGTGAAATTCTCAGGCTTAACCTGAGAGGGTCATCCAAAACTGCAAAACTTGAGTACTGGAGAGGATAGTGGAATTCCTAGTGTAGCGGTAAAATGCGTAGATATTAGGAGGAACACCAGTGGCGAAGGCGACTATCTGGACAGTAACTGACGCTGAGGCGCGAAAGCGTGGGGAGCAAACAGGATTAGATACCCTGGTAGTCCACGCCGTAAACGATGAATACTAGGTGTAGGGGGTATCGACCCCCCCTGTGCCGCAGCTAACGCAATAAGTATTCCACCTGGGGAGTACGACCGCAAGGTTGAAACTCAAAGGAATTGACGGGGGCCCGCACAAGCAGTGGAGTATGTGGTTTAATTCGAAGCAACGCGAAGAACCTTACCAGGGCTTGACATCCTCTGACGGCTGTAGAGATACAGCTTTCCCTTCGGGGACAGAGAGACAGGTGGTGCATGGTTGTCGTCAGCTCGTGTCGTGAGATGTTGGGTTAAGTCCCGCAACGAGCGCAACCCCTATGGTCAGTTGCCAGCACGTAATGGTGGGCACTCTGGCAAGACTGCCGTTGATAAAACGGAGGAAGGTGGGGACGACGTCAAATCATCATGCCCCTTATGTCCTGGGCTACACACGTACTACAATGGCAACAACAGAGGGCAGCCAGGTCGCGAGGCCGAGCGAATCCCAAAATGTTGTCTCAGTTCAGATTGCAGGCTGCAACTCGCCTGCATGAAGTCGGAATTGCTAGTAATGGCAGGTCAGCATACTGCCGTGAATACGTTCCCGGGTCTTGTACACACCGCCCGTCACACCATGAGAGTTTGTAACACCCGAAGTCAGTAGTCTGACCGTAAGGAGGGCGCTGCCGAAGGTGGGACAGATAATTGGGGTG',\n","                  'GCCCTTAGAGTTTGATCCTGGCTCAGGATGAACGCTGGCGGCGTGCTTAACACATGCAAGTCGAACGATTAAAGCTCACTTCGGTGAGTGTATAGAGTGGCGAACGGGTGAGTAACACGTGGGCAACCTGCCCCTCACACTGGGATAACCATTGGAAACGATGGCTAATACCGGATACTCCGACGAGATCGCATGGTCTTGTCGGGAAAGCTCCGGCGGTGAGGGATGGGCCCGCGGCCCATTAGCTTGTTGGTGAGGTAACGGCTCACCAAGGCAACGATGGGTAGCCGAGCTGAGAGGCTGATCGGCCACACTGGGACTGAGACACGGCCCAGACTCCTACGGGGGGCAGCAGTGGGGAATTTTGCGCAATGGGCGAAAGCCTGACGCAGCAACGCCGCGTGCGGGATGAAGGCCCTCGGGTTGTAAACCGCTTTCAGCAGGGATGAGATTGACAGTACCTGCAGAAGAAGCCCCGGCTAACTACGTGCCAGCAGCCGCGGTAATACGTAGGGGGCAAGCGTTATCCGGATTTATTGGGCGTAAAGCGCGTGTAGGCGGCTTGTTAAGTCAGATGTGAAAACCGGGGGCTCAACCCGCGGCCTGCATCTGAAACTGGCAGGCTTGAGTCTGGTAGAGGAAAGTGGAATTCCTAGTGTAGCGGTGAAATGCGCAGATATTAGGAGGAACACCAGTGGCGAAGGCGGCTTTCTGGGGCCACGACTGACGCTGAGACGCGAAAGCTAGGGGAGCGAACAGGATTAGGTACCCTGGTAGTCCTAGCCGTAAACGATGGGCACTAGGTGTGGGGGGTCATCGACTCCCTCCGTGCCGTAGCTAACGCATTAAGTGCCCCGCCTGGGGAGTACGGCCGCAAGGCTAAAACTCAAAGGAATTGACGGGGGCCCGCACAAGCAGCGGAGCATGTGGCTTAATTCGACGCAACGCGAAGAACCTTACCAGGGCTTGACATGCAGAGAAAAGCGGCGGAAACGTCGTGTCCGAAAGGGCTCTGCACAGGTGGTGCATGGCTGTCGTCAGCTCGTGTCGTGAGATGTTGGGTTAAGTCCCGCAACGAGCGCAACCCCTGTCGTATGTTGCCAGCATTTAGTTGGGGACTCATACGAGACTGCCGGCGTCAAGCCGGAGGAAGGTGGGGATGACGTCAAGTCATCATGCCCCTTATGTCCTGGGCTGCACACGTGCTACAATGGCCGGTACAATGGGCTGCGATACCGCGAGGTGGAGCGAATCCCATAAAACCGGCCCCAGTTCGGATCGGAGGCTGCAACTCGCCTCCGTGAAGTCGGAGTTGCTAGTAATCGCGGATCAGCATGCCGCGGTGAATACGTTCCTGGGCCTTGTACATACCGCCCGTCACACCACCCGAGTTGTCTGCACCCGAAGTCGCTGTCCCAACCCTTTGGGAGGGAGGCGCCGAAGGTGTGGAGAGTAAGGGGGGTGAAGTCGTAACAAGGTAGCCGTACCGGAAGGTGCGGCTGGATCACCTCCTTAAGGGC',\n","                  'GATGAACGCTGGCGGCGTGCCTAATACATGCAAGTCGAGCGGTAACAGGAGAAGCTTGCTTCTCGCTGACGAGCGGCGGACGGCTGAGTAACGCGTAGGAACGTACCCCAAAGTGAGGGATAATTCACCGAAAGGTGAACTAATACCGCATGTGCTCTAAGGAGTAAAGCTACGGCGCTTTGGGAACGGCCTGCGTCCGATTAGCTAGTTGGTAGTGGTAATGGCCTACCAAGGTTACGATCGGTAGCTGGTCTGAGAGGATGATCAGCCAGACTGGGACTGAGACACGGCCCAGACTCCTACGGGAGGCAGCAGTAGGGAATTTTCCACAATGGGCGCAAGCCTGATGGAGCAACGCCGCGTGCAGGATGACGGCCTTCGGGTTGTAAACTGCTTTTCTTTGTGACGACAATGACAGTAGCAAAGGAATAAGGATCTGCTAACTACGTGCCAGCAGCCGCGGTCATACGTAGGATCCAAGCGTTATCCGGATTTACTGGGCGTAAAGAGTTGCGTAGGTGGTTCGTTAAGCAAGACATGAAATCGTGTGGCTCAACCATACGGCTATGTTTTGAACTGATGAACATGAGAACGAGAGAGGTGGCTGGAATTCCCAGTGTAGGAGTGAAATCCGTAGATATTGGGAGGAACACCGATGGCGTAGGCAGGCCACTGGCTCGTTTCTGACACTGAGGCACGAAAGCGTGGGGAGCAAACGGGATTAGATACCCCGGTAGTCCACGCCGTAAACTATGGATGCTAGCTGTATAGAGTATCGACCCTCTGTGTAGCGAAGCTAACGCGTTAAGCATCCCGCCTGGGTAGTACGGTCGCAAGACTAAAACCTAAAGGAATTGACGGGGACCCGCACAAGCGGTGGAGCGTGTTGTTTAATTCGATGATAAGCGAAGAACCTTACCAAGACTTGACATCCTGAGAATTTCTATGAAAGTAGAGAGTGCCTTTTGGAACTCAGTGACAGGTGTTGCATGGCCGTCGTCAGCTCGTGTCGTGAGATGTTGGGTTAAGTCCCGCAACGAGCGCAACCCTTATGTTTAGTTGTATTTTTCTAAACAGACTGCCCTGGTAACAGGGAGGAAGGAGGGGATGATGCCAGGTCAGTATTACCCTTACGTCTTGGGCTACAAACACGCTACAATGGCCGGTACAAAGGGAAGCCAACCCGCGAGGGGGAGCAAATCCCATCAAAGCCGGTCCCAGTTCGGATTGTAGGCTGAAACTCGCCTGCATGAAGTCGGAATCGCTAGTAACGGTAGGTCAGCACACTACCGTGAATACGTTCCCGGGTCTTGTACACACCGCCCGTCAAGCCATGAAAGTCACCAACACCCAAAGTATGAGCATTGCCTCGTCCTAAGGT'\n","                 ]\n","inference_taxa = ['Firmicutes','Actinobacteria','Saccharibacteria'] \n","\n","for seq,taxa in zip(inference_read,inference_taxa):\n","    run_app(seq,taxa)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Input read: GATGAACGCTGGCGGCGTGCCTAACACATGCAAGTCGAGCGGAGTTTAACTGGAAGCACTTGTGCGACCGGATAAACTTAGCGGCGGACGGGTGAGTAACACGTGAGCAACCTACCTATCGCAGGGGAACAACATTGGGAAACCAGTGCTAATACCGCATAACATCTTTTGGGGGCATCCCCGGAAGATCAAAGGATTTCGATCCGGCGACAGATGGGCTCGCGTCCGATTAGCTAGTTGGTAAGGTAAAAGCTTACCAAGGCAACGATCGGTAGCCGAACTGAGAGGTTGATCGGCCACATTGGGACTGAGACACGGCCCAGGCTCCTACGGGAGGCAGCAGTGGGGAATATTGGGCAATGGGGGAAACCCTGACCCAGCAACGCCGCGTGAAGGAAGAAGGCCTTCGGGTTGTAAACTTCTTTGATCAGGGACGAAACAAATGACGGTACCTGAAGAACAAGTCACGGCTAACTACGTGCCAGCAGCCGCGGTAATACGTAGGTGACAAGCGTTATCCGGATTTACTGGGTGTAAAGGGCGTGTAGGCGGTTTCGTAAGTTGGATGTGAAATTCTCAGGCTTAACCTGAGAGGGTCATCCAAAACTGCAAAACTTGAGTACTGGAGAGGATAGTGGAATTCCTAGTGTAGCGGTAAAATGCGTAGATATTAGGAGGAACACCAGTGGCGAAGGCGACTATCTGGACAGTAACTGACGCTGAGGCGCGAAAGCGTGGGGAGCAAACAGGATTAGATACCCTGGTAGTCCACGCCGTAAACGATGAATACTAGGTGTAGGGGGTATCGACCCCCCCTGTGCCGCAGCTAACGCAATAAGTATTCCACCTGGGGAGTACGACCGCAAGGTTGAAACTCAAAGGAATTGACGGGGGCCCGCACAAGCAGTGGAGTATGTGGTTTAATTCGAAGCAACGCGAAGAACCTTACCAGGGCTTGACATCCTCTGACGGCTGTAGAGATACAGCTTTCCCTTCGGGGACAGAGAGACAGGTGGTGCATGGTTGTCGTCAGCTCGTGTCGTGAGATGTTGGGTTAAGTCCCGCAACGAGCGCAACCCCTATGGTCAGTTGCCAGCACGTAATGGTGGGCACTCTGGCAAGACTGCCGTTGATAAAACGGAGGAAGGTGGGGACGACGTCAAATCATCATGCCCCTTATGTCCTGGGCTACACACGTACTACAATGGCAACAACAGAGGGCAGCCAGGTCGCGAGGCCGAGCGAATCCCAAAATGTTGTCTCAGTTCAGATTGCAGGCTGCAACTCGCCTGCATGAAGTCGGAATTGCTAGTAATGGCAGGTCAGCATACTGCCGTGAATACGTTCCCGGGTCTTGTACACACCGCCCGTCACACCATGAGAGTTTGTAACACCCGAAGTCAGTAGTCTGACCGTAAGGAGGGCGCTGCCGAAGGTGGGACAGATAATTGGGGTG\n","Correct class: Firmicutes\n","Predicted class: Firmicutes\n","Input read: GCCCTTAGAGTTTGATCCTGGCTCAGGATGAACGCTGGCGGCGTGCTTAACACATGCAAGTCGAACGATTAAAGCTCACTTCGGTGAGTGTATAGAGTGGCGAACGGGTGAGTAACACGTGGGCAACCTGCCCCTCACACTGGGATAACCATTGGAAACGATGGCTAATACCGGATACTCCGACGAGATCGCATGGTCTTGTCGGGAAAGCTCCGGCGGTGAGGGATGGGCCCGCGGCCCATTAGCTTGTTGGTGAGGTAACGGCTCACCAAGGCAACGATGGGTAGCCGAGCTGAGAGGCTGATCGGCCACACTGGGACTGAGACACGGCCCAGACTCCTACGGGGGGCAGCAGTGGGGAATTTTGCGCAATGGGCGAAAGCCTGACGCAGCAACGCCGCGTGCGGGATGAAGGCCCTCGGGTTGTAAACCGCTTTCAGCAGGGATGAGATTGACAGTACCTGCAGAAGAAGCCCCGGCTAACTACGTGCCAGCAGCCGCGGTAATACGTAGGGGGCAAGCGTTATCCGGATTTATTGGGCGTAAAGCGCGTGTAGGCGGCTTGTTAAGTCAGATGTGAAAACCGGGGGCTCAACCCGCGGCCTGCATCTGAAACTGGCAGGCTTGAGTCTGGTAGAGGAAAGTGGAATTCCTAGTGTAGCGGTGAAATGCGCAGATATTAGGAGGAACACCAGTGGCGAAGGCGGCTTTCTGGGGCCACGACTGACGCTGAGACGCGAAAGCTAGGGGAGCGAACAGGATTAGGTACCCTGGTAGTCCTAGCCGTAAACGATGGGCACTAGGTGTGGGGGGTCATCGACTCCCTCCGTGCCGTAGCTAACGCATTAAGTGCCCCGCCTGGGGAGTACGGCCGCAAGGCTAAAACTCAAAGGAATTGACGGGGGCCCGCACAAGCAGCGGAGCATGTGGCTTAATTCGACGCAACGCGAAGAACCTTACCAGGGCTTGACATGCAGAGAAAAGCGGCGGAAACGTCGTGTCCGAAAGGGCTCTGCACAGGTGGTGCATGGCTGTCGTCAGCTCGTGTCGTGAGATGTTGGGTTAAGTCCCGCAACGAGCGCAACCCCTGTCGTATGTTGCCAGCATTTAGTTGGGGACTCATACGAGACTGCCGGCGTCAAGCCGGAGGAAGGTGGGGATGACGTCAAGTCATCATGCCCCTTATGTCCTGGGCTGCACACGTGCTACAATGGCCGGTACAATGGGCTGCGATACCGCGAGGTGGAGCGAATCCCATAAAACCGGCCCCAGTTCGGATCGGAGGCTGCAACTCGCCTCCGTGAAGTCGGAGTTGCTAGTAATCGCGGATCAGCATGCCGCGGTGAATACGTTCCTGGGCCTTGTACATACCGCCCGTCACACCACCCGAGTTGTCTGCACCCGAAGTCGCTGTCCCAACCCTTTGGGAGGGAGGCGCCGAAGGTGTGGAGAGTAAGGGGGGTGAAGTCGTAACAAGGTAGCCGTACCGGAAGGTGCGGCTGGATCACCTCCTTAAGGGC\n","Correct class: Actinobacteria\n","Predicted class: Actinobacteria\n","Input read: GATGAACGCTGGCGGCGTGCCTAATACATGCAAGTCGAGCGGTAACAGGAGAAGCTTGCTTCTCGCTGACGAGCGGCGGACGGCTGAGTAACGCGTAGGAACGTACCCCAAAGTGAGGGATAATTCACCGAAAGGTGAACTAATACCGCATGTGCTCTAAGGAGTAAAGCTACGGCGCTTTGGGAACGGCCTGCGTCCGATTAGCTAGTTGGTAGTGGTAATGGCCTACCAAGGTTACGATCGGTAGCTGGTCTGAGAGGATGATCAGCCAGACTGGGACTGAGACACGGCCCAGACTCCTACGGGAGGCAGCAGTAGGGAATTTTCCACAATGGGCGCAAGCCTGATGGAGCAACGCCGCGTGCAGGATGACGGCCTTCGGGTTGTAAACTGCTTTTCTTTGTGACGACAATGACAGTAGCAAAGGAATAAGGATCTGCTAACTACGTGCCAGCAGCCGCGGTCATACGTAGGATCCAAGCGTTATCCGGATTTACTGGGCGTAAAGAGTTGCGTAGGTGGTTCGTTAAGCAAGACATGAAATCGTGTGGCTCAACCATACGGCTATGTTTTGAACTGATGAACATGAGAACGAGAGAGGTGGCTGGAATTCCCAGTGTAGGAGTGAAATCCGTAGATATTGGGAGGAACACCGATGGCGTAGGCAGGCCACTGGCTCGTTTCTGACACTGAGGCACGAAAGCGTGGGGAGCAAACGGGATTAGATACCCCGGTAGTCCACGCCGTAAACTATGGATGCTAGCTGTATAGAGTATCGACCCTCTGTGTAGCGAAGCTAACGCGTTAAGCATCCCGCCTGGGTAGTACGGTCGCAAGACTAAAACCTAAAGGAATTGACGGGGACCCGCACAAGCGGTGGAGCGTGTTGTTTAATTCGATGATAAGCGAAGAACCTTACCAAGACTTGACATCCTGAGAATTTCTATGAAAGTAGAGAGTGCCTTTTGGAACTCAGTGACAGGTGTTGCATGGCCGTCGTCAGCTCGTGTCGTGAGATGTTGGGTTAAGTCCCGCAACGAGCGCAACCCTTATGTTTAGTTGTATTTTTCTAAACAGACTGCCCTGGTAACAGGGAGGAAGGAGGGGATGATGCCAGGTCAGTATTACCCTTACGTCTTGGGCTACAAACACGCTACAATGGCCGGTACAAAGGGAAGCCAACCCGCGAGGGGGAGCAAATCCCATCAAAGCCGGTCCCAGTTCGGATTGTAGGCTGAAACTCGCCTGCATGAAGTCGGAATCGCTAGTAACGGTAGGTCAGCACACTACCGTGAATACGTTCCCGGGTCTTGTACACACCGCCCGTCAAGCCATGAAAGTCACCAACACCCAAAGTATGAGCATTGCCTCGTCCTAAGGT\n","Correct class: Saccharibacteria\n","Predicted class: Saccharibacteria\n"],"name":"stdout"}]}]}